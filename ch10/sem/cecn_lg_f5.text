computational cognitive neuroscience understanding 
brain mind based biologically 
computational models networks neuron units 
field 
number different including neuroscience computation 
cognitive psychology computational cognitive 
neuroscience difficult subject 
difficult well book intended support 
subject providing coherent principled 
introduction main ideas field 
course 
text 
researchers related areas 
want learn new field 

idea book factors 
positions 
asked cognitive neural network modeling unable 
find existing textbook see discussion 
factor computational algorithm called 
leabra combined coherent major 
established well neural network principles model cognitive 
phenomena leabra provided means 
computational implementation wide range cognitive phenomena 
enabling learn phenomena 
unified framework possible complicated 
computational mechanisms algorithm 
greater underlying biological properties 
neurons cortical networks algorithms enabling 
behavior brain link clearly understood factor 
completion computer simulation environment 
called pdp developed 
provides intuitive flexible 
models allows presentation 
easy simulation exercises book 
powerful serve 
researchers develop models own research 

text subject computational cognitive 
neuroscience potentially set 
write different component aspects 
computation cognition neuroscience existing 
focusing specific issues 
firing patterns individual neurons 

oriented mathematically computational properties 
networks abstract models 
cognitive phenomena 
field based large 
part wide scope issues involved biological 
computational properties cognitive function requires 
perspective hand hand greater 
details captured 

similar 
main existing text scope 
issues original field pdp parallel 
distributed processing 

later works present computational 
biological mechanisms cognitive perspective 
attempt modeling range cognitive phenomena 
read 
convey pdp 
present different 
algorithms ideas simulation exercises separate 
text play 
role understanding complex behavior 
models finally neuroscience short 
treatment models abstract 
biologically implausible mechanisms 

objective replicate scope 
original pdp integrated unified manner 
related biology cognition provided 
intuitive simulations step way 
necessary achieve scope focusing 
consistent set principles form computation 
neuroscience cognition leabra algorithm provided 
computational framework objective 

point write 
textbook based kind new computational 
algorithm standard field fact 
early similarly 
start answer critical question simple leabra 
principled widely 
algorithms field including error backpropagation hebbian 
competitive learning simple point neuron activation 
function text standard algorithms 
context principles shows leabra 
implements same principles biologically 
plausible way addition extra benefit 
principles mutually consistent 
mutually beneficial combining algorithm 
repeated isnt 
consistent set principles underlying models 

summary book attempt integrate 
range fields phenomena coherent 
relatively easily reader 
level result viewed integration 
existing knowledge found 
process putting ideas led 
emergent phenomenon whole greater sum 
parts come away sense 
computational cognitive neuroscience writing 
book feel 

brain think challenging 
science answer 
question new data constantly 
work researchers related fields 
brain 
record biological thought color images 
images results important 
techniques understanding neural 
cognition considerably different 
approaches cognitive 
neuroscience goal important 

cognitive neuroscience remain years come 
complex difficult 
understand sequences images brain thinking 
network regions interact complex ways 
changing patterns thought picture worth words 
language fails attempt capture 
computational models 
based biological properties brain provide important 
language understanding complexity example 
models capture flow information eyes 
letters words parts brain 
activated different word resulting integrated 
text understanding 
phenomenon models enable describe 
levels processing involved common set underlying 
mechanisms implemented computer 
tested ultimately understood 

book provides introduction sub 
known computational cognitive neuroscience simulating human 
cognition based biologically networks neuron units 
provide style textbook treatment central ideas field 
integrated computer simulations allow 
own explorations presented 
text chapter present overview basic 
history computational cognitive neuroscience 
followed overview subsequent chapters range 
basic neural computational mechanisms part range 
cognitive phenomena part including learning perception memory 
language level higher cognition 

whole idea cognitive neuroscience 
notion human thought explained 
same way science reducing complex 
phenomenon cognition simpler components underlying 
biological mechanisms brain 
continues standard method 
fields example matter 
reduced components helps explain 
properties different kinds matter ways 
interact similarly biological phenomena explained 
terms actions underlying 

natural think terms physical 
systems explaining cognition terms physical brain 
possible achieve form terms 
abstract components system argue 
forms explanation form 
explain previously thing terms 
familiar constructs understand definition 
word terms familiar words 
years explain human 
cognition different 
example cognition explained assuming based simple 
assuming works standard serial 
computer approaches idea 
look brain itself language principles 
upon explain human cognition seems likely 
brain ultimately responsible 

cognitive neuroscience 
components based physical human cognition 
brain physical 

importance physical science well captured 
following recall 
provide 
source science components 
theory real things example space 
clearly true physical actually 
measure components theories atoms 
etc interesting 
biological 
biology theories common 
components etc based theory based 
physical evidence structure 
possible develop theories biological 
function terms real underlying components 
etc measured similarly 
previous current theories human cognition based 
constructs attention working memory 
based analysis behaviors physical 
measured cognitive 
neuroscience forms cognitive 
explain cognitive phenomena terms underlying 
neurobiological components principle 
measured 

aspects science particularly 
study human cognition 
emphasis process reducing phenomena component pieces 
essential complementary process pieces 
larger phenomenon refer latter process 
simply say 
brain neurons explain billions neurons 
produce human cognition 
computational approach cognitive neuroscience 
critically important difficult verbal arguments 
human cognition complex phenomenon 
action large number components show 
implement behavior components computer 
test capable 
phenomena simulations crucial developing 
understanding neurons produce cognition 
true emergent phenomena emerge 
interactions obviously present behavior 
individual elements neurons whole greater 
sum parts importance 
areas science cognitive neuroscience 
recently 
relatively fast computers 

illustration importance 
say system composed 
components show 
components interact produce overall behaviors 
interact produce changes speed 
effects emerge interaction 
property component 

shows simple illustration importance 
understanding systems here 
sufficient say system composed components 
shown panel 
specify interact shown panel 
interaction important behavioral 
properties changes speed emerge 
example smaller drives larger achieves 
decrease speed increase 
same driving interact 
smaller produce opposite effect 
essentially means behavior emerge 
interaction clearly 
property individual similarly cognition 
emergent phenomenon interactions billions neurons 

physical 
computational cognitive neuroscience appear sound 
straightforward approach understanding human cognition 
extreme complexity lack knowledge 
brain cognition produces result 
researchers notion hierarchical levels 
analysis order deal complexity clearly 
levels underlying mechanism appropriate explaining 
human cognition example 
try explain human cognition directly terms atoms simple 
focus 
higher level mechanisms exactly level 
right level essentially level presented 
book represents best time 

approach towards thinking issue levels analysis 
suggested introduced 
notion computational 
levels analogy computer 
take example list numbers 
specify abstract terms computation performed 
numbers 
list next next etc abstract 
computational level analysis useful 
different exactly 
think executive summary 
level details 
actually occurs different 
adopt tradeoffs terms speed 
amount memory etc critically algorithm provides 
information actually implement 
specify details language 
variable etc details left 
level actually written 
particular computer particular language etc 

levels corresponding emphasis computational 
levels early 
artificial cognitive psychology 
cognitive science based idea 
ignore underlying biological mechanisms cognition focusing 
important computational cognitive level 
properties traditional approaches based 
assumption brain works standard computer 
computational levels 
important details underlying neurobiological 
implementation 

version emphasis level computational 
widely complexity biology 
psychology approach 
here assumes possible 
identify optimal computation function performed person 
context whatever brain 
somehow same optimal computation 
ignored example argued 
memory curves tuned expected 
frequency retrieval demands items stored memory 
view matter memory 
mechanisms work ultimately driven 
matching expected demands items 
turn assumed follow general sound 
case definition 
ends number assumptions including 
nature underlying implementation real 
independent basis short defined 
purely objective terms optimal 
situation depends detailed 

thing levels 
approaches appear suggest 
level largely irrelevant standard 
computers true 
effectively equivalent level 
issues affect 
computational levels analysis effect higher levels 
analysis already assumed general form 
proper credit shaping whole 
place parallel computers people 
beginning limitations computation algorithms 
assume standard serial computer based address memory 
entirely new algorithms ways thinking 
problems developed order take advantage 
parallel computation brain clearly parallel 
computer billions computing elements neurons 
simple ideas based standard 
computers 

end researchers emphasized 
level primary 
computational argued cognitive models 
detailed neurons 
resulting model contains 
important biological mechanisms 
approach complementary emphasize purely 
computational approach clear understanding 
biological properties important 
ends complicated models difficult 
understand provide insight critical properties 
cognition further models inevitably fail represent 
biological mechanisms possible detail 
sure important 
missing 

level 
adopt fully interactive approach emphasizes 
connections data relevant levels 
reasonable balance simplified 
model known biological 
mechanisms possible place bottom 
working biological facts cognition 
top working cognition biological 
facts approaches example useful take set facts 
neurons encode set equations 
computer see kinds behaviors result 
depend properties neurons useful think 
cognition particular case 
computational level principled basis 
implementation see well 
know brain well 
cognitive job kind 
neurobiological cognitive principled computational 
otherwise emphasized text 

basic levels analysis text 
intermediate level help 

order summarize approach avoid 
associations terminology adopt following hierarchy 
levels see 
essentially simple level physical hierarchy 
lower level consisting neurobiological mechanisms 
upper level consisting cognitive phenomena 
end able explain cognitive phenomena directly 
terms action underlying neurobiological mechanisms 
order help levels 
analysis intermediate level consisting 
principles presented text 
think brain cognition fully described 
principles play role shown 
side figure serve 
clear connection certain aspects biology 
certain aspects cognition understanding 
principles based level computational 
aspects cognition keeping 
discussion levels want avoid 
principles provide level 
description view computational level 
thinking data basic empirical levels 
cognition relevant 
principles shaped help good 
balance primary levels analysis 

essentially levels analysis 
position biological mechanisms operating level 
individual neurons explain relatively complex level high 
cognitive phenomena question basic 
neural mechanisms understanding 
undoubtedly product billions 
neurons include 
neurons simulations seen scaling 
issue way 
scaled model real brain important emphasize 
need scaling least partially issue 
limitations currently available computational 
resources possible put following arguments 
test future larger complex models 
constructed scaled models easier 
understand good place begin computational cognitive 
neuroscience 

approach scaling problem following ways 

target cognitive behavior expect obtain 
models similarly scaled compared 
actual human cognition 
show simulated neurons units 
model approximate behavior real neurons meaning 
build models multiple brain areas neurons 
areas simulated units 
argue brain quality 
same basic properties apply physical scales 
basic properties individual neurons show 
higher levels relevant 
understanding scale large behavior brain 

argument amounts idea neural network models 
performing essentially same type processing human 
particular task reduced problem 
detailed information content human equivalent course 
phenomena different scaled 
content dimension seems reasonable allow 
important properties relatively scale invariant 
example argue major area human 
cortex reduced small portion content 
actually pixel retina 
pixels important aspects 
essential computation information preserved 
reduced model reduced cortical areas 
connected imagine useful simplified model 
reasonably complex psychological phenomena 

argument individual effects neurons 
neurons likely roughly similar average effects 
neurons likely 
true assuming similar patterns weights areas 
neurons extent neurons communicating signals 
depend largely average firing matter 
debate current evidence rule 
possibility see details 
course encode information 
robust effects noise constant 
scaled model approximation 
original argument applies computational neural 
network algorithms including main text 
average firing based rate activation again 
next chapter 

finally reasons believe brain 
character likely least cortex 
effective properties range long connectivity similar 
local range short connectivity example short 
range long connectivity produce balance excitation 
inhibition virtue connecting excitatory inhibitory 
neurons model 
based properties range short connectivity 
cortical area describe scale larger model 
containing cortical areas simulated level 
reason basically same 
averaging neurons average behaves roughly same 
individual neuron levels description 
similar self means place 

short arguments provide basis models 
based neurobiological data provide useful accounts cognitive 
phenomena involve large widely distributed areas 
brain models described book 
issue remains open important 
question computational cognitive neuroscience 
following perspective provides 
overview important issues shaped 
field 

field computational cognitive neuroscience 
relatively easily large 
number related 
time research aspect cognition 
neuroscience computation potential important 
contribution field entire space book 
account relevant history 
field section intended provide brief 
overview particularly relevant context 
motivation approach specifically focus 
understanding networks simulated neurons lead 
interesting cognitive phenomena occurred initially 
again period present 
form main approach 
clear said neural network 
modeling approach provides crucial link networks neurons 
human cognition 

field cognitive psychology 
early following key 
perspective least associated new 
field emphasis internal mechanisms 
cognition particular explicit computational 
models simulating cognition computers problem solving 
mathematical dominant 
approach based computer metaphor 
human cognition processing standard serial computer 
time researchers 
modeling cognition networks simple neuron processing 
elements operating parallel 
computer metaphor approach 
book 
simple neuron models significant computational 
limitations unable learn solve large 
basic problems 

researchers studying network neural 
models 
critical field back real 
early psychological 
computational 
based 
activation dynamics networks backpropagation learning 
algorithm 
parallel distributed processing pdp 

established neural network models 
critically backpropagation algorithm limitations 
earlier models enabling essentially function learned 
neural network led new cognitive modeling 
goes name based 
backpropagation algorithm backpropagation represented step 
computationally step backwards biological 
perspective clear implemented 
biological mechanisms 
based backpropagation cognitive modeling clear 
biological basis causing researchers same kinds 
arguments computer metaphor 
approach computational level arguments discussed 
previously biological issues 
field essentially computational cognitive psychology 
based neural processing principles true 
computational cognitive neuroscience 

parallel influence neural network models 
understanding cognition rapid 
oriented biologically modeling identify 
categories type research divide 
biological models emphasize learning 
learning non types models include detailed 
models individual neurons 
information approaches processing neurons networks 
neurons 

original 
models hold considerable 
due underlying mathematical terms 
statistical research led 
important tends direct 
relevant cognitively issues network itself 
provides important principles see 

based biologically learning models focused learning 
early visual system emphasis hebbian learning 

importantly large body basic neuroscience research 
idea hebbian mechanisms operating neurons 
important cognitively areas brain 

hebbian learning computationally weak suffers limitations 
similar generation learning mechanisms 
widely cognitive modeling 
generally learn psychological tasks 

addition cognitive biological 
neural network research considerable work done 
computational end apparent mathematical basis 
neural networks common statistics 
computational connection further 
recently bayesian framework statistical 
applied develop new algorithms 

generally understand existing ones models 
developed point provide framework 
learning works reliably wide range cognitive tasks 
simultaneously reasonable biological 
mechanism principal researchers 
computational end field concerned 
theoretical statistical learning kinds issues 
cognitive biological ones 

short field state 
computational cognitive psychology primarily focused 
understanding human cognition close 
underlying biological focused 
information constructs computationally weak learning 
mechanisms close cognition learning 
focused computational level analysis involving 
statistical constructs close biology 
cognition think strong set cognitively 
relevant computational biological principles 
years time attempt 
integrate principles coherent overall framework 

brief overview provides useful 
describing basic characteristics 
approach taken book basically tried 
develop single coherent algorithm includes 
based backpropagation driven error learning hebbian learning 
principles central principles 
network interactive satisfaction constraint style processing 
underlying implementation direct ion 
channels behavior real neurons described 
incorporates number 
established well anatomical properties 
neocortex described 
detailed connections biology cognition 
algorithm way consistent well established 
computational principles 

recent shaped nature integrated 
algorithm development biologically 
plausible way implementing backpropagation learning algorithm 
resulting algorithm called generec 
consistent known biological mechanisms learning greater 
biological properties brain including 
interactivity allows realistic simulated neurons 
development mechanism neural 
competition powerful distributed 
representations combined interactivity learning 
way previously possible 
competition number important functional benefits 
required hebbian learning mechanisms 

approach based combination generec hebbian 
learning competitive interactive activation 
dynamics set properties leabra algorithm 
local driven error associative 
describing hebbian 
realistic biologically algorithm 
text leabra pronounced 
emphasizes balance different 
achieved algorithm consider leabra 
coherent framework computational cognitive neuroscience 
provides useful existing ideas help 
identify limitations problems need solved 
future following section discuss 
basic principles approach detail 

finally worth noting arguments proposed 
favor developing unified theories cognition 
apply notion developing unified neural network algorithm 
thought unified theory essential 
principles underlying cognition requires 
terms overall architecture theory 
cognition essential point here 
relatively easy relatively 
specialized theories specific phenomena taking 
range data constraints 
theory account data likely 
true model 
architecture clear process unified 
architecture common set 
ideas net case 
neural network models models generally 
constraints modeling process fact 
single set principles implemented leabra algorithm 
model wide range phenomena covered book 
measure 

discussion benefits unified model 
number general issues regarding benefits computational 
modeling cognitive neuroscience 
think benefits generally 
important potential problems 
associated well provide brief summary 
advantages problems here note points 
mutually fact tend load central 
issues specific derivative issues provides 
richer perspective 

advantages 
computational model forces 
explicit assumptions exactly relevant 
processes actually work example people tried 
explicit computational models object recognition didnt 
seem difficult problem 
story going 
implement model say 
didnt 

computational model deal complexity 
ways verbal arguments times 
interactions emergent phenomena 
model produce satisfying explanation otherwise 
hand arguments 

computational model control 
variables precisely real system 
replicate results precisely enables explore 
role different components ways otherwise 
impossible 

computational models enable data 
multiple levels analysis integrated related 
example computational models book show 
biological properties rise cognitive behaviors ways 
impossible simple verbal arguments 

theories cognition 
particular components theory 
end work theory 
example executive theory frontal pre cortex 
function executive explaining 
good brain areas 
explained well put 
box 

computational model provide insight 
works example providing 
explanation phenomenon 
phenomena complex interactions 
components obtain insight putting set 
principles assumptions implementing seeing 
happens parameters see kinds 
effects 

computational model sense 
real exists behaves 
novel 
running model novel context exercise 
verbal theory due lack specificity 
flexibility verbal constructs 

computational model forces 
consequences assumptions generally 
possible computational models 
general flexible account new 
data via acts verbal hand model 
modified account new data clear exactly 
changes easily 
evaluate resulting previous theory 

difficult people detect 
purely verbal theory hard time 
keeping computational model 

actually work 

computational model forces 
aspects problem otherwise 
ignored considered irrelevant ends 
aspects see 
problems useful exercise 
problems anyway 

problems 
properties relevant computational models 
require know 
relevant details end number 
arbitrary assumptions learn model 
partially correct assumptions end 
inevitably ends empirical question 
depends wrong assumptions influence 
results identify 
critical principles models behavior 
demonstrate relative assumptions 
work problem 
model viewed concrete 
principles end itself 
essential step principles clear 
taken 

theory easy computational 
model theory point 
fact model set data explanation 
data said model viewed concrete 
principles way model 
principles account data 
clear theory level principles 
interactions model 
approach complexity model 
advantages modeling place 
detailed behavior model terms 
principles nature 

models explain 
inevitably successful models true 
principles model properly 
understood 
took powerful model fit data 
tell principles 
model clear account data 
contribution now relationship 
principles data completely 
principle train neural network learn task 
data fact subjects learn task 
isnt saying apply model 
range data different tasks greater detail 
task detailed properties learning process 
principles work tested 
continue fit data constitute important 
advance end cortex 
understood relatively small number powerful principles 
seems rule possibility powerful model 
account lot data 

final chapter book revisit issues again 
benefit comes 

aspects human cognition particularly 
kinds neural mechanisms described text 
describe important aspects here 
order further connections 
cognition 
aspects cognition obvious average 
person nature own cognition 
tend emphasize aspects 
definition aware appear serial 
thought time focused subset things 
occurring brain fact undoubtedly 
standard serial computer model 
understanding human cognition point 
comparison discussion 

argue aspects human cognition 

cognition possible 
relatively 
understand cognition focusing 
difficult necessary 
keep place 
whole thing important 
notion 
space phenomena 
focused metaphor 
limits following light 
important ideas 
keep aspects cognition 
discuss 

gradedness 
interactivity 
competition 
learning 

cognitive neuroscience 
unable say useful experience 
phenomenon book note 
last chapter book specifically level higher 
cognition closely associated 
experience present set ideas models provide 
basic mechanisms principles developed 
rest book sequential discrete focused 
nature experience view properties 
due particular brain areas 
prefrontal cortex pfc hippocampus result 
emergent phenomena arise basic properties 
neural processing 
processing system chapter emphasizes 
continuum 
processing 

able 
same time simple case parallel 
processing thing same time 
essential understanding neural basis human cognition 
level analysis know 
neurons human brain 
contributes bit overall human cognition 
brain 
neurons 
important true 
figure 
based 
different brain removed 
tested simple tasks found didnt seem 
matter 
large amount 
solve simple tasks figure 
problem tasks simple 
solved number different mechanisms 
example lesion completely ability 
visual processing solve task 
later research showed part brain 
contributing own special way overall cognition 
particular cases contributions redundant 
good thing survival perspective function 
basic level 

lots examples typical experience 
parallel processing evident driving 
etc 
name 
effect speaking reading 
reading name last example 
particularly interesting processes reading 
speaking know lots 
examples parallel processing taking place 
cases simply aware multiple things 
going example look visual scene part 
brain processes visual information identify 
seeing part things 
people lesions brain areas 
things apparently 
way view world product 
bunch specialized brain areas operating 
fashion opened 
techniques brain 
obvious multiple brain areas inevitably 
activated cognitive tasks 

challenge parallel processing want 
understand cognition difficult figure 
processes sub eventually end 
sensible whole contrast cognition 
bunch discrete sequential steps task 
easier identify steps sequence 
body problem 
understanding interaction things 
simple number things operating 
same time mutually 
hard figure going 

virtue approach cognition presented book 
based start parallel processing providing powerful 
mathematical intuitive understanding 
interactions large number processing units 
neurons lead useful cognition 

example graded nature categorical 
representations middle item 
categories 

contrast discrete binary memory 
representations standard computers brain 
graded nature see next chapter 
neurons integrate information large number different input 
sources producing essentially continuous real valued 
number represents relative strength 
inputs compared inputs received 
neuron graded signal rate firing 
activation neurons function relative 
strength value result neurons good 
graded signals convey extent 
degree true example 
neuron convey object 

last likely 
research people categories 
shown tend things graded 
manner according close item 
example category 

graded activation values important 
representing continuous dimensions position force 
color coarse coding function basis representations shown 
here units shown graded activation signal 
roughly close point continuous 
dimension units preferred point defined 
point response 

gradedness critical kinds perceptual motor 
phenomena deal continuous underlying values position 
force color turns brain tends 
deal same way continuum 
different neurons represent different 
values continuum cases 
essentially placed points respond graded 
signals reflecting close current 
preferred value see type 
representation known coarse coding 
basis function representation actually precise 
indication particular location continuum 
weighted estimate based graded signal associated 
basis values 

important aspect gradedness fact 
neuron brain receives inputs 
neurons individual neuron critical 
functioning neurons contribute part 
graded overall signal reflects number neurons 
contributing well strength individual 
contributions fact rise phenomenon 
function 
increasing amounts damage neural 
explain saying neurons reduces strength 
signals eliminate performance entirely 
contrast standard computer tend fail 

obvious equally important aspect gradedness 
way processing happens brain 
familiar 
associated trying remember come 
mind immediately trying 
different things hit upon right thing 
psychologists 
tongue phenomenon 
tongue remember gradedness critical 
here allows brain bunch relatively weak 
ideas see ones stronger 
things ones weaker away 
similar bunch relatively weak factors 
add support idea single clear 
discrete reason computationally phenomena 
examples bootstrapping multiple constraint 
satisfaction bootstrapping ability system 
itself taking weak 
information eventually producing solid result multiple 
constraint satisfaction refers ability parallel graded 
systems find good solutions problems involve number 
constraints well discuss greater length 
basic idea factor 
constraint solution rough proportion graded 
strength importance resulting solution represents 
kind constraints 
roughly same direction 
number constraints remain sounds 
write equations 
works run simulations showing action 

way brain standard serial computer 
processing direction time 
lots things same time 
going backwards 
known interactivity 
processing think brain 
organized processing areas visual stimuli 
example processed simple level low way 
terms oriented lines present image 
subsequent stages features represented 
combinations lines parts objects objects 
etc least correct system 
interactivity amounts bottom top 
processing information flows simple 
complex complex simple 
combined gradedness interactivity leads 
satisfying solution number otherwise cognitive 
phenomena 

example well documented people 
faster accurate letters context 
words context random letters word 
effect finding unidirectional 
serial computer perspective letters identified words 
read context word help 
letter finding seems natural 
interactive processing perspective information higher 
word level come back affect processing lower 
letter level gradedness critical here allows 
weak estimates letter level activate 
word level comes back 
letter estimates 
overall representation word letters explanation 
word effect proposed 
interactivity important 
bootstrapping multiple constraint satisfaction processes 
described allows constraints levels 
processing bootstrap converge good overall 
solution 

ambiguous letters 
context words example 
interactivity level word processing level letter 
processing 

examples interactivity 
psychological involve stimuli 
ambiguous level context higher level 
processing example shown 
words constrain ambiguous stimulus look 
case 

saying competition good thing 
true brain 
evolution brain competition neurons leads 
selection certain representations 
strongly active 
context bootstrapping described analogy 
process survival idea 
important force shaping learning processing 
encourage neurons better adapted particular situations 
tasks etc argued kind 
competition provides sufficient basis learning brain 
find number important 
mechanisms biologically well known 
inhibitory interneurons provide mechanism 
competition areas brain central cognition 

cognitively competition evident phenomenon 
attention closely associated perceptual 
processing clearly evident aspects cognition 
phenomenon spatial attention demonstrated 
posner task good example here ones attention 
particular region visual space cue 
bar computer screen stimulus 
target presented 
cue opposite region space subject respond 
pressing key computer whenever detect 
onset target stimulus result target 
detected significantly faster location compared 
non happens faster move ones eyes 
kind internal attention 
result processing cue stimulus see 
results 
related ones accounted simple model 
competition neurons mediated inhibitory 
interneurons 

well nature versus debate development human 
inevitably terms 
genetic configuration brain results based experience 
learning important contributions fact 
advance understanding exactly 
genetic configuration learning interact produce human 
cognition understanding major goal 
computational cognitive neuroscience unique position 
able simulate kinds complex subtle 
exist certain properties 
brain learning process addition 
learning process learning occurs constantly cognition 
possible identify relatively simple learning 
mechanism appropriately initial 
architecture organize billions neurons human brain 
produce whole range cognitive functions exhibit 
obviously cognitive neuroscience 
reason text dominated properties 
learning mechanism biological cognitive environment 
operates results 
course focus importance genetic 
basis cognition feel 
context learning mechanism genetic parameters 
fully understood role itself shaping 
understood context emergent 
process 

takes learn important 
gradedness aspects biological 
mechanisms discussed problem learning considered 
problem change learn change way 
information processed system easier 
learn system responds changes graded 
manner way behaves 
allows system try new ideas ways 
processing things kind graded 
indication changes affect processing exploring lots 
changes system evaluate 
improve performance 
learning bootstrapping phenomenon described 
respect processing earlier depend number 
weak graded signals exploring possibly useful 
proceed further building look 

kind bootstrapping possible discrete system 
standard serial computer responds 
small changes way putting computer 
typically works right 
missing step typically provides indication 
well perform complete same thing true 
system relationships typically 

discrete systems typically provide 
effective learning 

present view learning dominated 
bootstrapping small changes idea think 
kinds learning discrete nature 
trial error kind learning familiar 
experience here discrete hypothesis 
behavior trial outcome 
error update hypothesis next time 
discrete find best 
implemented same kinds graded neural mechanisms 
kinds learning 
discrete kind 
learning associated particular 
discrete facts events appears brain specialized 
area particularly good kind learning called 
hippocampus properties learning 
discrete character discuss type learning further 

described book based relatively small 
coherent set principles introduced part 
text applied part range different 
cognitive phenomena principles implemented leabra 
algorithm exploration simulations 
integrated chapter form 
part text tried ensure 
lot book simulations 

emphasis book understanding set 
principles leads wide human cognitive phenomena 
provide detail aspects neuroscience 
computation cognition potentially related 
central present expect 
mathematical computational researchers cognitive 
psychologists find book detailed 
area provide coherent framework 
areas consistent well established facts 
domain provide useful means 
knowledge areas current 
debate upon choice presented 
relevant arguments data presented short 
existing relevant supporting arguments details 
necessarily presented idea book cases 
likely find relatively 
coherent clear 
picture rapidly maintain 
fit working memory reader 
follow rapid enable 
motion ideas book proceed 
rapidly cognition order 
overall picture emerge leaving 
reader facts 

tradeoffs 
cover large space existing neural 
network algorithms 
cover range computational algorithms provided 
interested reader further 
algorithms ideas covered here 
represent novel potentially provide important 
principles cognition approach 
introduce new principles existing ones 
found obviously case scope 
cognitive phenomena covered book said 
conclude principles anyway 
considered final inevitably rough 
domain level satisfaction 
present time 

overview approach 
sections clear leabra algorithm book 
incorporates important ideas shaped history 
neural network algorithm development book 
principles introduced simple clear manner possible 
explicit development ideas 
comes time implement explore ideas simulations 
leabra implementation consistency 
reader knowledge standard 
algorithms unified integrated perspective helps 
understand relationship 
learning work 
different algorithms favor 
integrated algorithm necessary 
fully understand practical level 
simple required understand algorithm 
wide 

interactive computer simulations 
relevant principles interact 
produce important features human cognition detailed step step 
exploring simulations provided 
set exercises 
evaluation purposes models wide range human cognitive 
phenomena presented perception memory 
language level higher processing controlled appropriate 
focus cognition consider perception form 
cognition emphasize processing takes place human 
neocortex typically referred simply 
cortex large neurons 
part brain plays 
important role cognition interesting property 
relatively area area same basic 
types neurons present same basic types connectivity 
patterns principally allows single type 
algorithm explain wide range cognitive phenomena 

terms detailed organization already mentioned 
main parts book part basic neural 
computational mechanisms part large scale brain 
area organization cognitive phenomena chapters 
part individual neurons networks 
neurons learning mechanisms chapters 
part perception attention learning 
memory language level higher cognition 
chapter large scale brain area functional 
organization chapter begins overview 
followed detailed table let reader 
know scope covered chapters 
appear end chapter 
read 
list reading provided 

key words defined extensively 
easy 
found index explorations 
evaluation answer key available 
required provide 
written answer good idea look 
consider answer important 
issues book 

original pdp distributed parallel processing 
considered field 
remain relevant 

collection important early neural 
networks 

basic cognitive neuroscience 
levels analysis suggest chapter 
chapter 

computational modeling artificial neural 
networks see 

neuron unit abstract computational 
models provides basic information processing mechanisms 
human cognition computationally neurons 
function dedicated specialized detectors 
smoke detector neuron integrates information different 
sources inputs single valued real number reflects 
well information matches neuron 
specialized detect sends output reflects 
results evaluation standard 
fire integrate model neural function output 
provides input neurons continuing information 
processing network interconnected neurons 
biologically neuron provides basic mechanisms necessary 
integration evaluation information receives 
results neurons 
self way processes information 

chapter provides overview level computational 
description neuron detector biological mechanisms 
underlie neural information processing focus 
neuron cortex 
consistent general focus book cortex 
sum biological mechanisms known activation 
function resulting output neuron called 
activation value actually bit known 
neural activation function leabra algorithm balance 
based biologically mechanisms hand 
keeping computational implementation relatively simple 
point neuron activation 
function biological details regarding basic 
dynamics information processing neuron 
spatial extent neuron single point 
simplifies computational implementation simulations 
chapter basic properties activation 
function arises underlying biological properties 
neurons show activation function 
understood terms mathematical analysis based bayesian 
hypothesis testing 

standard serial computer basic information processing 
simple memory manipulations 
retrieval brain 
based same kinds appear 
case order understand functions 
biological mechanisms neuron need come 
computational level description neuron 
purpose section standard 
computer point comparison 

standard computer memory processing separated 
distinct processing central 
processing unit information 
memory processed stored 
back memory contrast brain appears 
parallel distributed processing pdp processing 
occurs simultaneously parallel billions neurons 
distributed brain memory processing 
similarly distributed brain computational 
level description neural processing explain neuron 
provides memory processing functions distributed way 
producing useful neurons work 

central idea explain neuron 
detector simply put neuron detects existence 
set conditions responds signal 
extent conditions think smoke 
detector constantly air looking conditions 
indicate presence fire brain 
neurons early stages visual system constantly 
visual input looking conditions indicate 
presence simple visual features bars light 
position orientation visual scene higher 
visual system neurons detect specific sets objects 

emphasize useful view function 
neuron detector content exactly 
detecting well captured relatively 
things smoke detectors contrast 
detectors neuron considerably complex 
different inputs huge dynamic 
network neurons possible 
describe roughly neuron detecting need necessarily 
case see later neurons contribute 
overall computation number detecting different 
hard subsets combinations things neurons 
response context sensitive depends 
things otherwise example 
oriented bar detectors early visual system 
respond visual scenes bar light appropriately 
oriented detector result 
aspects scene context 
bar light further detector respond time scene 
viewed next result dynamic changes 
network determine ones focus attention 

way think context sensitivity 
neuron act dynamic detector plays multiple 
roles different situations 
model smoke detector analogy further 
detection obviously appropriate sensory processing 
describe processing motor output pathways 
brain purely internal processing neuron detect 
motor response output 
leads response abstract internal 
actions attentional system thinking 
appropriate word describe thought 
terms detecting appropriate conditions things 
virtue detector model easily 
complexity basic framework 
understood initially simple intuitive 
terms analogy simple smoke detectors 

detector model neuron emphasizes important properties 
emphasizes neurons dedicated specialized 
smoke detectors neuron 
memory cell computer hold arbitrary 
information neuron dedicated detecting 
specific set things matter difficult 
describe enables neuron 
simultaneously perform memory processing functions memory 
amounts conditions applies inputs 
order detect whatever detects processing 
way goes conditions 
communicating results neurons neurons 
traditional line specialize 
step process whereas 
computers build 
different parts themselves analogy helps 
explain useful emerge actions large 
number individual neurons captures 
achieved parallel processing compared serial 
processing 

specialized nature neural detectors important enabling 
refer representation neuron group 
neurons term history 
widely way roughly consistent idea 
neurons representation simply detects 
neuron detects oriented bar position said 
represent oriented bar position common 
refer pattern activity multiple neurons 
representation case refers things 
group detects term reflect 
properties neural response itself example 
distributed representation multiple neurons participate 
representing thing next chapter 

finally important note traditional view 
brain standard serial computer inconsistent 
biological facts rise detector model 
traditional production system model cognition potentially 
consistent least general here cognition 
simulated 
seen detectors ones 
fire activated 
detect appropriate configuration information active 
system result firing 
activation state appropriate way example production 
adding numbers detect presence digits 
active add fires 
digits sum state 
provides appropriate configuration activating 
production revisit issue 
see neurons implement 
production constraints 
dedicated specialized representations 

detector model function 
corresponding neural components 

detector model provides direct interpretation 
functions components neuron shown 
view neural functioning 
known fire integrate model simplest 
view neural functioning detector 
needs inputs provide information 
detection elaborated neuron receives 
inputs via synapses typically occur 
dendrites extend 
large cell body neuron human brain 
relatively neurons directly connected sensory 
inputs rest getting inputs earlier stages 
processing multiple levels detectors lead 
powerful detection 
work directly raw sensory inputs 
job difficult 
hard figure kinds input information 
neuron detection 
related sensory inputs 

regardless gets detector needs perform 
processing inputs relative contribution input 
value overall detection controlled 
weights implemented relative 
synapse inputs neuron aka synaptic 
synaptic strengths weights provide 
critical parameters neuron detects 
essentially neuron detect different patterns 
inputs input patterns best fit pattern weights 
producing detection response appear 
neural individual inputs 
treating part overall input pattern 
done combining integrating weighted 
inputs form measure degree 
input pattern fits expected happens 
properties dendrites resulting eventually 
membrane potential voltage cell body central 
part neuron reflects results combination 

detector needs evaluate extent 
combined input sufficient count detected worth 
communicating smoke want evaluation 
shouldnt 
gets 
want real fire amount 
setting threshold combined input responding 
threshold neuron 
mechanism cell body implementing threshold 
modified result neural activity threshold 
turned neuron saying 

finally detector needs communicate results 
processing form output 
neurons detected looking 
smoke 
output smoke neurons 
provide graded output signals reflect 
weak outputs 
low strong outputs 
high ones neural outputs communicated long 
process cell body called 
axon forms synapses neurons dendrites 
providing inputs described repeating 
chain neural processing relatively neurons 
actually produce physical output way 
forms true output entire network cognitive 
models details output simulated 
internal output representation captures 
necessary relevant output information 

order detector model neuron real 
consistent larger understanding networks 
detectors perform useful computations exhibit human 
cognition details larger picture subsequent 
chapters critical idea learning provide means 
getting bunch neural detectors useful 
learning neurons implemented weights synaptic 
provide main parameters neuron 
detects shaping weights learning shapes neurons 
detect turns powerful ways sure 
neuron learns detect end useful 
larger task performed entire network overall 
result network learning contains number 
detectors way produce 
proper outputs set inputs 
internal detectors related way 
humans way perform task 

learning shape networks neural detectors 
ways result good task performance understand 
individual detectors work typically understand 
direct mathematically way entire network 
behaves general level reason 
network neurons 
possible different ways detectors configured 
different combinations activity 
detectors understand 
consequences different detailed way 
put way neural networks complicated systems 
easily computational level 
researchers provide 
better ways learning 
processing networks inevitably requires significant 
limitations behavior network 

state potentially consider 
analogous situation standard serial computer say 
understood pieces basic 
solid put 
shape computer right 
thing kind 
greatly objective better 
computers number factors 
situation considerably better seem already mentioned 
important existence learning mechanisms 
derived sound mathematical basis provide 
network converge good solutions wide range 
tasks addition useful ways mathematically 
summarizing aspects networks behavior ways 
help understand behaves way 
principally energy functions covered next 
chapter finally large number principles 
explain important aspects behavior networks 
situations provide rich basis understanding 
principles explained book appropriate 
bottom line practice networks neurons 
successfully perform wide range cognitive tasks see 
subsequent chapters 

described function neuron detector now 
explore biological mechanisms enable neuron 
integrate combine information inputs mechanisms 
based movement atoms called ions 
neuron generating currents 
neuron electrical system understood 
basic principles electricity addition electrical 
need understand behavior concentrations 
areas relatively high low 
ions liquid process called 
diffusion basic ideas introduced 
applied towards understanding neuron works turns 
neuron different concentrations ions 
cell enables diffusion forces 
interesting basic result neuron integrates 
inputs controlled flow specific ions 
cell turn changes electrical potential voltage 
neuron drives neuron produce outputs 
function potential 

electricity behavior movement called 
charge basic matter 
purposes 
negative charge positive charge 
same magnitude opposite sign atoms 
equal numbers 
net charge ions 
atoms versa vice 
negative positive net charge typically 
difference indicated writing name 
ion corresponding number plus minus ions 
relevant neural functioning 
potassium calcium 

interesting thing charge 
rule negative charges close 
possible positive charges versa vice means 
larger concentration ions 
negative ions towards 
area happens electrical current 
simply movement charge place 
sign same charges 
ions area 
move away assuming positive 
negative ions ions liquid case 
neurons same type current caused positive 
ions leaving negative ions negative 
current opposite case positive ions 
negative charges leaving area positive 
current 

extent amount positive negative charge 
place called electrical potential reflects 
potential amount opposite charge area 
again negative charges rise negative 
potential positive charges positive 
potential note area starts opposite 
charges starts potential 
new opposite charges net charge 
potential area changes function current coming 
area play important role neuron 
behaves excited charges 

sketch ohms law action 
charges leads potential difference 
charge represented line 
drives current channel conductance 

ions 
move caused liquid 
small channels 
pass sense greater 
ions move greater 
amount potential required imagine 
top bunch 
higher 
potential ion 
relationship known ohms law 
current amount motion 
electrical potential 
possible same thing slightly convenient 
form terms called 
conductance conductance represents easily ions 
place labeled 
ohms law written terms 

forms basis equation describing neuron 
integrates information summarized 
brief neuron channels 
determine conductances type ion 
function input receives potential referred 
membrane potential updated computing 
current ohms law tell charges 
moving neuron membrane potential 
change applying ohms law 
again compute changes potential time 
model neuron computes turns due 
combined forces diffusion explained electrical 
potential ion respond membrane potential 
different way ion ends own unique way 
contributing overall current add 
currents overall current 

addition electrical potentials main factor 
causes ions move neuron 
force called diffusion recall electrical potentials 
caused concentrations positive negative ions 
location diffusion comes play 
concentrations put simply diffusion causes 
type distributed space 
time large concentration location 
diffusion acts spread concentration 
possible sounds simple underlying 
causes diffusion complicated results 
fact atoms liquid constantly moving 
results process tends average 
cause diffusion direct 
force electrical potential indirect effect 
well key thing diffusion 
type move gets 
large concentration type ion 
place 
equally large concentration ion same charge 
contrast 
electricity care different types ions positive 
charge same perfectly 
ions large concentration 

sketch diffusion action types 
ions move same direction independent 
due effects random motion 

fact direct force treat 
diffusion reliable effect 
electrical force convenient write similar 
force equations diffusion essentially 
same terminology electricity describe happens 
ions result concentration differences 
charge differences imagine box 
liquid separated closed 
large number particular type ion imagine blue 
concentration 
difference 
results diffusion potential cause 
ions move 
removed diffusion current generated ions move 
side case electricity reduces 
diffusion potential eventually well 
diffusion acts electrical conductance 
relationship analogous 
ohms law holds 

order compute current ion produce need 
way summarizing results 
diffusion forces ion accomplished 
special equilibrium point 
diffusion forces balance concentration 
ions exactly individual ions 
moving randomly point zero current 
respect type ion current function 
net motion ions course simple system 
electrical forces equilibrium point electrical 
potential actually zero different concentrations 
ions neuron resulting diffusion forces 
equilibrium point typically zero electrical potential 

absolute levels current involved generally relatively 
small assume relative concentrations 
ion cell remain relatively fixed time 
addition see neuron special mechanism 
maintaining relatively fixed set relative concentrations 
turns equilibrium point expressed amount 
electrical potential necessary effectively constant 
diffusion force potential called equilibrium 
potential reversal potential current changes 
sign side zero point 
driving potential flow ions drive 
membrane potential towards value 

equilibrium potential particularly convenient 
correction factor ohms law simply 
away actual potential resulting net 
potential 
call diffusion version ohms law 
applied ion ion basis current 
type ion see 

now put basic principles electricity diffusion work 
understanding neuron integrates information 
say bit neuron kind 
environment neuron cell 
cell body purposes think 
neuron liquid 
membrane generally prevents things 
leaving cell including ions interested 
membrane difference 
electrical charge neuron referred 
membrane potential 

practical purposes ions interested 
cross membrane specifically let 
special called channel imagine 
channels allow specific types ions cell 
opened closed mechanisms 
important later level conductance 
ion determined number open channels pass 
type ion 

neurons liquid environment brain called 
space similar 
interesting thought 
place certain amount 
results reasonable 
concentration ions reasons 
described ions lower concentration 
neuron space 

summary major activation ions 
channels 

now consider type ion turn assess electrical 
diffusion forces channels allow flow 
neuron summarized 

exists greater concentration 
neuron diffusion force 
neuron order neuron 
positive charge relative equilibrium 
potential positive typical value 

relatively low internal concentration 
produced active energy mechanism called 
potassium ions neuron 
smaller amount ions net effect 
produce negative resting potential 
potential holds inputs coming neuron 
positive ions cell 
net negative charge negative 
charge typically 

primary types channels pass 
important purposes excitatory synaptic input 
channel opened neurotransmitter glutamate 
released sending neurons activated 
dependent voltage channel opening 
dependent level membrane potential 
plays central role action potential 
described later channel implemented directly 
leabra algorithm summarize action potential 
mechanisms simpler implementation general plays 
central role excitation activation 
neuron 

ions exist greater 
concentrations neuron again 
diffusion force neuron 
negative charge diffusion force 
negative potential neuron equilibrium 
potential negative typical value right 
note same negative resting 
potential caused maintained low internal 
concentration additional mechanism necessary 
maintain concentration maintained 
negative resting potential itself 

main channel inhibitory synaptic input channel 
opened neurotransmitter released 
inhibitory interneurons activated note due 
equilibrium potential similar resting potential 
inhibition neurons effect 
current generated neuron starts 
excited membrane potential phenomenon 
described inhibition 

potassium exists greater concentrations 
cell previous ions 
diffusion force neuron 
positive charge neuron needs negative charge 
keep leaving equilibrium potential 
negative value typically 
tend maintain concentration due 
negative resting potential internal 
concentration bit potassium 
ions cell 
negative equilibrium potential concentration 
difference larger 

different types channels 
relevant purposes leak channel 
constantly open lets small amounts potassium 
turns channel lets small amounts 
equilibrium potential same 
ion same resting potential 
roughly dependent voltage channel 
effects excitation produced 
action potential larger amounts 
neuron excited again channel implemented 
directly algorithm deal details 
action potential generation type channel present 
function amount calcium ion present 
neuron extended periods activity 
channel produces accommodation effect 
active neurons discussed further last section 
chapter general plays largely 
role neuron 

calcium present concentrations 
neuron larger huge concentrations 
neuron diffusion force 
positive internal potential back 
potential order due 
relatively large concentration differences involved note 
extra positive charges 
electrical potential acts strongly ion ions 
net charge difference 

leabra algorithm explicitly simulate 
important channels tend influence 
activation cell membrane potential 
cause things happen example nmda 
channel glutamate released excitatory 
neurons critical learning mechanisms described 
subsequent chapter accommodation effect opposite 
sensitization effect depend presence ions 
neuron measure neural activity ions enter 
neuron gated voltage channels presence 
indicates recent neural activity exist small amounts 
neuron concentration provides reasonable 
indication average level neural activity recent 
time period useful 
things happen neuron 

covered major ions channels neural 
processing now position put 
equation reflects neural integration information recall 
result equation updating membrane 
potential variable voltage 
membrane ohms law compute 
current ion channel add currents 
type ion channel 
now need know things 
equilibrium potential fraction 
total number channels ion open present 
time maximum conductance result 
channels open ions channels let 
current channel 
diffusion ohms law described 
total conductance fraction open times maximum conductance times 
net potential difference membrane potential 
present time equilibrium potential 
basic channels activation work 
neuron excitatory synaptic input channel 
inhibitory synaptic input channel leak 
channel total net current 
channels 

said current affects membrane potential 
movement charges decreases net charge difference 
causes potential place following equation 
updates membrane potential model based previous 
membrane potential net current 
time constant 
simulator typical value potential change 
capturing corresponding slowing change neuron 
primarily result cell membrane 
details particularly relevant here 
fact slow changes 

understanding behavior neurons useful think 
increasing membrane potential resulting positive current 
excitation shows 
according electricity increasing membrane potential 
results negative current match 
relationship potential current simply change 
sign current model add 
previous membrane potential 
course mathematically equivalent 
captures intuitive 
relationship potential current now 
refer order 
notation finally mathematical terms say current 
temporal rate change variable 
derivative membrane potential 

computed net current 
membrane potential updated excitatory inputs time 
conductance conductance 

equation provides means integrating 
inputs neuron show here different 
values conductances different ion channels well see 
moment actually compute conductances function 
inputs neuron equation 
point simulate response neuron fixed input 
providing values conductances 
shows graph net current membrane 
potential starting current rest potential 
responding different inputs come form 
value starting time step value 
constant inhibitory conductance run 
example 
important thing note figure membrane 
potential response excitatory input 
level dependent strength conductance 
excitation compared leak conductance 
conductances present membrane potential provides 
basis neurons subsequent output described 
tell stronger inputs put 
neuron threshold responding 
weaker clearly threshold sub 
interesting note fact value clearly 
corresponds change derivative 
back towards settles 
equilibrium value 

equilibrium point perfect balance 
forces system remains 
equilibrium membrane potential important 
phenomenon determining long present 
patterns network analysis presented 
showing relationship 
biology hypothesis testing basically reasonably 
short period time excitatory inhibitory channels 
opened effects dependent voltage channels 
subsequently activated membrane potential settle 
new stable value reflects new balance forces 
neuron new equilibrium membrane potential 
net current return zero 
individual currents particular channels zero non values 
add zero 
net current present changes taking place 
membrane potential expected 
current mathematical derivative membrane 
potential 

useful able compute value equilibrium 
membrane potential configuration conductances 
clearly equation computing value 
relevant 
equilibrium membrane potential equation 
equation computing membrane potential 
membrane potential function depends itself 
provides updating membrane potential 
tell directly value membrane potential settle 
provided constant input neuron easily 
solve equilibrium membrane potential equation noting 
equal zero value 
change obvious 
setting equation equal 
zero solving value appears 
places equation equilibrium value 
membrane potential solve value 
longer function time 
state 

now equation directly solve equilibrium 
membrane potential fixed set inputs see 
form equation 
understood terms hypothesis testing analysis detector 
benefit analysis equation shows 
membrane potential basically weighted average 
inputs conductances obvious 
follows 
membrane potential towards driving aka reversal 
equilibrium potential channel direct 
proportion fraction current channel 
total current example lets examine simple case 
excitation drives neuron towards membrane potential 
arbitrary units leak inhibition drive 
towards lets assume 
channels same conductance level 
total 
current excitation total 
neuron move way towards excitation 

reversal potentials normalized 
values maximum conductance values channels simulated 
leabra based biologically constants 
including resting potential firing 
threshold accommodation 
hysteresis currents discussed greater detail 
note value 
range based 

finally remaining issue regarding units values 
computing simulations typically 
normalized values range 
biological values weve here normalized 
values easier common axis 
meaningful related easily probability values 
see 
shows table basic parameters 
simulations biological normalized 
values performed minimum 
dividing range 

excitatory inhibitory channels open close function 
synaptic input coming neuron leak channels 
open turns 
way computing inhibitory input neuron 
described next chapter simplifies 
algorithm capturing main functional contribution 
inhibition need consider compute 
excitatory synaptic input point 

typical cortical neuron excitatory synaptic inputs come 
synaptic channels located dendritic tree large 
structure dendrites primary input 
region neuron synaptic 
inputs onto single neuron synaptic input 
individual channels typically neuron receives inputs 
number different brain areas different groups inputs 
called projections case inputs 
different projections different parts 
dendritic tree way compute excitatory input 
sensitive level projection structure allowing different 
projections different levels overall impact neuron 
allowing differences expected activity level different 
projections case models 
automatically 

important component excitatory input model comes 
bias input likely neurons individual 
differences leak current levels differences 
biology rise 
differences biases overall level 
differences computationally important allow 
neurons effectively different 
different sense threshold discussed next 
section 
activation strong inputs units 
active closely matching weight pattern 
require weak inputs units active 
closely matching weights types 
useful necessary solving particular tasks 
important include differences model 
adapt learning order 
problem hand see further 
discussion order keep implementation simple 
actually know biological mechanism responsible 
implement bias input way artificial neural 
network models additional bias input term input 
equation specifically introduce bias weight 
determines amount bias input modified 
learning weights network 

leabra algorithm levels involved computing 
overall excitatory conductance discussed 
critical understand details 
computation come back necessary 
general points understood 
point excitation single input 
product sending activation times weight value 
inputs combined averaging 
similar standard way computing input unit 
neural network simple sum individual inputs 
divide total number inputs obtain 
average sum result 
number values represent lot 
excitation values represent excitation 
rest details ways different 
projections combined 

levels computing excitatory 
synaptic input individual input synapse 
collection inputs same area dendritic 
tree averaged 
arbitrary scaling parameters 
bias input shown property 
active conductance equal bias weight scaled 
total number inputs equivalent 
input value sum scaled inputs including bias 
taking place 

shows levels 
computing excitatory inputs individual synaptic 
channel associated input collection inputs 
same projection bias input entire set inputs 
entire tree described basic computation 
involves determining fraction inputs active time 
projection level structure affects relative 
scaling set inputs compared determining 
fraction 

step compute fraction excitatory synaptic input 
channels open input 
detailed structure 
synapse computation 
biological level summarize number channels opened 
sending unit fires function weight overall 
synaptic efficacy strength connection 
receiver assume maximum 
possible level efficacy connection represent 
weight proportion maximum number 
case model individual spikes 
simply equal weight value whenever sending unit 
fires otherwise rate code 
activations see activation 
sending unit represents firing 
frequency weight value 
expected fraction channels open time single input 

next individual synaptic conductance come 
same input projection normalized following 
way overall fraction open channels 
projection computed simply dividing sum 
individual input conductance total number 
inputs 
required convenient 
introduce scaling factors affect value 
due activity constraints imposed 
inhibitory conductances brain area represented layer 
simulation models characteristic expected activity level 
expected proportion neurons active time 
level fraction total number neurons expressed 
vary significantly layer layer 
useful inputs factor differences 
due expected activity levels otherwise layer lower 
average activity contribute lower fraction active 
channels influence neuron 
cases significantly simplifies 
simulations automatically 
automatically differences computing 
input conductance fraction simply 
divide 

projections 
now apply mechanism determining relative strength 
different projections done relative scaling 
parameter simulator associated 
projection represented 
scaling parameters 
normalized sum projections 
neuron ensures excitatory conductance 
same range relative contributions 
changing function scaling net conductance 
projection 
index goes input projections neuron 
implementing projections 

relative scaling form scaling 
useful form absolute 
scaling different inputs well example 
see network performs absence particular input 
projection changing scaling inputs 
needs absolute scaling parameter affects 
input projection question accomplished 
absolute scaling parameter 
simulator directly projection input value 
typically 
set 

necessary scale input bias weight 
otherwise large impact relative 
synaptic inputs dividing total number 
input connections seems work well bias weight 
roughly same impact normal synaptic input impact 
bias weight modified absolute scaling 
parameter relative scaling parameter effect 

finally level projection conductances simply added 
bias input overall excitatory conductance 
neuron 
total excitatory input commonly called net 
input neuron computational models net input 
includes inhibitory inputs included here 
referred text 

additional aspect way net inputs computed 
reflects general feature dendritic processing 
averaging time net input term 
rapid cause network 
otherwise propagate information reliable 
fashion time averaging reflects way dendrites respond 
inputs due part fact 
relatively large membrane surface acts 
here averaging time implemented 
time constant simulator typically 
modified version input net equation follows 

aspects structure 
dendritic tree level projection organization time 
averaging point neuron model dendritic 
effectively single point 
considerable simplification effects detailed 
structure actual neuron currents generated 
dendrites cell body 
known including 
dendrites share basic properties way 
properties dendrites basic 
effects properties potentials generated 
synaptic currents experience time 
cell body further 
turns dendrites number active 
gated voltage channels input signals way 
eliminate 
included 
simple model active channels communicate 
output spikes produced cell body see next section back 
dendrites useful learning based overall 
activity postsynaptic neuron 

researchers emphasized complex interactions 
occur inputs dendritic 
excitatory inputs occur aspects dendrites 
effects well 
demonstrated actual neurons inconsistent keep 
simple model see 
discussion simple model described 
leabra algorithm allows scaling 
averaging time otherwise assumes synaptic inputs combine 
linear fashion 

addition phenomenon temporal 
mentioned particularly relevant understanding 
model basic idea here effective 
drive neuron inputs come roughly 
same time result relatively large proportion 
open excitatory channels large excitatory input 
contrast same number excitatory inputs spread 
longer time window excitatory input 
away leak current 
add produce large current 
typically code rate leabra 
essentially detailed timing issues associated 
neural spiking mechanism temporal 
present model time 
window temporal net input time 
averaging parameter 

prior section explained neuron integrates information 
different sources reducing single value 
membrane potential section neuron 
membrane potential results 
neurons evaluation process actually simple 
neuron applies threshold sending signal 
neurons membrane potential threshold remaining 
otherwise implemented explicitly algorithm 
describe biological mechanisms rise 
threshold describe nature signal form 
spikes produced biological neurons followed 
widely approximation terms code rate 
individual spikes represents expected 
rate spiking leabra model implement 
spikes code rate latter commonly 
important property output signal 
version underlying membrane potential 
emphasizes differences right threshold 
collapsing differences relatively strong potentials finally 
discuss details synapses provide 
sending receiving neuron 

good computational practical sense neuron apply 
threshold membrane potential communicating 
neurons same reason sense run 
people see fire time 
smoke detector sense neuron 
communicate fact detected 
conditions looking threshold ensures 
significant events communicated generally 
relatively rare consistent principle sparse 
activations discussed next chapter biological level 
takes resources neuron communicate 
neurons sense important events 
takes computational resources simulated neurons 
communicate threshold faster simulations 

threshold arises biologically action voltage 
gated channels mentioned previously gated voltage 
channels open membrane potential exceeds threshold 
value open allow ions neuron 
resulting further excitation 
gated voltage channels open complementary gated voltage 
channels open act neuron result 
spike activity action potential membrane 
potential rapidly goes comes back membrane 
potential back channels tends 
resting potential slightly causes 
period following spike unable fire 
spike membrane potential back 
threshold level again period means 
effectively fixed maximum rate neuron fire spikes 

illustration thresholded activations 
membrane potential exceeds threshold 
case activation occurs spiking activation 
function spike membrane potential 
reset code rate version zero 
coded rate activation value results according equation 

detailed mathematical equations derived 
describe way 
gated voltage channels open close function membrane 
potential provide detail need model 
modeling spikes 
simple threshold mechanism results positive activation value 
membrane potential exceeds threshold zero otherwise 
see activation 
value depend spiking code rate mechanism 
described 

illustration principal aspects 
output system including axon action 
potential spike separated 
spike combination 
active properties 

spike start axon place 
called axon large concentration 
relevant gated voltage channels value 
membrane potential point threshold applied 
determining spiking neuron spike 
communicated length axon combination 
different mechanisms explicitly simulated 
model describe active 
mechanism amounts reaction chain involving same kinds 
gated voltage channels distributed length axon 
spike start axon increase membrane potential 
bit further axon resulting same spiking 
process taking place think effect 
active mechanism relatively 
speaking relatively slow requires opening 
channels neurons sections 
axon propagate spike mechanism due 
properties similar present dendrites 
propagation faster purely 
electrical process suffers 
neuron way 
active spiking mechanism signal 
called order 
axon 
covered called 
typically concerned level timing issues 
level typically ever takes 
spike axon relevant 
biological details perspective model 
important result ability rapidly 
information large number neurons 

implementation spiking process leabra simple 
spike whenever membrane potential exceeds 
threshold membrane potential reset resting sub 
level subsequent time step parameter 
spike processed receiving neurons next time step 
order simulate extended temporally effects single 
spike postsynaptic neuron extended multiple cycles 

comparison code rate version described 
simulator simple averaged time version firing 
rate call code rate equivalent activation 
called simulator 
computed period updates cycles 
follows 
number spikes time 
period total number cycles 
gain factor value better 
range rate code activations result 
ensure range 

modeling individual spikes typically 
convenient model rate spikes 
particular level excitation 
consistent simple view detector neuron data 
fire integrate model neural processing see 
discussion order 
need function takes membrane potential 
present time expected firing rate 
associated membrane potential assuming remain 
constant spiking membrane potential 
reset reflects balance inputs 
neuron computed membrane potential update equation described 
previous section simple version spike 
mechanism main factor determines spiking rate 
time takes membrane potential return threshold 
level reset previous spike see 
unable write form closed 
expression time function non 
membrane potential simulations summarized 
reasonably accurately function plus 
form 
threshold value arbitrary 
gain parameter expression means positive 
component zero negative interestingly function 
same form compute itself 
similar bayesian interpretation terms comparing 
thresholded value constant null hypothesis 
represented number see 
written 
simply 
clear relationship function 
standard sigmoidal shaped logistic function typically 
abstract neural network models difference 
presence logistic 
sigmoidal logistic function applied 
directly net input membrane potential 
simplified model neuron purposes analysis 
case extensively studied important 
mathematical properties 

comparing function actual spiking need 
take account presence noise spiking model note 
simulated spiking neuron fire spikes completely 
regular constant input see 
inconsistent fact 
detailed spiking neurons random 
difference explained part timing 
spikes coming neuron inputs discrete 
randomly constant 
expect system containing spiking units exhibit 
appropriate take additional step ensure 
code rate function properly accounts presence noise 
adding noise membrane potential 
directly adding noise processing 
processing results need averaging large 
obtain reliable effects produce modified code rate 
function directly incorporates average effect noise 
result units 
reflect expected average effects noise 

illustration point 
new function line produced adding values 
normalized gaussian centered point times 
original function solid line 

noisy activation function threshold 
written showing effects 
gaussian noise case 
noise gain standard membrane 
potentials range 

averaging noise activation function done 
distributed gaussian noise function 
activation function 
illustrated simply amounts 
shaped gaussian noise function times neighborhood 
points surrounding including point activation 
function adding 
new value point new values reflect 
probabilities neighboring points 
point noise added result operation 
shown call new function 
plus noisy noisy function 

important effects noise shape 
activation function curve 
threshold gradually curves zero starting 
threshold point original function 
important neurons graded overall activation function 
advantages gradedness discussed 
note means 
activity associated threshold sub membrane potentials 
due noise sending threshold 
effect noise reduces gain 
activation function 

average spiking rate function equilibrium 
membrane potential threshold threshold written 
constant excitatory inhibitory 
conductances compared noisy function same 
conditions note due 
effects resulting time parameters 
standard values shown 
previous figure essential form 
function obviously well captured noisy function 

now position compare noisy function 
simulation actual spiking rate results seen 
shows good overall fit 
noisy activation function simulate average 
expected effects spiking neuron averaging 
relationship reasonable assume coded rate unit 
represents effects small spiking 
neurons context scaling issues discussed 
coded rate units 
models summarizing actual neurons 
spiking neurons detailed high 
resolution simulations 

illustrates important 
characteristics output function neuron hold 
rate code discrete spiking versions function 
roughly linear small potentials threshold 
potentials higher maximum value 
large membrane potential 
due spiking case 
increasingly effects period 
potential threshold reset following 
spike property sigmoidal shaped 
functions logistic 
similar kinds suggested 
neural spiking mechanism synaptic 
effects 

important consequences sigmoidal function 
emphasizes differences variable neuron 
threshold differences well 
threshold further gain parameter 
sensitive region threshold 
thresholded membrane potential spiking 
function appears relatively large gain factor 
unit highly sensitive small values threshold 
relatively differences larger values 
effects important 
put neurons networks 

diagram synapse action potential causes 
button causing bind 
presynaptic membrane release neurotransmitter 
postsynaptic receptors 
allow ions nmda cause 
postsynaptic chemical processes take place 
produced terminal via 

complete biological picture neuron describe main 
properties synapses here directly simulated 
model underlie important features synapse 
sending neurons axon 
receiving neurons see 
diagram excitatory cortical neurons synapse 
axon buttons sending 
side dendritic receiving side inhibitory 
interneurons typically own dendrites 
reason called neurons synapses 
directly onto dendrites excitatory neurons onto 
types neurons terminal buttons 
projections interestingly information 
happens neurons via synapse chemical 
process involving release neurotransmitter 
whereas essentially electrical 

action potential coming sending neuron 
terminal button opening 
gated voltage channels bring terminal 
possibly internal 
binding neurotransmitter 
membrane terminal process fully 
understood detailed level upon binding membrane 
release synaptic excitatory 
neurons release glutamate inhibitory neurons release 
released postsynaptic 
receptors receiving neuron causes open 
allow ions enter described 
results 
chemical processes postsynaptic neuron 
receptor provides primary excitatory input via ions 
nmda receptor important learning allowing 
ions enter trigger chemical processes lead 
learning glutamate receptor 
important learning activating chemical processes 
synapses inhibitory neurons 
receptors main 
provide main inhibitory input allowing ions 
enter excitatory channels opened resulting change 
postsynaptic membrane potential called excitatory 
postsynaptic potential similarly individual 
inhibitory inputs called 

order maintain release 
gets bound membrane later new 
new important via 
released 
taken back axon terminal 
allowed 
mechanisms keep 
opening activating receptors cause problems 
affect stages process including 
receptor activation postsynaptic chemical 
processes activated receptors important studying 
components complex biological system 

synapse number properties affect 
way behaves function prior activity commonly 
noted effect 
spikes coming reasonably rapid stronger 
presumably due presence membrane 
result prior release likely extended 
high firing synaptic resources 
resulting increased numbers release 
fails released spike contribute 
neural output function 

couple important features biology synapse 
emphasized number ways 
different components synapse affect 
overall efficacy strength 
information receiver net effect 
summarized weight neurons 
modification results learning main presynaptic 
components weight number released 
action potential amount 
assumed vary efficacy 
mechanism main postsynaptic factors include total number 
channels neurotransmitter 
presynaptic release efficacy 
individual channels researchers argued 
shape postsynaptic important impact 
conductance electrical signals synapse 
whole exactly factors 
modified learning matter considerable debate 
appears multiple pre 
postsynaptic side things 

important consequence synaptic biology 
type cortical neuron typically type 
neurotransmitter turn activates particular 
types postsynaptic receptors means 
note true cortical neurons types 
specialized subcortical neurons excitatory 
inhibitory same time neurons typically provide 
excitatory inputs inhibitory inputs neurons 
artificial neural network models separation 
excitation inhibition individual units communicate 
positive negative signals biologically 
implausible least models cortex leabra model 
explicitly incorporates division basic 
properties specialized inhibitory mechanism 
provides inhibition simulating way inhibitory 
interneurons work standard units communicate 
excitation 

summary point neuron activation function 
showing activation flows sending units 
weights resulting excitatory net input 
inputs including bias weight 
excitatory input combined inhibition leak compute 
membrane potential activation thresholded 
sigmoidal function membrane potential 

now covered major components computation 
level individual neuron including computation 
excitatory inputs weighted function sending unit activity 
integration excitatory inhibitory leak forces conductances 
thresholded activation output refer 
collection equations point neuron activation 
function provide fairly accurate 
actual neuron effectively single point 
space factors detailed neural 

major steps function summarized 
note order emphasize 
essential aspects function details scaling different 
input projections represented computation 
excitatory net input gain parameter 
noise shown activation function 
simulator variable actually represents 

now proceed explore properties function 
computer simulations 

now simulator explore properties 
individual neurons simulations directory 
see read 
introduction pdp simulation environment 

start following 
chapter leabra 
bring windows large windows 
graphlog displays shows 
simulation configured consists single input unit 
single receiving unit seeing here 
single input turned again 
response receiving unit look 
units response occurs 
convenient graphlog displays information 
time allows multiple variables viewed same time 
allows multiple runs different parameters 
compared point ahead iconify 
window 

addition graphlog windows see 
windows control panel contains parameters 
actions exploration take moment 
parameters click 
parameter view brief description 

control panel buttons 
bottom 
addition purpose special buttons vary simulation 
simulation sure read 
functions buttons 
remember buttons 
control panel 
menu project window select 

plots produced simulation shown 
excitatory leak currents 
operating simple neuron conductances 
reversal potentials 
currents shown 
control panel select activation function 
parameter keep default 
time 

press button control panel plot 
current parameters see lines plotted 
time steps cycles axis note standard 
normalized parameters default switch 
biological values later 

lets focus line red 
shows net input unit starts 
rapidly remaining time steps 
goes back again recall net input 
name total excitatory input neuron net 
simulation sending unit sends 
value later 
manipulate value control 
panel control magnitude net input 
default value timing input 
controlled parameters 
control panel total number cycles controlled 
excitatory input form 

line orange shows net current 
unit expected shows excitatory current 
input comes inhibitory 
input goes line plotted own special axis 
going shown orange lines share 
axis input net red note axis 
line coded color right variable buttons 

line yellow shows membrane 
potential starts resting potential 
increases excitation decreases back rest goes 

line green shows activation 
value coded rate activation function 
results membrane potential goes roughly 
button line graph precise 
value point back again note 
activation rise net input cycles due 
time takes membrane potential reach threshold 
verify occur right 
threshold value 

line blue turned default 
simply value code rate 
activation function come 
spiking units 

now parameters control panel explore 
properties point neuron activation function 
focus controls amount excitatory 
conductance simulation proportion excitatory input 
channels open input turned 
determines overall net input value general 
interested seeing unit membrane potential reflects 
balance different inputs coming here excitation 
leak output activation responds resulting 
membrane potential 

happens increase 
press buttons see effects 
decrease 
difference neural response changes 
magnitude away initial value important aspect 
point neuron activation function 

different runs top happen 
naturally want clear log press clear 
button clear graphlog reason graph 
goes blank somehow button 
things systematically parameter range 
point membrane potential threshold recall 
normalized units noisy 
activation function soft threshold switch 
setting control panel 
hard threshold exploring parameter 

places value unit 
threshold think better way finding value 
hint remember equation equilibrium membrane 
potential particular set inputs compute exact 
value excitatory input required reach threshold showing 
note leak channels 
open input inhibition 
present here ignored 
determined empirically value hint 

play value leak conductance 
button control default 
parameters see happens increase decrease 
leak conductance 

response unit change change 
question compute exact amount leak current necessary 
put membrane potential exactly threshold 
explain results terms relationship 
leak excitation equilibrium membrane potential 
equation 

now sense unit responds different 
currents computes resulting membrane potential reflects 
balance currents explore role reversal 
potentials 

happens change leak reversal potential 
sure pay attention aspects results 
time input active conclude 
relationship resting potential leak reversal 
potential 

happens change excitatory reversal potential 
changing value places 
value essentially same 
activation value default parameters same approach 
solve exact value 
default parameters show 

point good idea conductance 
reversal potential parameters influence resulting membrane 
potential demonstrate real 
difference behavior unit 
switch normalized 
reversal potential values based biologically ones click 
button switch biological values 
bring new graphlog display click 
now membrane potential plotted scale 
larger 
scale scaling gain activation function 
identical 

look 
simulation see exactly switch normalized 
biological parameters way unit behaves simulator 
controlled called unit 
particular 
part overall project via 
menu project window select 
appear select 
side side going parameters 
top bottom easily see different values 
sense differences click 
parameter explanation 
parameter values control panel 
previous exercises small subset full 
see locate 

section explore way unit computes 
activation output main objective understand 
relationship spiking code rate activation functions 
same project previous section press 
begin 

previous section know changing level 
excitatory input affect membrane potential resulting 
activation value lets explore relationship spiking 
activation function set press 
presentation period now see caused 
spiking mechanism membrane potential 
threshold activation spikes membrane potential 
reset resting sub potential reflecting 
spiking mechanism potential back 
process itself 

spikes firing 
rate hard graph 
useful click graph line plotted blue 
shows value code rate equivalent rate spike value 
function spike train see 

try changing 
effect spiking rate compared 
default closely resulting values 
end spike train compare activations 
computed function switch 
values tell good 
approximation rate spikes produced 
actual spiking function 

details spiking coded rate 
activations explained differences parameters 
shapes functions particular spiking 
function dependent setting membrane potential 
update rate parameter 
determines fast membrane potential reach threshold 
whereas noisy function care parameter 
membrane potential equilibrium value default 
parameters simulation set slow membrane 
potential updating activation dynamics easier see 
time results step 
activation function spiking model partially 
adding noise 

important aspect spiking real neurons timing 
spikes random overall rate 
firing remains obviously evident 
single constant input far results 
regular firing introduce noise adding small 
randomly generated numbers membrane potential see 
kind effect 
multiple spiking inputs coming cell 
now note additional noise plays similar role 
noise function noisy function 
case noisy function 
incorporates averaged effects noise here actually 
adding random numbers themselves behavior 

set variance noise 
control panel otherwise 
default parameters set 
tell spike timing random looking run 
happens multiple runs top 
look overlapping runs determine 
spike timing random regular 
hint try lower values noise compare 
exhibits greater variability measured 
number different possible multiple runs 
stimulus train detailed spike timing 
explain result 

interesting understand relatively small amount 
variability introduced membrane potential noise 
variance produce relatively amounts 
spike timing way see noise 
membrane potential membrane potential plots 
last spike noise membrane potential point 
constant value rest noise 
slightly small causes large effects 
random spike timing threshold string small 
membrane potential delay spiking 
string 
threshold effectively small differences 
large distinction small values 
membrane potential 

now lets return explore 
properties noisy code rate activation function compared 
possible functions well comparing 
noisy non version noisy 
linear function 
difference membrane potential output 

change excitatory input 
curves functions different 
noisy compared 
happens increase note 
activations fit range see 
curve stops 
gradually relationship noisy 
vary function level excitation 

activation obvious 
observed indication natural 
property functions present 
linear function functions approximate linear 
function lower levels excitation 

done simulation completely 
simulator window selecting 
project locate project window select 

explored basic equations point neuron 
activation function now explore basic function 
neuron detecting input patterns see particular 
pattern weights simulated neuron respond input 
patterns level 
neuron neuron respond pattern best 
fits weights graded manner patterns 
close weight pattern provides insight 
point neuron activation function works way 

previous simulation start 
following 
chapter leabra 
previous project open 
project window 

project find main control panel 
press button 
window comes enviroview window shows 
different events environment 
presented unit order measure detection responses 
patterns presented units later networks 
units contained environment 
environment real world provides external inputs 
individual pattern contained event 
represents distinct possible state environment 
see environment case digits 
represented simple grid pixels grid picture 
elements pixel event digit drive 
corresponding input unit provides synaptic input simulated 
neuron 

now press control panel bring 
window showing network containing input 
event activations unit receives 
inputs sure 
operation view 
pattern weights synaptic strengths receiving unit 
input idea unit 
detect press button lower 
hand left side window 
scroll bar find bottom list 
click receiving unit single unit 
input grid now see input grid 
pattern weight pattern receiving unit 
connections input units weight value displayed 
corresponding sending input unit 

weight pattern determine extent input 
patterns activate receiving unit see action 
present different input patterns determine 
unit responds weights locate 
control panel window upper right 
screen epoch process 
presentation events environment 
network sure 
operation process control panel 

press button process 
start event press button 
process event environment digit 
presents input pattern onto input layer 
network setting units directly corresponding values 
event pattern updates activations network 
equilibrium activations effectively 
reasonable reached units see 
process known 
settling viewing final results settling 
network window viewing 
weights network window see resulting 
activation values noted weights remain 
constant events need view activations 
network window select button lower left 
hand part window scroll back 
act top see pattern digit 
input layer clamped pattern event 
representing digit note receiving unit shows activity 
value getting 
lets proceed remaining digits observe 
responds inputs continue press 
digit presented 

seen receiving unit activated digit 
presented activation zero digits 
expected receiving unit acts detector 
pattern responses reflected shows 
activation unit function event digit number 
axis 

now lets try understand exactly unit responds 
key understand relationship 
pattern weights input pattern 
display weights current input pattern 
clicking button 
already selected sure select 
button hold key left button 
select receiving unit already selected 
case click 
click now see unit display 
divided left activation 
right weight value note activations 
provided environment maximum 
tell difference weights 
activations now epoch process 
followed 

digit report number input units 
weight input unit active 
easily display find 
variability numbers digits 
activation value receiving unit reflect 
variability better variable examine order 
view underlying variability 

now click variable graphlog window 
general relationship plot variable 
numbers computed previous question 
equations explain exactly 
net input computed results values plotted 
graph digit remember click line 
graph obtain exact values order 
work verify couple digits recall 
default case need know 
input layer projection set 
ignore scaling parameters 
set 
projection anyway bias weights 
ignored ignore averaging time 
looking values settling 

result working now 
detailed understanding net excitatory input 
neuron reflects degree match input pattern 
weights observed activation value ignore 
graded information present input signal now 
explore change information 
activation signal locate 
value leak current set sufficient 
strength excitatory inputs 
strongest best input patterns 

happens pattern receiving unit activity 
reduce value note 
hit button control panel apply 
new value run epoch process 
digits happens values 
explain effect changing 
terms point neuron activation function 
consequences different response patterns 
units output receiving unit 
try possible advantages 
higher lower values 

clearly important difference neuron 
inputs tradeoffs associated different 
levels brain kind problem 
neurons code input neurons 
high threshold low threshold 
types providing corresponding advantages 
specificity response bias weights 
important parameter determining behavior 
see next chapter value leak 
current partially inhibitory 
input plays important role providing dynamically 
level inhibition excitatory net 
input ensures neurons generally right 
range useful information 
neurons dependent neurons 
important consequences imagine explorations 

note section contains abstract mathematical ideas 
essential understanding subsequent 
depth 
perspective 

primary ways expressing computational level 
description via mathematical 
manipulations turns mathematical language 
probability statistics particularly appropriate describing 
behavior individual neurons detectors relevant parts 
language introduced here see provide 
interesting explanation basic form point neuron 
activation function described 

particular statistics relevant here 
hypothesis testing general idea 
hypothesis relevant data 
evidence want determine well hypothesis 
supported data provides language 
same basic operation detector performs data input 
processing performed detector hypothesis 
thing things detector detects likely 
present data identify important 
hypotheses detector hypothesis detected thing 
null 
hypothesis thing labeled 
want compare relative probabilities 
hypotheses true produce output reflects 
extent detection hypothesis null 
hypothesis current input 

result come detailed 
follows probability current input 
data written simple function 
functions relationship hypotheses 
data written here 
resulting probability function strong 
support detection hypothesis null hypothesis 
function familiar psychologists 
choice mathematical 
psychology models number years well explore 
functional form onto equilibrium membrane potential equation 
already see 
same weighted average quality 

way putting objective analysis 
important issues want evaluate extent 
believe hypotheses 
important 
possible actually measure 
objective probabilities hypothesis true 
particular set data typically impossible number 
reasons settle subjective definition 
probability refers objective fact 
case probabilities simply numbers 
means happens true 
means happens true 
intermediate values mean 
meaning happens time 
average getting 
intermediate probabilities correspond intermediate 
values value means true 
distinction 
fully time 
important ultimately concerned here 
valued real averaged time numbers consistent 

simple vertical line detector detects presence 
vertical line amounts inputs active 
input assumed 
inputs driven way visual signals 
vertical line visual inputs inputs 
tend light system noisy inputs 
active inactive 

possible states world vertical 
line detector frequency number times 
state occurs world hypothesis line 
exists world null hypothesis 
exist numbers inputs data 
frequencies show states inputs active 
likely hypothesis true versa vice bottom 
line contains total number states 
computing probabilities 

purposes 
hypothesis testing framework simple detector example 
shown detector receives 
inputs sources assume driven world 
vertical line present detectors likely 
activated hypothesis represented detector 
vertical line actually present world represent 
hypothesis variable hypothesis 
true null hypothesis vertical line 
present world represented 
states 

order compute objective based frequency probabilities 
opposed subjective probabilities example need table 
states world frequencies occurring 
shows table states 
define world purposes example state 
consists values variables world 
hypotheses data inputs 
frequency associated state indicates 
times state actually occurs world note states 
frequency meaning occur 
world simple able include possible states here 
note hypotheses mutually 
true same time 
essentially same 

table shown compute 
probability occurrence interested 
simply number times occurs 
table dividing total number states 
table compute probabilities needed 
hypothesis testing clear objective basis relevant 
equations inputs present 
table 
large due huge number different unique combinations input 
states example inputs binary actually 
true neurons worse table requires 
inputs hypotheses 
roughly inputs 
result typical inputs 
cortical neuron main reason need develop 
subjective ways computing probability terms 
applied understanding realistic neuron 

relevant probabilities computed 
table 
data 

basic probabilities interested 
computed directly world state table 
overall probability hypothesis true 
written short found 
finding states adding 
corresponding frequencies dividing result total 
frequency count states computation illustrated 
result 
next probability current input data 
receiving inputs present time compute 
need pick particular data state lets choose 
data case inputs 
active shows probability 
data state short 
occurs time hypothesis 
times true world finally 
need know times hypothesis true data 
present obviously relevant indicates 
predictive data hypothesis true called 
joint probability hypothesis data 
written data 
shows 

detector primarily interested predictive data 
hypothesis true gets inputs know 
clearly 
interested joint probability hypothesis 
data right information 
input data tend think 
hypothesis likely true data 
problem properly 
space computing probabilities joint 
probability tells occur compared 
possible states want know 
hypothesis true receive particular input data 
conditional probability 
hypothesis data written 
defined follows 
example data want know 
matching 
tells inputs active indicates 
chance level hypothesis 
vertical line present true basic information 
well correlated input data hypothesis comes 
joint probability 
critical restricting space events table 
considered computing probability subset 
entire table dividing 
now total subset table 
information appropriate context cases particular 
input data actually occurred 

basic equation 
want detector solve turns way 
subjective probabilities require computing kind 
conditional probability called likelihood 
likelihood opposite conditional probability 
conditional probability 
data hypothesis 
bit think computing probability 
data 
inputs experiment based hypothesis 
thing sure perfect sense 
think likely 
data based assumptions hypothesis 
words likelihood simply computes well data fits 
hypothesis comes same 
joint probability hypothesis data 
different way time scope cases 
hypothesis true determine fraction 
total particular input data state 
expect receive 
data time hypothesis true tells 
likely predict getting data 
hypothesis true 

straightforward 
world state table neurons 
explained find way 
computed 
way key step likelihood terms 
computed directly input data itself reference 
objective probabilities table effect 
idea easily measure 
hypothesis implemented neuron simply way 
inputs integrated weighted definition 
hypothesis place probabilities 
subjective frequencies 
events world mathematical 
manipulations probabilities consistent self 
ultimately 

computing likelihood integrating weighted 
input values inputs connected detector 
weight integrated 
resulting likelihood value 

shows likelihood value 
computed function integration 
different input values simple example input 
equal weights important 
model real neurons relationship objective 
based frequency probabilities complicated 
effectively ignored here setting total 
likelihood ends appropriately normalized sum total 
input 
weights activities 
inputs equation came 
place constructed equation 
produce same likelihood values compute 
world state table 
now good time verify 
equation produce same likelihood values 
cases computed world state table 

course likelihood equation 
consistent actual 
based frequency probabilities impossible didnt 
world state table exactly situation now 
considering kind simple 
assumption relationship input data weights 
resulting likelihood value typically assumption 
operation 
likelihood term particularly kind direct 
computation inputs relatively easy define 
likely particular input 
generated neuron configured particular set 
weights 

assuming likelihood function now need 
figure way write terms 
likelihood functions following steps take 
note definition likelihood 
new way expressing 
joint probability term appears 
back 

last equation known formula provides 
starting point whole field known bayesian statistics 
allows write called posterior 
bayesian terminology terms likelihood times 
prior called prior basically 
indicates likely hypothesis true seen 
data hypotheses plausible true 
reflected term 
favor simpler hypotheses likely 
necessary application here prior terms 
end constants actually measured least 
underlying biology 

terms normalized probability data 
turns turn term 
involving likelihood terms 
null hypothesis again want likelihood terms 
relatively simple equations 
hypothesis null 
hypothesis mutually 
hypotheses considering write 
probability data terms part 
hypothesis plus part null 
hypothesis 
amounts computing 
top bottom separately 
adding results overall result 

formula joint probabilities 
turned conditional probabilities simple 
conditional probability definition 
following 
formula resulting 
now expression easily terms 
hypotheses 
equation showed 

simple 
form reflects 
likelihood favor hypothesis 
form biological properties neuron implement 

general null likelihood serves 
strength likelihood 
term reasonable overall probability term 
world state table simple example 
null likelihood computed likelihood 
computation 
turns neural equivalent null 
likelihood constant try 
simple example function activity 
neurons communicated via inhibitory inputs 
computations frequency 
table nonetheless reasonable serve 
important computational roles described subsequent 
chapters 

now position compare equation equilibrium 
membrane potential 
hypothesis testing function 
developed reference 
equilibrium membrane potential equation here 

general idea excitatory input plays role 
likelihood support hypothesis inhibitory 
input leak current play role support 
null hypotheses considered null 
hypothesis analysis easy extend 
ignore leak current time 
inhibitory input play role null hypothesis 

now order compare biological equation hypothesis 
testing equation need appropriate values reversal 
potentials resulting membrane potential same 
range probabilities assume excitatory 
input drives potential towards 
inhibitory leak currents drive potential towards 
sense considering complete support 
excitation hypothesis result 
probability complete absence support 
excitation leak inhibition result probability 
values biological equation 
following relationship 

particular equations identical following 
excitation hypothesis neuron 
detecting inhibition null hypothesis assumed 
fraction channels open likelihood value 
excitation 
inhibition essentially assumed already 
computing likelihood function sending activations 
times weights baseline conductance levels 
represent prior probability values 
respectively provides satisfying 
level computational interpretation biological mechanism 
neuron integrating information way 
good statistical sense true 
actual values relevant biological parameters 
scale apparent relationship probabilities 
important thing form equation balance 
excitatory detection hypothesis inhibitory null hypothesis 
forces due form applied linear 
scaling values anyway 

finally full equation leak current 
reflecting case 
different independent null hypotheses 
represented inhibition leak see detail 
inhibition dynamic null hypothesis 
changes function activation units 
network leak constant null hypothesis sets basic 
minimum standard detection hypothesis compared 

far simple view neuron 
detector shown consistent biological 
properties mathematical description neural function 
based hypothesis testing detector model 
importance graded processing learning provide basis 
thinking neuron performs relatively simple task 
idea huge amount biological 
complexity present neuron amount information 
processing power neuron potentially exhibit 
example dendrites neuron potentially perform complex 
processing neural inputs including 
further sequence output spikes neuron 
potentially convey huge amount information varying timing 
spikes systematic ways individual neuron 
complex relatively simple detector 

number fundamental problems idea 
complex processing neural level 

learning requires graded response order 
bootstrap changes described introduction later 
neuron viewed 
performing lots discrete communicating via 
precise spike timing 
changes learning likely 
robust powerful learning algorithm neurons 
learning mechanism 
difficult organize networks neurons perform 
effectively 
brain robust noise damage 
example constant brain results 
significant movement neural undoubtedly 
kinds noise processing known 
sources noise due biological 
mechanisms neurotransmitter release addition 
substantial effects detailed firing properties 
individual neurons effect cognition graded 
catastrophic further known high levels 
damage neurons sustained 
effects cognition individual 
neuron contributing relatively simple way 
cognition 
neuron receives inputs neurons 
sends output signal computing 
detailed inputs attention 
detailed spike timing individual inputs complexity 
difficulty organizing large number 
further complexity detail 
somehow reduced single output signal end 
provide fraction total input 
neurons clear point complex 
processing end 
large number biological properties neurons 
consistent fire integrate model neuron 
detector hypothesis idea 
detailed spike timing important 
further neurons tend respond graded way 
noisy versions stimuli consistent detailed 
tend neural response 
sensitive specific appears 

finally bottom line able model wide range 
cognitive phenomena simple style detector neurons 
additional complexity appear necessary point 

note section mechanisms described 
applicable limited range phenomena 
active default simulations reader 
come back later find need 
mechanisms 

addition integration evaluation functions 
basic point neuron activation function neurons 
complex activation dynamics enable neuron 
way responds subsequent inputs function prior 
activation history thought form 
regulation self neurons response 
simulations explore later complex dynamics enter 
picture operate longer time 
typically typically interested initial 
activation state produced response input pattern 
complex longer term aspects response 
simulations ignore additional dynamics assume 
additional dynamics present 
simulations run settling long observe effects 

forms regulation self known 
accommodation hysteresis accommodation causes neuron 
active 
active same amount excitatory input hysteresis 
causes neuron active remain active period 
time excitatory input 
forces otherwise 
resolved hysteresis operate 
time period neurons active term short 
tendency remain active hysteresis 
active longer accommodation term longer accommodation 
results tendency network switch different 
interpretation input pattern see subsequent 
explorations 

addition hysteresis accommodation dynamics 
term longer threshold adaptation mechanism ensures 
neuron constantly active active 
neuron constantly active value activation threshold 
increased likely able remain active 
conversely threshold goes neuron active 
time threshold adaptation active neuron 
essentially accommodation adaptation active 
neuron hysteresis form 
sensitization active neuron gets increasingly 
inputs likely active 

details processes following 
sections 

potential biological accommodation 
hysteresis accommodation arise effects potassium 
channels activated response membrane 
potentials voltage gated channels increased concentrations 
calcium ions neuron result 
sustained activation gated channels increased 
current effectively larger leak current 
membrane potential strongly back towards rest 
accommodation arise lasting long effects certain 
types inhibitory channels hysteresis arise 
gated voltage channels open neural membrane 
potential sustained level recall 
results excitation neuron 
attempt include detailed biological mechanisms 
model adopt simple understand easy 
implementation dynamics same basic equations 
different parameters accommodation hysteresis 

delayed effects accommodation hysteresis 
accomplished basis variable 
represents activation pressure relevant 
channels basis variables updated function 
activation value neuron according following function 
basis value time average activation state 
time constant difference accommodation 
hysteresis time constant faster hysteresis 
typically accommodation typically 

actual activation channel function basis 
variable basis variable gets activation 
threshold value conductance channel begins 
increase different time constant basis 
lower threshold value 
conductance decreases again 

finally conductances accommodation hysteresis 
added conductances point neuron 
equations contribute overall current experienced 
neuron see 
accommodation conductance computed 
similarly hysteresis 
conductance computed same equation different 
parameters default values reversal potentials 
channels shown 

open project 
directory looks pretty 
simulation explored earlier plots variables 
basis conductance values accommodation hysteresis 
contains parameters control self 
channels control panel 

locate overall control panel note 
bottom parameters 
stimuli press button observe 
activation result accommodation hysteresis 
turned expect cycles 
lasting note particular 
apparent effect prior activation 
response latter 

now lets turn accommodation click button 
control panel graph log window click 
variable accommodation net conductance 
accommodation displayed graph press 
increases unit active 
reach activation threshold set see 
field control panel 

unit needs remain active bit longer order 
basis variable point activation way 
achieve set time 
stimulus later set 
press again now 
observe accommodation conductance starts 
increase stimulus goes unit 
inactive takes basis variable decrease 
threshold 
field takes long time next input 
stimulus comes cycles strong accommodation 
current unit activated input 
cycles basis variable finally goes 
threshold starts decrease 
unit active 

unit network units received input 
stimulus activated 
stimulus units active immediately 
produce different 
representation input accommodation provides means 
network respond subsequent inputs based 
prior activity 

point explore parameters 
play determines strong overall 
accommodation current try see 
weak completely unit 
done exploring press button return 
default parameters 

now lets add hysteresis click 
buttons graph log click 
button field 
see unit remains active time input 
stimulus goes back cycles 
additional period activation causes accommodation current 
activated eventually turns unit 
result accommodation saw unit 
activated immediately input now play 
parameters see units response 
properties 

summary see considerable potential 
complex dynamics emerge interactions different 
channels fact actual neurons channels 
complex dynamics suggests basic point neuron model 
lot simpler real thing evolution 
time dynamics 
considerable 
typically ignore complexity 
difference aspects 
behavior modeling simulations later text 
away 

changing balance excitatory inhibitory currents 
coming neuron accommodation hysteresis 
change threshold activation threshold adaptation 
useful units participate 
overall representational scheme network 
network algorithms known well model 
depend large degree 
unit active roughly same percentage time 
leabra model brain depend strongly 
idea basic idea force units 
represent different aspects input environment 
force active roughly same 
number different input patterns unit tend 
focus different subset patterns whole space 
covered 

shown powerful computational idea 
important limitations assumes 
inputs different relevant types distributed 
time likely true real world 
basically type model predict went 
novel environment 
ability represent back 
units active 
represent present current environment 
order amount activity 

view threshold adaptation 
upper lower activation frequency precise 
target value addition themselves wide 
strength typically 
weak implementation threshold adaptation 
depends average running activation value 
time constant computing average typically 
small average takes account activity long 
time period equation applied end 
settling applicable activations different 
input patterns 

average activity value unit exceeds wide lower 
upper units activation threshold 
appears constant modified 

maximum threshold value typically 
unit active 
level typically 
minimum threshold value typically unit 
active lower typically 
average activation 
threshold back standard value 
typically rate threshold 
typically actual simulations specify 
single parameter 
simulator standard 
threshold 

open project 
threshold adaptation process control panel press 
start due excitatory input activation average 
orange line increases value 
point threshold yellow line increases units activity 
point average threshold 
again slightly allowing unit active again 
repeating cycle continue 
result units average activity substantially 

now play parameters see kind 
effects threshold adaptation process 

biological functional properties neuron consistent 
detector constantly 
information available looking conditions match 
specialized detect view neural 
function different standard serial computer 
serves basis comparison whereas standard computers 
relatively general purpose computational neuron 
relatively specialized dedicated detecting 
particular set things refer things neuron 
detects representation emphasize individual 
neurons representations typically difficult describe 
simple verbal terms smoke smoke detector 
neurons exist huge numbers operate parallel 
whereas standard computer operates serial performing 
operation time good reasons think neurons 
perform relatively simple computation arguments 
mathematical description neuron detector 
framework bayesian statistical hypothesis testing 
produces same form mathematical activation function 
actually neurons learning shapes neurons 
detect different things plays critical role sure 
detectors working parallel accomplish sensible 
particular task solve sum detector 
model neuron provides good intuitive model 
function help sense underlying biological 
properties 

detectors neurons need receive combine information 
number different input sources relevant biological 
consists channels synapses 
contain particular types channels allow atoms 
ions neuron different ions 
different concentrations neuron 
leads generation electrical current 
channels open allow ions flow concentration 
cell neurons excited 
ions enter cell 
synaptic channels receiving areas called dendrites 
synaptic channels opened neurotransmitter known 
glutamate released sending 
presynaptic neuron different inputs provide different amounts 
activation depending neurotransmitter released 
channels postsynaptic 
receiving neuron open result different synaptic 
synaptic strengths different inputs 
refer weights critical determining 
neuron detects contrast excitation neurons 
ions enter 
neuron channels opened neurotransmitter 
released inhibitory interneurons 
basic negative current caused positive ions potassium 
leaving neuron via leak channels 
open simple equation way overall 
electrical voltage cell known membrane potential 
integrates currents valued real number 
equation same form derived principles 
based computational level detector model neuron 
point neuron activation function leabra algorithm 

detector integrated information evaluate 
evidence strong conclude 
detected communicate 
care membrane potential results 
integration turn determines neuron produce 
action potential spike causes neurotransmitter 
released ends neurons sending projection 
axon form synapses onto neurons dendrites 
see action potential thresholded meaning 
occurs membrane potential start axon 
called axon gets certain critical value 
called threshold means neurons 
communicate detected level 
biologically computationally 
leabra simulate individual action potentials typically 
rate code valued real number 
represents frequency rate cell 
produce spikes based membrane potential 
spiking rate units activation value 
called activity output unit useful 
small simulations 
spike timing causes problems present 
larger networks averaging number units reduce 
impact noise rate code intended represent 
scaling assumption individual units model 
represent number roughly similar neurons average impact 
spiking neurons rate code 
leabra spike rate function initially roughly linear 
threshold 
approaches maximum firing rate 
biologically determined number factors including 
period spike essentially 
impossible fire rate synapses 
release neurotransmitter activation function 
important computational power neural networks 
producing stable activation states interactive networks 

neurons number self channels affect 
way neuron responds based prior history activity 
different mechanisms included leabra 
essential aspects algorithm 
simulations accommodation causes neuron 
active 
active same amount excitatory input 
hysteresis causes neuron active remain 
active period time excitatory input 
obviously accommodation 
typically operates time period finally 
term longer threshold adaptation mechanism implemented 
way ensure neuron constantly 
active active way 
active neurons similar simple accommodation mechanism 
capacity neuron sensitive 
sensitization active recently 

neuron 

neuron provides basic unit processing 
network neurons required accomplish 
simple tasks able describe essential 
computation performed neuron terms detector model 
simple computational metaphor applies 
computation performed entire network adopt 
approach understanding networks work 
chapter identify explore important principles 
general behavior networks next 
chapter show learning responsible setting 
detailed weight values specify unit detects shape 
behavior networks according principles build 
upon developed chapter 

begin summary general structure patterns 
connectivity cortex neocortex 
biological basis general types networks 
cortical areas 
terms neuron types general patterns connectivity 
rise generic cortical network structure 
modeling kinds different psychological 
phenomena explained previous chapter 
excitation inhibition 
separated cortex implemented different types 
neurons different patterns connectivity separation 
useful understanding basic principles network 
function unidirectional feedforward processing 
information via excitatory interactions performs information 
processing transformations essential cognition 
bidirectional connectivity advantages unidirectional 
connectivity cortex requires 
inhibition control positive excitatory feedback effects 
cortical inhibition summarized inhibitory 
function explore functional case 
excitation context inhibition end 
clear forms interaction separable 
overall network behavior possible summarize 
overall effects interactions terms constraint 
satisfaction networks achieve state activation 
simultaneously satisfaction external constraints 
environment internal constraints patterns weights 
connecting neurons 

cortex neocortex forms part 
brain humans relative 
data idea 
cognition takes place important remember 
cortex depends critically subcortical brain 
areas proper functioning cortex divided 
number different cortical areas specialized 
different kinds processing areas critical 
recognizing objects appear process spatial information 
areas perform language processing higher level planning 
etc able single algorithm writing book 
large part due fact cortex fairly consistent 
general structure applies different cortical 
areas specialized processing takes place 
similar general network structure simulated 
common computational framework properties structure 
section 

detailed description biological properties cortex 
entire details considerably 
here general neurons 
identified cortex excitatory neurons release 
excitatory neurotransmitter glutamate inhibitory 
neurons release inhibitory neurotransmitter 
see details 
primary excitatory neurons 
neurons larger number 
different inhibitory neurons 
excitatory neurons 
constitute roughly total number neurons cortex 
apparently responsible information 
flow form networks range longer projections 
different areas cortex subcortical areas 
following discussion connectivity focused 
excitatory neurons contrast inhibitory neurons project 
small areas cortex consistent 
role providing local inhibitory feedback mechanism 
see 

cortical neurons organized distinct layers 
cortex neuron types found layers 
layer place 
neurons found neurons typically found 
layers cortex identified anatomical 
important understanding detailed biology 
cortex purposes picture 
considering functional layers input 
hidden output layers information typically 
order see 
illustration term layer refer 
functional layers term cortical layer 
based biologically layers input layer corresponds 
cortical layer receives sensory input way subcortical 
brain area called thalamus information 
retina sense output layer 
corresponds cortical layers sends motor 
outputs wide range subcortical areas including basal 
ganglia 

simple layer interpretation cortical 
structure consistent general connectivity patterns 
provides useful starting point modeling direct 
excitatory connectivity shown open 
connections inhibitory interneurons indicated 
connections operate cortical layer 
receive same types excitatory connections excitatory 
neurons lines indicate connections exist 
consistent flow information input 
hidden output limited data difficult determine 
important connections 

transformation input output mediated hidden 
layer directly visible connected 
cortical non input output areas functional layer 
corresponds upper cortical layers 
cortical layer largely hidden layer 
thought internal model environment 
provides useful typically elaborated processed basis 
driving outputs network function inputs 
example hidden layer internal model represents 
according categories red orange yellow green etc 
continuous representation provided visual 
sensory inputs categories relevant 
networks output color words provide useful 
basis outputs raw input itself goes 
cognition thought terms 
elaborate internal models next chapter find 
models developed learning 
useful 

essentially same functional cortical layers 
suggested long supported 
different types data anatomical connectivity 
different areas suggests 
information coming input layer next 
primarily hidden layer output layer see 
addition firing 
properties neurons different cortical layers shows 
hidden output layer neurons complex responses 
input layers 

view cortex presented 
considerably simplified order focus distinctive 
structure reality cortical areas process 
sensory input same ones produce motor output 
large number areas direct sensory 
input direct motor output 
presents accurate picture structure main 
different kinds cortical areas correspond 
functional layer types input hidden output 
different types areas emphasizes corresponding 
functional layers input areas well developed cortical 
layer including different layers sub layer primary 
visual input area output areas well developed 
output layers hidden areas reduced input 
output layers explore ideas function 
later chapters 

larger scale version cortical showing 
different types cortical areas 
input area developed well layer receiving 
sensory input thalamus producing motor output 
directly hidden area called 
level higher association area receives input input 
areas hidden areas sends outputs output areas 
hidden areas reduced input output layers 
primarily via layer connectivity 
output area motor control area 
subcortical brain areas drive motor system 
real layer larger layer lines indicate layers 
connections reduced importance area 
lines again represent connections exist 
consistent output hidden input model 
information flow 

picture view cortical structure 
sensible information comes network specialized 
areas layers processed potentially long sequence 
internal processing areas hidden layers results 
output drive motor system add important 
complexity otherwise simple picture 
excitatory connections bidirectional 
cortex cortical layer number 
inhibitory neurons function discussed 
importance features clear finally 
possible role connectivity thalamus 
subcortical structures hidden areas 
simple view discussed later chapters 

previous section showed cortex excitatory 
interactions clearly primary importance section 
explore basic types computations networks neurons 
excitatory interactions perform start 
simpler case unidirectional feedforward 
connections rare cortex basic 
computations generalize bidirectionally connected case 
certain brain areas play important role 
relevant hippocampus see 
transformations possible 
type connectivity basic cognition 

digit network input layer digit 
images hidden layer units represent different 
digits 

already covered computation performed 
unidirectional information 
forward direction feedforward network 
discussed role neuron detector 
explored simulations 
basically feedforward 
connectivity allows sequences detectors process 
input signal detecting different patterns 
function weight values connections case 
explored input signal 
consisting images different digits grid 
unit detects extent input 
digit looks imagine multiple units 
layer hidden layer containing detectors 
different digits image digit 
presented unit represents digit activated 
digit network 
discussion explorations 

order understand nature transformation performed 
hidden units digit network layers detector units 
generally need develop terminology describing 
differences input representations hidden 
representations input layer activated pattern 
looks natural assume input 
layer represents digit sense 
representation specific digit 
same input layer different activity pattern 
represent digit image entirely 
hidden unit set weights 
specifically configured detect pattern 
seems different type representation 
provided input difference specificity 
critical understanding benefits 
detectors hidden layer 

specifically say unidirectional flow 
information digit image input layer hidden layer 
results transformation image 
specific representation hidden layer emphasizes 
distinctions images different digits 
distinct categories same time 
collapsing differences category treating 
images same digit same imagine 
whole subset input patterns correspond digit 
digits hidden unit 
representing digit groups images 
same category 
images important try say 
way transformations emphasize distinctions 
distinctions relevant 
items represented distinctions digits case 
digit network emphasized distinctions 
relevant distinctions different instances same 
digit 

pretty easy effects 
transformation performed single unit plot 
response unit input patterns 
exercises 
considerably difficult entire hidden layer 
units useful cluster 
plot similarities patterns activity 
hidden layer cluster plot groups patterns 
groups patterns similar similarity 
typically measured distance recall 
computed 
distance points measured dimensions 
easily generalized number dimensions 
units present hidden layer adding squared difference terms 
dimension cluster plot based 
distance matrix shows 
patterns visual form cluster plot similarity 
information easy see example clusters 
plot similarity information captured 
underlying representations 

problem cluster plot similarity relationships 
different patterns indirect measure 
transformations performed individual units 
necessary reduce high 
hidden unit representation somehow meaning information 
bit time 
learning certain features cluster plot arise 
underlying properties individual unit transformations 

cluster plots feedforward 
transformation digit images digit categories 
shows cluster plot input digit images shows cluster 
plot hidden layer digit detectors shown images 
complex patterns digit similarity 
separated digit category labels specifically represent 
digit note items cluster 

example cluster plot simple case digit network 
hidden unit exactly representing digit shown 
compared cluster 
plot input images themselves shown 
note axis plots 
index different patterns axis shows 
distance distance items cluster 
length horizontal line coming common vertical 
line axis note axis scaled 
sure look actual values assume 
different plots same length 
particular plot hidden units 
distance away squared sum 
distance put 
single cluster contrast complex pattern 
similarities input layer reflects amounts 
overlap shared pixels images 

sketch summarizing results previous figure 
showing transformation overlapping patterns digit 
representations input layer specific 
categorical digit representations hidden layer 
circle input represents collection pixels 
digit image overlap indicating similarity 
produced shared pixels hidden layer representations 
overlap digits clearly specifically 
represented 

order interpret plots terms transformation 
input patterns need clear idea 
transformation accomplish case lets say want 
transformation digit completely separate 
distinct put way want hidden unit 
specifically represent single digit digits 
similarity relationships digits want 
essentially similarity structure digits representation 
equally distinct exactly see 
course actually 
kind transformation want least fairly 
transformation input patterns take 
issue types transformations generally better 
following section sketch 
transformation shown 
intended roughly capture similarity structure shown 
cluster plots function overlap 
representing individual digit images accurately 
impossible dimensions 

cluster plots digits different 
images digit shows cluster plot noisy 
digit images shows cluster plot hidden layer digit 
detectors shown images note length zero 
lines digit clusters sub here indicates 
exactly same pattern zero distance 
input represents image distinct pattern 
digit hidden layer 
differences digit category 
distinctions categories 

transformations defined emphasize 
collapse important benefit type 
representation shown previous able 
tell different images digit somehow 
equivalent beneficial 
digit looking purposes mathematical 
details color size 
particular digit example shown 
different 
versions digit image original noisy versions 
input pattern image same digit 
different digit different hidden layer 
versions same digit 
length zero horizontal lines labels right vertical 
cluster line cluster plot indicate 
cluster identical identical distinct versions 
different digits now simple way 
different versions digit cluster 
seems pretty transformation 
clusters 
simple example kinds transformations 
actually useful adding bit complexity steps 
transformations result powerful processing 

cluster plots letters digit 
network shows cluster plot input letter images 
shows cluster plot hidden layer digit detectors 
shown images hidden layer units activated 
letter inputs representations overlap 
meaning hidden layer distinction 
patterns 

good example specificity transformation 
tendency collapse irrelevant distinctions seen 
presenting images entirely different things digit network 
example presented images letters network 
expect sensible hidden layer representation 
contrast based image input representation simple grid 
perfectly capable representing letters clear 
categorical distinctions 
illustrates phenomenon showing digit hidden layer 
letters case 
collapsing happens hidden units specifically 
configured detect digits simply respond 
letters exception hidden unit digit 
responding sufficiently similar letter resulting 
single cluster hidden unit activity 
things done problem units 
respond easily distinctions remain 
due fact weights specifically tuned 
letter patterns distinguish 

process patterns activation central 
cognition view cortical networks 
sensory information ways produce specific 
representations relevant distinctions necessary 
survival collapsing irrelevant ones 
aspect unidirectional connectivity 
bidirectional case see later 

now explore ideas presented start project 
chapter leabra 
see network digits network window called 
display activity states 
network time 
panel visible addition standard pdp 
project windows see information 

lets examine network looks 
layer 
digit images layer 
hidden units representing digit select button 
lower left window need scroll click 
different hidden units see weights 
exactly match images digits units represent 
units referred matching units 
respond proportion match input 
specific weight weights developed learning 
mechanisms described typically 
specific single patterns resulting distributed 
specific hidden unit representations individual units 
participate representation multiple different input patterns 
important issue next section 

pretty obvious unit respond 
digit input patterns lets explore nonetheless lets 
view unit activities network selecting 
button next locate process control panel 
overall control panel hit 
button followed button presents 
input pattern network updates activations 
network equilibrium activations effectively reached 
units see proceed 
activities net inputs hidden 
units digits now display 
press button control 
panel select run digits 
essentially same pressing process 
control panel allows select input patterns 
present observed unit activated 
matching digit presented digits 
presented 

continue important understand role 
bias weights simulation digit images input 
patterns different numbers active units 
observed detector exercise previous chapter 
press control panel select 
different net input activation levels 
corresponding hidden units activations shown 
appear roughly similar here 
bias weights 
differences overall activity level coming inputs see 
select window see 
pattern different values see effect press 
hit 

affect hidden unit activities now 
activations consistent number active units 
input patterns turn bias weights back 
window isnt already selected explain bias weights 
contribute producing originally observed hidden unit 
activities 

run network biases again now 
cluster plots shown hit button 
window containing cluster plot similarity 
relationships digit images shown 
compare amount overlap 
activated pixels digit images click 
control panel select cluster plot 
results iconify events window done 
window iconify button window 

now again selecting 
time cluster plot shown 
note 
values greater 
hidden unit activation values purposes 
small differences activation values 
units otherwise reflected cluster plot 
present purposes interested binary patterns 
activation different units detailed 
differences activation units putting cluster 
plots context underlying unit activities 
discussion previous section concrete reading 
point useful 

next step try running case multiple 
instances digit call case 
run selecting press 
see appropriate hidden unit active version 
digits small levels activity units 
observed cases 
compare 
different digit images same digit 
noisy version digit additional units active now 
explore effects selectivity units 
behavior 

control panel shows leak conductance 
hidden units set value case 
detector example 
changing parameter affect specificity units 
responses happens generally hidden activations 
reduce value inputs 
affect cluster plot hidden unit activities 
goal network 
same hidden representation version same 
digit different representations different digits 
changing specificity units responses affect 
network 

set leak conductance back now see 
network responds letter inputs digits 
press button control panel select 
layer network controls top 
scroll display back start letter 
top 
display single button good choice fine 
visible response came hidden unit 
letter input similar press 
end forward fast button grid log 
continue scroll next press 
pressing 
inputs now see 
digit units respond 
letter stimuli 

based experiences previous question 
expect happen cluster plot hidden responses letter 
inputs leak current value 
right say hidden 
representation good letter identity 
information find setting 
letter information 

clear digits example explored 
highly specific matching hidden representations 
provided useful simplicity particularly realistic 
powerful type representation general term representations 
unit represents input pattern 
local localist referred 
cell representations 
brain neuron uniquely represents ones 
detector model 
neuron associated type representation 
clear detector function apply 
complex difficult describe local non 
representations 

alternative localist representations distributed 
representations individual units participate 
representation multiple input patterns addition multiple 
units typically active input pattern saw 
good examples types representations 
leak current explorations previous section 
useful think distributed units representing features 
input patterns whole input pattern composed number 
features particular feature pattern 
encoded pattern active units known 
based feature representation course notion 
detectors case units need clearly 
defined easily described representations distributed 
representation effective case coarse 
coded representations continuous dimensions see 
features arbitrary 
underlying dimension unit graded 
response 

distributed representations general tend specific 
localist representations units participate 
representing multiple items features represented 
units serve enhance distinctions collapse 
localist case 
difficult see distributed case 

main advantages distributed representations localist ones 
follows 
total units required represent 
number input patterns representation shared 
units otherwise unit pattern required 
distributed representations provide natural means 
encoding similarity relationships different patterns 
function number units common pattern overlap 
network distributed representations 
respond appropriately novel input patterns 
appropriate novel combinations hidden units 
impossible localist networks require 
entirely new unit 
multiple units 
representation robust damage 

representing continuous dimensions distributed 
coarse coded representations accurate 
equivalent number localist representations 
lot information contained relative activities set 
units whereas localist units represent 
different values units 
distributed representations allow 
bootstrapping small changes critical learning see 

clear distributed representations critical 
key properties neural networks described 
chapter researchers maintain 
localist representations preferred part 
simplicity useful previous section 
remaining models book 
distributed representations 
discuss important difference sparse 
distributed representations generic distributed 
representations 

finally evidence cortex distributed 
representations researchers presented visual stimuli 
systematically number different dimensions 
size shape color etc activities neurons 
areas cortex process visual inputs 
cases aware 
shown cortical neurons exhibit relatively 
broad tuning curves means respond stimuli 
range different parameter values consistent 
coarse coding stimulus dimensions kind 
abstract hard based feature representation 
level higher based object representations neurons 
early visual system forms evidence suggest encode 
oriented bars light see 
again consistent based feature representation 

now explore difference localist distributed 
representations start project 

chapter leabra 
open 
project window 
select 

project looks similar previous containing 
network grid log press 
same localist network now 
pick see new network 
appear same place distributed 
network contains hidden units lets explore 
network examining weights hidden units select 
notice units configured detect parts 
features digit images entire digits case 
localist network imagine units 
active whenever features present input 
test idea now verify 
firing patterns hidden units sense 
features present different digits 

case hidden unit firing 
digit fires left right match 
weight pattern actually important 
encoded hidden unit 
middle horizontal line detector actually serve 
multiple roles simple case kind 
complexity attempt describe content 
detected neurons imagine 
weights complicated pattern values 
start feel complicated neurons responses 

cluster plot distributed networks hidden unit 
representations 
compare cluster plot input patterns 
cluster plot localist hidden units terms 
visual similarity digits representations capture 
hint remember clusters plot 
similarity information captured underlying 
representations explain consistent general 
level specificity distributed representations relative 
localist ones account differences 
distributed hidden units compared input 
patterns 

select test distributed 
network letter inputs cluster plot resulting 
hidden units compare cluster plot letter 
inputs distributed network provide good representation 
letters think case 
relate answer specificity distributed 
representations previous question 

distributed network achieves useful representation 
digits number hidden units localist network 
number hidden units number input units 
greatly input representation explain 
achieved binary representation units 
active pattern 
number units required represent digits 

ways bidirectional excitatory connectivity behaves same 
way unidirectional feedforward connectivity explored 
transformation goes ways addition 
able produce digit category image digit 
feedforward networks explored bidirectionally 
connected network produce image digit 
digit category produce 
image based digit 
involves processing going top 
bottom explored lateral connectivity 
connections units same layer part 
pattern activate parts process goes 
name pattern completion pattern 
full processing part closely related phenomenon 
mutual support 
lateral connectivity activations 
interconnected units strongly activated 
phenomena described general term 
dynamics networks activations appear 
particular final configuration range 
initial activity patterns range initial 
lead same final pattern called 
explore cases 
proceed amplification properties 
bidirectional connectivity 

lets begin exploring bidirectional case same kind 
transformations unidirectional open project 
previous digit networks same windows etc 
main control panel called 

examine network note 
connecting layers going 
coming back indicates bidirectional 
connectivity view connectivity selecting 
button network window click 
hidden units see familiar digit image 
now click individual units input image 
see receive hidden units well note 
different input units receive different numbers sending units 
reflects different types pattern overlap 
digit images 

easier way see happen activated hidden 
unit view sending weights selecting 
sending weights look receiving ones 
appropriate digit image verify fact weights 
click 
notice difference display remember 
weights important need determine 
level symmetry future interesting thing 
weights see unit activates 
same things activate kind 
consistency things activated bidirectional 
networks important see 
later 

now run network see weights produce 
replicate previous feedforward results press 
select present images input 
layer digits notice network displays 
activations settling process 
changing equilibrium come 
later note grid log shows input 
hidden activation states now results run 
exactly same feedforward network 
weights additional effect here 
input units clamped pattern event 
actually computing activations otherwise 
attention weights pressing 
run network digit category units 
hidden layer digit images input resulting 
input patterns driven top weights 
units saw dynamics activation 
updating network window settling pattern 
otherwise difficult tell difference 
runs produce basically same patterns 
activity layers tell hidden units 
clamped same 
activation value whereas value input images 
presented images slightly lower activity value 
driven digit category units 

simple exercise demonstrates bidirectional connectivity 
enables information flow transformations computed 
bottom top number 
important issues surrounding phenomenon 
different digit images correspond same categorical 
hidden unit exploring bit 
happens multiple 
combinations hidden units active press 
enviroview event pattern displayed pattern 
contains input presented hidden layer units 
moment lets run 

describe happens input layer activations digit 
categories activated sure note subtle differences 
activation account result change 
value enhance differences activation 
present explain helps kind 
enhancement differences generally useful cognition 

now click left button digit 
categories input way select 
combinations digit categories activate sure press 
changes take effect trying different 
combinations 

now explore pattern completion network 
bidirectional connections single layer units 
top bottom processing network exhibits 
lateral processing difference 
same underlying processing mechanisms 
units involved somehow 
whereas top bottom 
hierarchical relationship 

open project addition 
usual windows see network window 
single network environment enviroview window 
event click left button 
single event display pattern window 
clicking pattern change network complete 
usual begin examining weights network 
select click different units see 
units image digit interconnected 
weights value units weights 
appear presenting part image 
result activation remaining parts 

test press button main 
control panel presents input 
units determined event pattern shown 
view window activated notice 
viewing activations updated settling 
tell ones clamped environment 
result fundamental 
difference pattern completion phenomenon 
types excitatory processing examined far simply 
result units detecting pattern activity 
amongst activated pattern 
sufficiently matches pattern encoded weights 
pattern completion particularly useful thinking 
recall memory cue 
distinctive image etc triggers ones 
memory results recall related event 
discussed further 

note previous simulations 
called soft inputs event pattern 
presented additional excitatory input neurons 
directly setting activations corresponding values 
latter form previously 
called hard results faster processing 
soft soft necessary case 
units layer need update activation values 
function weights order produce pattern completion 

pattern weights think 
number units need clamped order produce pattern 
completion full now test answer 
units event pattern network longer 
produces complete pattern 
press view clicking 
units necessary completion parameter 
lower number value parameter allows 
completion input active happens activate 
inputs part pattern weights 
layer configured support representation 
pattern think difference similar 
new pattern pattern 

phenomenon closely related pattern completion mutual 
support happens activity 
set units excitatory produces extra 
activation units amount extra activation 
provides useful indication strongly interconnected 
units enables better interference 
noise inhibition see effect simulation 
clicking entire pattern input network 
unit part set leak current 
hit note units 
part interconnected pattern experience mutual 
support able overcome relatively strong level 
leak current unit weights suffers 
significantly 

finally note preceding simulations highly 
simplified basic phenomena underlying 
mechanisms clear see start learning 
algorithms networks 
complex deal large number patterns 
encoded same units resulting behavior 
powerful difficult understand 
detail same basic principles work 

examples involving bidirectional connectivity 
controlled avoid 
problems bidirectional connectivity positive 
feedback positive feedback rise 
useful phenomenon amplification excitatory signals 
neurons result activation strengths 
amplification critical explaining aspects cognition 
including word effect discussed introduction 

later positive feedback problem results 
spread excitation entire network 
producing effectively 
unit activated inhibitory mechanisms described 
next section necessary take full advantage bidirectional 
amplification consequences 

section explore couple simple 
amplification action see lead ability 
bootstrap weak initial activation fully active 
pattern see cause spread 
activation network 

begin exploring simple case top amplification 
weak bottom input via bidirectional excitatory connections open 
project see network 
layers input bidirectionally connected hidden 
layers unit 
layer graph right plot activations 
layer hidden units time network settles response 
input 

bootstrapping phenomenon activation 
hidden unit strong start activating 
hidden comes back hidden producing 
strong activation 

press control panel 
see 
graph window here activation coming 
top unit 
relatively weak initial activation unit 
resulting strong activation units 
example bootstrapping 
unit activate unit place 
receive additional top excitation 
increase strength leak current 
observe resulting activation now 
activate bootstrapping 
amplification occurs decrease leak current bottom 
pattern relatively strong anyway bootstrapping 
amplification 

simple case bootstrapping amplification provides 
insight level word information 
presumably higher cortical processing stream level letter 
information come back bootstrap 
activation corresponding level letter stimuli revisit 
example detail 

note example organized according bottom 
top processing same principles apply lateral 
connectivity simply move unit 
same layer 
parts interconnected pattern activated 

previous example illustrated benefits bidirectional 
excitatory amplification bootstrapping occur 
distributed representations amplification lead 
activation units particular 
overlapping connections required implement distributed 
representations allow excitation spread 
see spread inhibition 
resort increasing leak current prevent 
activation spread problem here saw 
example leak current impossible 
bootstrap representations activity benefits 
bidirectional excitatory amplification available 
example provides strong motivation next section 
inhibitory interactions 

open project network here 
previous example now multiple 
units layer lets examine connectivity 
button click units notice 
receive corresponding input input units 
called connectivity notice left 
right units receive uniquely left right 
units now click left 
right units observe connectivity 
symmetric left unit receives left center 
right units connectivity pattern 
representing separable features input 
objects consisting features 
labeled simulation think object 
features 

feature 
overlap objects shared feature 
causing 

locate control panel press 
unique object see 
network settle looking grid log 
right showing activations 
comes back activate feature 
good example pattern completion phenomenon 
top activation lateral activation fails 
presented non feature active 
default leak current level 

try setting leak current see 
center hidden feature active describe 
happened explain happened terms connectivity 
network try parameter space 
increments see find level 
strong activation 

activate ambiguous center input feature pressing 
increments strong activation 
common feature weak activation 
features explain results 

finally set leak current press 
button try notice full 
pattern presented network activates 
level object units active find value 
reasonable solution 

observed explorations bidirectional 
excitatory connectivity 
interesting amplification pattern completion processing 
easily away further see 
networks tend strongly respect 
small parameter changes activated 
area important property 
networks later chapters 

network behavior supported 
present point neuron activation function 
see particular 
property sigmoidal plus 
function provides necessary upper positive feedback 
gain parameter 
changes threshold value contributes 
character units 

simulation far leak 
current played central role determining networks 
behavior leak current far 
excitatory input coming neurons 
consistent role described 
seen 
likelihood null hypothesis bayesian hypothesis 
testing framework problem situation leak 
current constant easily respond 
dynamic changes activation network 
manipulate role dynamic 
excitatory input played 
inhibitory inputs neuron produced 
inhibitory interneurons described 
neurons sample general level 
activation network provide dynamically 
amount inhibition based activation level 

think general role inhibitory neurons 
controlled air prevents 
network getting active 
sample air inhibitory neurons 
sample activity network inhibitory neurons 
detect network getting active produce 
inhibition increased activity 
turn gets 
conversely detect activity provide 
inhibition principle necessary 
inhibition same kind set point behavior 
roughly same activity 
level see provides convenient reasonable 
approximation effects inhibition explorations 
see effects different parameters 
understood analogy 

basic types inhibitory connectivity 
excitation shown open connections 
inhibition ones shows 
feedforward inhibition driven input layer activity 
shows feedback inhibition driven same 
layer note inhibitory interneurons 
typically themselves well 

forms connectivity involving inhibitory neurons 
cortex rise feedforward feedback 
inhibition see example 
feedforward inhibition inhibitory interneurons driven 
directly input layer inhibition hidden layer 
neurons hidden layer 
neurons receive amount inhibition function 
level activity input layer projects excitatory 
connections hidden layer feedback inhibition occurs 
same layer inhibitory neurons 
producing negative feedback 
note inhibitory neurons 
themselves providing negative feedback control 
own activity levels turn important 

related ways forms 
inhibitory connectivity act control excitation 
feedforward inhibition 
excitation coming layer layers 
take account 
provide result 
feedforward inhibition hidden layer excitatory neuron receive 
roughly amounts excitation 
inhibition think prevent neurons ever 
getting active place acts kind 
neurons particularly strong 
excitatory weights current input pattern able 
overcome feedforward inhibition comparison feedback 
inhibition level excitation layer 
itself provides negative feedback prevents 
excitation 
units observed previous section 
standard same air 
see following explorations types 
inhibition necessary 

order speed simulations summarize 
effects inhibitory interneurons computing inhibition 
function directly function amount excitation 
layer need explicitly simulate inhibitory 
interneurons themselves simplest effective inhibition 
functions forms take kwta function 
described 
see following explorations combined effects 
feedforward feedback inhibition rough set 
point behavior overall activity level layer 
levels activation set point back 
set point value 
characteristic kwta functions implements directly setting 
target set point units total active 
case function 
prevent network getting active result 
function allows network active 
units active isnt excitation coming 
network discuss particularly 
useful functional properties inhibition 

important general functional consequences 
inhibition inhibition leads form competition 
neurons case feedforward inhibition 
strongly activated neurons able overcome 
inhibition case feedback inhibition neurons 
active better able inhibitory 
feedback activity contributes inhibition 
neurons competition thing 
network provides mechanism selection finding 
appropriate representations current input pattern 
selection process natural selection 
based competition natural resources results 
evolution itself selection process network occurs 
moment moment line basis longer time 
periods interaction learning mechanisms described 
next chapter learning context competition 
produces evolution representations 

value competition long recognized artificial neural 
network models 

showed feedforward inhibition 
result form feedforward pattern completion 
return finally 
tried strong mapping selection 
takes place neural network 
rely basic competition learning 
mechanisms understanding process 

way viewing effects inhibition terms 
sparse distributed representations produced 
appropriate levels inhibition particularly kind inhibition 
produced kwta inhibition function representations 
distributed level inhibition multiple units 
active time sparse inhibition 
strong prevent relatively small percentage 
units active according parameter 
kwta sparseness understood terms specificity 
underlying representations explorations 
difficult 
unit active specific unit 
fires typically means fires 
specific obviously extreme case 
sparse representation localist representation 
sparse distributed representations intermediate fully 
distributed localist representations represent 
advantages types representations 

emphasized 
sparse distributed representations particularly useful 
representing things world simple form argument 
goes follows things world generally share number 
underlying features things world 
discussed sense 
represent things terms distributed representations 
composed underlying features large 
space possible features assuming 
features relatively specific relatively 
relevant thing situation units 
representing features 
active network built 
produce sparse distributed representations better suited 
representing things world particularly important 
context learning discussed next chapter 

finally way viewing inhibition sparse distributed 
representations terms balance competition 
needs take place distributed 
representation multiple units represent 
thing complete competition localist complete 
fully distributed generally good 
balance 

begin exploration open project 
usual windows including overall control 
panel network contains input layer projects 
hidden layer excitatory units layer 
inhibitory neurons inhibitory neurons 
activation level hidden layer units thought 
inhibitory units hidden layer 
own layer purposes simulation 
inhibitory units total hidden units 
found cortex commonly roughly 
inhibitory neurons 
excitatory neurons outputs contribute 
inhibitory conductance neuron excitatory 
conductance set activation parameters 
different inhibitory neurons discussed 

lets begin usual viewing weights network select 
units weights random 
inhibitory units fixed constant value notice 
hidden layer excitatory units receive input 
inhibitory units inhibitory units receive feedforward 
connections input layer feedback connections 
excitatory hidden units well inhibitory connections 
themselves control panel parameters 
determine relative contribution feedforward 
feedback inhibitory pathways applies 
feedforward weights input inhibitory units 
inhibitory units controls 
scaling inhibitory connections back onto inhibitory neurons 
themselves see important 
parameters arbitrary relative 
scaling parameters described 
important importance properties 
different types inhibition 

now lets select view activations network 
window press control panel see 
input units activated random activity pattern 
cycles activation updating hidden inhibitory units 
active activation appears controlled 
inhibition excitation input layer note 
level leak current small 
excitation 
performed inhibition leak current case 
previous simulations average activity hidden 
inhibitory layer units plotted graph window 
overall average activity hidden units 

graph window clearly shows inhibitory units 
activated advance hidden units important 
simulation incorporates difference 
excitatory inhibitory neurons rate updating 
controlled parameters 

see excitatory neurons updated 
inhibitory faster faster updating inhibitory 
neurons allows rapidly adapt changes overall 
excitation level important function 
feedforward inhibition 

important practical point update rate 
constants important advantage 
simplified inhibitory function described next section 
rate constants set relatively slow order 
prevent behavior see set 
largely 
time scale excitatory 
neurons update activity smaller steps inhibitory 
neurons better able set parameters back 
default press button 

now manipulate parameters control 
panel determine roles producing observed activations 
lets start conductance inhibitory current 
excitatory units scales 
level inhibition coming excitatory neurons clearly 
predict plays important role 

change effect 
average level excitation hidden units 
inhibitory units increase 
happens now explain pattern results 

set back now lets see happens 
manipulate analogous parameter inhibition coming 
inhibitory neurons expect 
results similar obtained 
inhibition upon inhibitory neurons 
interesting consequences try 
good idea run comparison 
see excitatory activation inhibitory 
level roughly same try value now 
excitatory activation level increases inhibition again 
remains same difficult phenomenon understand 
think here ways understanding 
going seems straightforward reducing 
amount inhibition inhibitory neurons result 
activation inhibitory neurons look 
activity inhibitory neurons true 
increasing inhibition results lower 
activation feedback inhibition starts 
hidden units active inhibitory activity 
same level runs sense greater 
activation inhibitory units 
case hidden units causing 
lower activation result 
activation inhibitory units coming feedback 
hidden units imagine reduced activation 
inhibitory neurons increased activation lower 
activation level hidden units remain 
lower activation levels inhibition goes back original 
activation level 

way explain noting dynamic 
system balance excitation inhibition 
imagine time excitatory hidden units start 
bit active turn activate inhibitory units 
easily themselves 
turn provides extra inhibition advance 
hidden units effectively played level 
changes activations 
absolute levels explain 
see evidence looking absolute levels 

intuitive details way 
understanding effect inhibition inhibitory neurons 
terms location relative output 
place close 
constant distance away 
far away output 
strongly driven output analogous 
parameter larger values 
result higher levels activation greater 
hidden layer smaller values 

set back important point take away 
explorations number different 
ways changing parameters achieve roughly same resulting 
level hidden unit activation means actual 
biological system difficult reason 
result greater activation levels backwards underlying 
parameter change rise course 
tell different 
parameters typically involve level low biological 
inhibitory inhibitory connectivity 
strength empirically individual neurons certain 
parameter changes evidence 
know 

now lets influence feedforward versus feedback 
inhibition overall effects inhibition weve observed far 
set effectively feedforward 
excitatory inputs inhibitory neurons input layer 
affect behavior excitatory inhibitory 
average activity levels explain result hint think 
effects feedforward inhibition next set 
feedback inhibition now happens try finding value 
activity level initial default system 
differ initial system explain pattern results 
kinds inhibition useful 

important things inhibition 
changes weight values learning 
typically units learn levels excitatory input develop 
greater variance input patterns patterns providing 
strong excitation producing 
natural result specialization units representing 
things test current inhibitory 
mechanism changes simulating effects 
learning units excitatory weight values higher 
level variance press return 
default parameters case networks weights produced 
generating random numbers mean 
variance mean case 
baseline comparison now click 
control panel select weights 
same mean variance gaussian 
distributed values produces higher variance 
excitatory net inputs units hidden layer 
increase total overall weight strength increase 
variance larger weights 
mean press see 
difference overall excitatory level 
observed greater level excitation 
weights compared weights verify 
system change increasing 

things simpler far exploring 
relatively easy case inhibition network 
bidirectional excitatory connectivity clearly 
requires inhibitory feedback saw explorations 
now lets try running network bidirectionally connected 
hidden layers select back default 
parameters comparison 
network bidirectional excitatory connectivity examining 
weights usual note layer inhibitory neurons 
receive excitatory projections back layer enabling 
feedforward type impact activity 
hidden layer now new 
network graph log shows average activity 
hidden inhibitory layers note initial part 
point hidden layer begins active 
same layer activates 
back layer inhibitory neurons active 
excitatory neurons overall activity level 
remains control substantially different 
earlier simulations 

see inhibition important bidirectionally 
connected networks set parameter 
reduce amount inhibition excitatory neurons 
note relatively small impact initial 
feedforward portion activity curve hidden 
layer active network 
activated fit set 
parameter back 

final exploration point provides motivation 
summary inhibition function presented next section 
here want explore happens activity levels 
different overall levels excitatory input presented 
network press button enter 
value change input pattern 
units active default 
now activity level 
substantially different previous case difference 
observe increases activity level system 
appears relatively robust changes overall input 
excitation show effect 
demonstration comes relatively small differences 
initial activity level hidden units compared subsequent 
level input hidden layer 
approximate set point behavior system tends 
produce relatively fixed level activity regardless 
magnitude excitatory input 
inhibition function described next section 

explain general terms system exhibits set 
point behavior 

change activation function 
units spiking activation function 

saw appropriate combination feedforward 
feedback inhibition rise controlled activation 
excitatory neurons bidirectionally connected 
possible summarize effects 
inhibitory neurons setting inhibitory current 
excitatory neurons directly inhibitory function 
based level excitation layer 
avoid need explicitly simulate inhibitory neurons 
connectivity significantly amount 
computation required simulation addition 
able way avoid need slow time 
constants updating units processing 
resources 

simplest understand inhibitory 
functions known take kwta function 
idea here set point property 
inhibitory system set wide layer level inhibition 
number units achieve threshold 
equilibrium membrane potentials level inhibition 
rest remain threshold order implement 
function need able compute amount inhibitory 
current put unit threshold present 
level excitatory input interestingly networks learn better 
computation performed excitatory input minus 
contribution bias weights sense 
biologically bias input visible 
inhibitory interneurons excitatory input presumably 
functional level allows bias weights affect 
overall levels activation inhibition 
computation trying produce fixed point set activation 
level 

necessary equation familiar exercises 
equilibrium 
membrane potential equation compute threshold 
level leak current same thing here inhibitory 
conductance threshold written 
threshold membrane potential value 
represents excitatory input minus contribution 
bias weight value compute 
wide layer inhibitory conductance setting 
values units layer positions 
list units level 
excitatory conductance words 
wide layer inhibitory threshold placed 
active units layer ensures 
unit remains threshold unit 
expressed formula 
constant determines exactly place 
inhibition units value typically 
enables unit reasonably far 
inhibitory threshold depending terms 
distributed 

possible level excitation 
units layer plotted 
related excitatory net input axis 
order number units layer axis kwta function 
places wide layer inhibitory current value 
active units shown 
lines shape distribution significantly affects 
extent highly activated units rise 
threshold figure note simple 
relationship inhibition excitation 
assumed purposes figure shows normal 
distribution active units reasonably 
inhibition strongly activated units 
threshold resulting small inhibitory excitatory 
activated units strongly active 
units resulting large 
activated units 

shows distribution excitatory 
activation net input units layer 
large impact well active units able overcome 
inhibition computed kwta function 
strongest activity 
produced clear separation active 
units active functional 
property inhibition function activation 
reflects active units 
excited property weakly 
activated units tend activated 
position threshold reasonable leak 
current prevent units active 
parameter represents upper level 
activity lower levels activity occur response 
weaker excitation accurate name function 
wta 

version kwta function provides 
greater flexibility regarding precise activity level produced 
providing relatively upper tradeoff 
lack exact activity level worth 
advantages network able bit flexibility 
representations version called 
based average kwta inhibitory function 
simulator inhibition computed function top 
units remaining units 
function units 
specifically inhibition 
computed function average 
threshold conductance inhibitory terms top 
units written 
average threshold conductance inhibitory remaining 
units same 
formula putting inhibitory conductance 
values 
time value typically 
simply tends work better simulations 

illustration based average kwta inhibitory 
function computed wide layer inhibitory current value 
placed average values 
top units average 
remaining values 
simpler kwta function 
entire distribution average terms whereas 
kwta values units 
case inhibition similar place 
simple kwta function results 
lower level inhibition simple kwta 
units active results higher 
level inhibition simple kwta units 
active 

depending underlying excitation 
involved units 
active inhibitory function see 
learning algorithm 
shaping representations network bit 
appropriate overall levels activation 
different cases addition flexibility advantage 
scheme resulting global inhibitory conductance 
based units layer 
stable robust 
activation updating reasons version kwta 
function generally preferred simpler 
layer activation sparse unit active 
time simpler kwta function works better generally 
case explorations 

now lets explore kwta inhibition functions compare 
behavior previous networks already 
open open already open reset 
parameters default values 
button press choose 
select bidirectionally connected network 
standard activation graph case actual 
inhibitory neurons now select choose 
press notice roughly same level activity 
results inhibitory activity zero units 
function activity function 
kwta function effectively perfect job 
appropriate level inhibition required 
further activation hidden layer starts earlier 
faster parameter 
now select 
based average kwta function again 

order test set point behavior kwta functions run 
network levels addition 
standard types kwta function notice 
functions exhibit stronger set point behavior 
inhibitory unit based inhibition based average kwta showing 
slightly variability overall activity level 
kwta functions explicitly set 
point whereas inhibitory units roughly produce point set 
behavior remember kwta functions 
approximation effects inhibitory 
neurons identical fashion 

order see main advantages kwta functions 
select try find 
update parameter increments 
maximum result significant 
behavior 
value found 
compare value parameter based unit 
inhibition advantages faster updating 
think kwta fast update rate based unit 
inhibition 

order property simple kwta 
function apply set leak current value 
prevents weak excitation activating units allows 
strong excitation produce activation 
select value 
units allows excitation activate layers 

explorations inhibition return 
digits example revisit issues originally explored 
benefits inhibition better sense 
inhibition specifically simple kwta function performs 
case representations random activation 
patterns 

open project essentially 
identical project explored 
simple kwta inhibition 
effect controlled parameter 
maximum number hidden units strongly 
active bias weights turned default 
localist network reasons clear 
default 
parameter expected localist result single 
hidden unit strongly active input note leak 
current relatively weak 
contributing selection active unit 
setting running 

now increase parameter 
strongly activated 
units identical excitatory net input values meaning 
inhibition placed right placed 
right units threshold 
results weak activation shown due effects 
noise noisy activation function described 
situation shown 
active unit 
well ones way 
network produce active units 

continue increase parameter observe 
provides increasingly distributed patterns hidden layer 
total number active units due 
effects discussed advantage controlling 
number active units kwta inhibition function 
error trial manipulation parameter 
precise direct control outcome 
parameter set apply regardless changes 
network parameters affect parameter 

now lets explore kwta inhibition distributed 
based feature network choose 
force distributed network single hidden unit active 
difficult achieve setting 
units similar levels excitation 

unit activities 
well feature representing similarity 
structure digits value 
parameter produces cluster distinctions 
hidden patterns actually units 
active mechanism pattern 
found happens activity levels 
reduce leak current 

speaking kwta inhibition function 
single takes wta function simple case 
competitive learning algorithm 
excited units 
activity set rest zero version 
idea developed same 
kinds bayesian discussed 
case units 
activated extent likelihood generating 
input pattern larger units 
activity unit took form 
likelihood measure unit 
comes 
conditional probabilities hypothesis associated 
different units mutually 
hypothesis null hypothesis 

mutual assumption significant 
limitation type model saw distributed 
representations obtain considerable power enabling things 
represented multiple units 
assumption inconsistent distributed 
representations single wta models localist 
representations units graded soft activation 
values simpler models important 
mathematical understanding level 
entire network generally possible kwta 
function 

related form activation function simple wta function 
network builds ideas 
proposed here single 
chosen now neighborhood units 
active activation 
function distance 
learning networks exhibit interesting properties 
built tendency treat neighboring items 
similar ways revisit ideas 
model early visual processing 
learning 

important limitation network 
representation full power 
distributed representations match 
different units represent different combinations features 
see later build neighborhood bias 
lateral connectivity units effectively 
same thing network 
flexible kwta framework described approach 
similar explicit 
lateral connectivity 

finally number models constructed units 
communicate excitation inhibition directly units 

model goes name 
interactive activation competition 
bring number principles discussed chapter 
cognitive phenomena word effect 
described previously interactive activation same thing 
bidirectional excitatory connectivity provided top 
bottom processing model 
development kinds bidirectionally 
connected networks 
properties discussed 

models important limitations 
fact special inhibitory neurons clearly 
separation excitation inhibition found brain 
kind direct inhibition units good 
distributed representations multiple units active 
competing case unit active 
stable balance 
excitation inhibition required keep units active 
same time 
units getting active separate inhibitory neurons 
inhibition units 
layer point point excitatory 
neurons inhibition results consistent 
activation dynamics distributed representations saw 
earlier explorations 

explored distinct effects excitation inhibition 
now position think global level analysis 
bidirectional excitation inhibition seen part 
larger computational goal overall perspective called 
constraint satisfaction network seen 
simultaneously trying number different constraints 
imposed via external inputs environment 
weights activation states network itself 
mathematically shown connected 
bidirectional networks sigmoidal activation functions 
extent constraints 
original demonstration point due 
applied ideas 
towards understanding network behavior 

crucial energy 
function physical system energy 
function associated provides global measure energy 
system depends strength 
interactions connections atoms 
system system higher moving 
faster higher energy interactions 
constraints system part 
energy system function strong 
constraints extent system 
system constraints higher energy 
takes energy constraints think 
constraints takes energy 
system higher 
constraints greater extent higher energy system 
constraints lower state energy 
turns nature settle lower energy state 
satisfying constraints 

simple example energy function sum 
distance function objects 
separated energy function 
system 
dimensions objects 
closer distance value obviously gets smaller 
meaning constraint objects closer gets 
according energy function see 
energy functions typically squared form 
distance function 

network neurons state constraints 
thought lower energy apply 
energy functions networks find 
simple act updating activations units network 
results same kind settling lower energy state 
satisfying constraints standard form network energy 
function follows 
represent sending receiving unit 
activations respectively weight connecting 
important weights connecting units 
symmetric influences unit 
unit enables network find single 
consistent state units 
weights symmetric unit 
same state obviously 
lead consistent global state 

constraints represented function extent 
activations consistent weights 
large weight units units 
strongly active contribute smaller energy 
value minus sign magnitude 
sign energy term 
negative value energy function called harmony 

here units said contribute greater harmony 
aka lower energy strongly active connected 
large weight terminology network settling acts 
increase overall harmony activation states 

see occurs lets take simplest case linear unit 
computes activation follows 
take derivative harmony equation 
respect units activation value 
note term appears sum 
term harmony equation 
term same meaning 
updating activations network units same 
thing harmony network 

analysis network sigmoidal 
units similar point neuron activation function 
see 
work mathematically energy harmony equation needs 
entropy term reflects 
extent activation states units network 
middle range values 
resulting overall equation called based harmony 
free energy based 
results adding additional term typically affect 
relationship different states 
words harmony fairly redundant 
measures restrict focus simpler harmony 
term 

point neuron activation function leabra 
result increased activity units connected strong weights 
simpler cases 
function tend increase overall harmony 
network understand general processing 
networks performing constraint satisfaction working 
overall harmony exercises explore idea 
empirically 

local minimum energy function noise 
system represented 
global minimum least better local minimum 

noise membrane potential activation values play 
important role constraint satisfaction basically noise helps 
keep things getting think 

form noise system 
getting optimal sub state similarly noise added 
activations units prevent network getting 
optimal sub state fails constraints 
optimal states see 
optimal sub states called local local 
depending harmony energy 
respectively compared global 
optimal possible 
states maximum harmony value possible 
simplest networks typically settle local 
noise likely find 
better local 

actual neurons naturally occurring noise 
precise timing output spikes code rate 
functions discrete spikes source noise 
noisy function builds noise shape 
function see 
relatively rare cases network 
local problems add additional noise 
back activation function switch spikes 
ambiguous stimuli truly equal 
possible interpretations see cube example noise 
necessary break 

constraint satisfaction problems need 
resort special technique called simulated 
slow 
quality high neural network 
simulated gradually reducing level noise 
network settles idea early processing want 
explore wide range different activation states search 
better relatively high levels 
noise good state want reduce 
noise level way maximum 
simulator 
controls noise level function number processing 
cycles performed settling 

illustration dimensions units kwta 
inhibition restrict search space smaller subset 
possible patterns represents network kwta 
inhibition explore possible combinations 
activation states shows effect kwta inhibition 
restricting activation states explored 
faster effective constraint satisfaction 

point set kwta inhibitory function important impact 
constraint satisfaction performance network basically 
constraint satisfaction form parallel search 
network number different possible states 
finding constraints fairly 
unit updating parallel search parallel 
sequentially huge number distinct states 
sequence viewed way role kwta inhibition 
restrict search space 
possibility going states wide range different 
overall activity levels network kwta inhibition 
produce states relatively range overall activity 
levels see illustration 

advantages restricting search space network 
settle faster reliably good states assuming 
good states allowed kwta 
algorithm idea 
sparse distributed representations described 
learning means network 
important possible representations range activities 
allowed kwta function see sparse 
distributed representations useful cognitive tasks 
studied later chapters network kwta inhibition 
settle faster reliably good constraint 
satisfaction solutions 

begin exploration simple semantic network 
intended represent small set relationships 
different features represent set 
world case representing features 
color size toy network 
contains information number individual 
able information 
general common unique 
tell consistency 
feature harmony 
function useful total constraint satisfaction 
level network particular configuration feature 
inputs network perform pattern 
completion way information particular 
individual individuals simple network 
covered chapter 

feature values individual 
exploration 

knowledge embedded network summarized 
knowledge encoded simply 
setting weight representing 
individual corresponding feature value 
individual groups features values 
column table represented distinct layers 
own layer inhibition addition 
identity units name units own separate layers 
well inhibitory function here 
important network flexibility 
actual number active units layer parameter set 
layers 

see start project 
weights network verify weights implement 
knowledge shown table now locate enviroview 
right click button see 
network displayed window 
inputs soft clamped corresponding network 
units button 

lets verify present individuals name input 
recall information individual 
form pattern completion single unique input cue 
see layer units 
enviroview already default locate 
overall control panel press 
see network activates appropriate features 
ahead try name activations sure 
press button enviroview 
changes sure click previous 
clicking again activity input 

now lets see network general information 
versus level information 
set individuals select 
network activates features typical 

explain reason different levels activation 
different features activated 
useful information 

now lets constraint satisfaction ideas 
notice graphlog network window 
bring run 
network again input selected graph 
shows value harmony cycles network 
settling notice expected value appears 
increase settling indicating network 
increasingly satisfying constraints activations 
updated now lets specific network 
activating color input addition 
see initial harmony value slightly 
larger reflecting greater excitation present input 
final harmony value significantly lower 
likely network 
able well put way easy 
constraint resulting harmony large 
plus applies 
things harmony lower 

number different ways 
network ahead present different input patterns 
see kinds responses network 
reasonable response set 
constraints provided input pattern 
interesting try figure activation spread 
network settles network 
enviroview useful purpose showing state 
network default cycles updating 

cube seen looking 
looking 

now lets explore constraint satisfaction processing 
ambiguous stimuli example cube 
shown viewed 
cube orientations people tend back 
forth viewing way versus 
rare view same time 
words tend form consistent overall interpretation 
ambiguous stimulus consistency reflects action 
constraint satisfaction system favors interpretations 
constraints imposed possible interpretations 

open project network window 
see units cube 
representing possible interpretations left cube 
corresponding right 
units 
layer simple kwta inhibition operating 
parameter active 
time constraints usual lets examine 
weights notice unit connected local neighborhood 
gets active tend 
activate consistent interpretation 
entire cube active same time 
interpretation cube activating via 
inhibition competing active 

return viewing network press 
view competition process action running 
interpretations receive equal weak amounts excitatory input 
see network settles point 
units active strength 
back forth cube eventually fully active 
remains inactive try times 
note cube random 
eventually observe case part cube activated 
entire cube active 
happens note plot harmony value graph log 
substantially correspond 
consistent solution cube inconsistent partial 
satisfaction weight constraints lower harmony full 
satisfaction constraints cube 

noise added membrane potential playing important role 
simulation break 
cube interpretations see lets manipulate 
level noise 
try following values 
report differences observed settling behavior 
network different values tell 
noise process 
try playing different activation functions 
setting parameter 

finally important psychological aspects cube 
stimulus people tend possible 
interpretations occurs neurons 
activated interpretation eventually allowing 
competing units active process neurons 
getting called accommodation well established 
property neurons covered 
button turn property 
network runs cycles 
observe least cube next 
neurons 

cortex typically described different layers 
neurons primary groups 
functional layers see 
input cortical layer hidden cortical layers 
output layers cortical layers input layer 
receives information senses via thalamus 
output neurons motor control outputs wide range 
subcortical areas hidden layer serves 
output network response input signal means 
internal model provides useful typically 
elaborated processed basis driving outputs network 
large amount interconnectivity layers 
forms data support idea information flows 
primarily input hidden output via excitatory 
neurons further excitatory neurons bidirectionally 
connected information flow backwards 
pathways inhibitory neurons exist cortical layers 
receive same types excitatory inputs excitatory 
neurons layers outputs 
far typically number excitatory neurons 
relatively close themselves inhibition appears 
provide kind local feedback mechanism 

excitatory neurons excitatory neurons 
distinct ways unidirectional feedforward 
connectivity pattern set neurons 
set versa vice typically found cortex 
properties feedforward processing generalize 
common bidirectional case unidirectional excitatory connectivity 
input activity patterns way emphasizes 
certain distinctions specifically representing certain 
patterns inevitably collapsing possible 
distinctions cognition understood terms 
process developing specific representations emphasize 
relevant distinctions collapse irrelevant ones 
extreme case specific representations localist 
representations specific distributed representations 
number properties 
models apparently cortex 

bidirectional aka recurrent interactive 
connectivity common cortex 
important functional properties found simple unidirectional 
connectivity neurons affect themselves via 
connections emphasize symmetric case 
same weight value relatively simple 
understand compared case capable 
performing unidirectional transformations 
enables top processing similar 
propagate information 
units layer leads pattern completion 
partial input pattern presented network 
excitatory connections activate missing pieces pattern 
bidirectional activation propagation typically leads 
amplification activity patterns time due mutual 
excitation neurons important 
effects due bidirectional excitatory 
connections including mutual support top support 
biasing bootstrapping phenomena 
described general term dynamics 
network appears particular activation state 
bidirectional excitatory connectivity allows useful 
dynamics processing neurons 
activations neurons communicate enabling 
dynamically influence type processing performed 
see next chapter bidirectional connectivity 
important communicating error signals drive 
learning 

bidirectional excitatory connectivity next 
driving 
own monitor output feedback basically 
positive feedback causes system signals 
maximum maximum 
firing rate neuron problem cortex 
well known study order control 
positive excitation cortex inhibitory neurons 
activate inhibitory synaptic channels see 
excitatory neurons order 
balance excitatory inputs forms inhibition 
present cortex feedforward driven level 
excitation coming layer feedback driven 
level excitation layer itself combination 
forms inhibition results set point behavior 
occurs point excitation leads inhibition 
resulting excitation excitation leads 
inhibition resulting excitation system 
preferred level excitation explicitly simulating 
inhibitory neurons summary function 
kind set point inhibition directly resulting 
greater simplicity simulations kind 
inhibition called take kwta set 
point parameter neurons 
total neurons active time addition 
controlling excitation inhibition results form 
competition neurons competition 
produces selection pressure activation dynamics 
learning results evolution survival 
adaptation representations finally produces 
sparse distributed representations sense terms 
aspects general structure world 

combined effect excitatory inhibitory interactions 
understood terms constraint satisfaction 
activity patterns network settle time 
way satisfaction constraints internal 
external network process understood 
mathematically energy function 
overall level satisfaction network 
show net effect activation propagation 
increase overall measure finally explore role 
inhibition consistency 
settling time associated large constraint satisfaction problems 

cognitive neuroscience provide 
information cognitive functions different 
cortical areas good choice 

provides information 
anatomical properties cortex 

detector model neural function learning provides 
primary mechanism setting parameters determine 
neuron detects parameters principally weights 
synaptic connections neurons including 
bias weights learning depends individual 
level neuron mechanisms specify parameters 
level network principles developed previous chapter 
produce overall network behaves appropriately 
environment importance mathematically treatment 
learning algorithms driven critical 
mathematical played 
key role learning algorithms developed 
levels analysis developing ideas learning 
occur human cortex 

begin known biological mechanisms 
underlie learning mechanisms known terms long 
term ltp long term ltd 
refer 
weights transient non long term manner form ltp 
found cortex associative hebbian form 
depends pre postsynaptic neural activity hebbian 
learning viewed performing model learning 
objective develop good internal model important 
statistical structure environment type learning 
called organizing self 
explicit feedback environment hebbian learning 
perform well task learning objective 
produce specific output patterns particular input patterns 
contrast driven error learning algorithm 
delta rule task learning direct 
target output actual output patterns 
target information labeled 
learning see 
valid sources require constant 
presence generalization 
delta rule called backpropagation allows errors 
occurring layer backwards earlier 
layers enabling development useful intermediate 
representations overall task easier solve 
original mathematically direct 
backpropagation algorithm biologically implausible 
bidirectional activation propagation communicate error 
signals called generec consistent known properties 
ltp overall biology 
cortex further allows error signals occurring affect 
learning combination based hebbian model learning 
based generec task learning typically 
producing better results finally 
learning mechanisms necessary address important 
difficult forms task learning known sequence 
temporally delayed learning 

study biology learning 
level long term ltp 
term describe 
findings order contrast forms 
inevitably transient 
knowledge refers 
increase measured excitation 
controlled stimulus onto receiving neuron 
frequency high 
typically cause long lasting 
effect increases synaptic efficacy 
now know considerable amount biological mechanisms 
underlying ltp related phenomenon ltd long 
lasting synaptic efficacy 

common form ltp cortex known 
mediated nmda ltp nice connection 
way nmda receptor works functional characteristics 
form ltp functional characteristics generally 
summarized term associative means 
activity states pre postsynaptic neurons important 
enabling occur 
association neurons ltp evidence 
suggests pre postsynaptic neurons relatively 
strongly activated occur consistent 
idea active representations strongly 
called hebbian learning 
covered detail next section conditions 
ltd occurs clear appears somehow reduced 
zero non activity pre postsynaptic neurons 
lead ltd 

focusing ltp case moment observed 
explained fact nmda receptor 
open 
postsynaptic membrane potential sufficiently 
excited cause ions move opening 
nmda receptor channel otherwise 
excitatory neurotransmitter glutamate bind 
receptor opening channel ions pass 
presynaptic activation glutamate release postsynaptic 
activation present order nmda 
channels open 

nmda channels open allow calcium ions 
enter postsynaptic neuron low base 
concentration neuron new calcium able 
trigger complex chemical processes ultimately 
results modification synaptic efficacy weight 
primary excitatory input receptors receptors 
discussed 
number factors 
pre postsynaptic lead modification overall 
synaptic efficacy debate factors important 
far resolved appear pre postsynaptic 
factors involved ltp learning rules 
depend nature events trigger 
synaptic modification mechanisms actually implement 

sketch biological mechanisms lead ltp 
nmda channels open postsynaptic membrane potential 
sufficiently glutamate released 
presynaptic neuron allows calcium ions 
postsynaptic neuron triggers complex chemical 
processes ultimately result modification synaptic 
efficacy 

ltd well understood 
presented data ltd occurs synapse active 
lower level required trigger ltp 
explanation finding effective opening 
nmda channels channels opened 
time results lower concentration calcium ions triggers 
different chemical processes ultimately end 
reducing synaptic efficacy 
shows illustration 
relationship ltp ltd threshold ltd 
higher ltp threshold 
relationship consistent hypothesis nature 
complex chemical 
further form ltd 
consistent ltd necessary model learning 
necessary task learning explained 

relationship ltp ltd 
amount increased calcium leads 
ltd larger amount leads ltp 

biological picture complicated 
number reasons 

receptors channels play role 
ltp example evidence 
glutamate receptors 

number ways calcium enter postsynaptic 
neuron non nmda dependent voltage calcium channels 
ltp number 
dopamine etc ways 
understood appropriately experiments 
evidence standard 
occurs natural activation patterns 
cortex generally ltp sensitive particular 
combinations activation signal properties timing 
frequency etc ways 
explored empirically important biologically 
plausible driven error learning 
impact nmda receptor show 
preserved measured learning nmda 
generally difficult interpret 
nmda significant effects behavior 
addition preserved learning observed 
accounted 
simple nmda story role 
nmda non factors gated voltage calcium channels 
receptors 

story least level biological mechanisms 
end involved mediated nmda 
described 

number biological 
data suggests associative learning occurring 
cortex generally state data serves 
need computational models explore kind 
synaptic modification rules lead effective overall learning 
details appear difficult extract 
biology 

sketch objective model learning 
produce internal model represents important 
features world complicated true state 
world hidden see bunch 
projections world state 
somehow produce corresponding state 
model 

section develop level computational motivation 
general goal learning see goal accomplished 
biological mechanisms discussed previous 
section call goal model learning order emphasize 
idea learning towards developing internal 
models world shows 
illustration internal model captures important 
features world difficult thing 
basic idea kind underlying 
structure regularities natural constants general 
characteristics world somehow 
represented order function properly example highly 
reasonably accurate representation effects 
seems 
fairly learn things least ones 
apparent took understand 
fundamental role hidden 
think model learning fairly easy 
automatic case least 
cognitive trying human brain 

fundamental problems model learning 
nature sensory 
underlying structure world 
amount information senses 
seem 
senses large low quality information 
highly processed order produce apparently 
world experience see quality 
problem introducing appropriate 
built start biases 
organize information information problem 
addressed biasing learning favor simpler 
parsimonious models end kinds information 
favor representing relevant information 
form techniques recognized way 
science itself works explicit 
extension model learning process 
same problems 

problem world via senses 
receive series relatively limited dimensional 
sound etc thought 
mathematical term dimensional higher 
matrix onto lower dimensional think 
real world projections 
dimensional high world onto dimensional lower sensory 
process world states 
back world job model 
learning difficult information 
projection onto senses characterized 
situation saying problem 
input data sufficiently constrain interpretation 
difficult large number possible 
internal models fit sensory input data equally well 
put way difficult know real underlying 
causes 
noise 

important way model learning 
problem easier integrating 
individual experiences single 
ambiguous noisy time 
science known believe 
phenomena reliably demonstrated different 
individual experiments different etc law large 
numbers noise averaged away integrating 
statistics large sample see integration 
process critical successful model learning 
naturally performed slowly adding small weight changes 
resulting weights represent statistics 
large sample experiences network ends representing 
stable patterns emerge wide range experiences 
world 

experiences 
enable development good internal model example 
averaged pixel images experienced 
retina big 
needs prior biases 
kinds patterns particularly informative organize 
structure representations way sense 
general structure world biases provide reasonable 
fit properties actual world model learning 
task easier reliable individuals 

example lets imagine task 
learning control complex new 
explicit help available playing 
know advance kinds actions 
generally sets likely 
control actions words set 
appropriate biases structure internal model 

details experiments playing time 
contrast complete resort 
randomly systematically depending trying 
corresponding responses 
slowly 
actions 
button task 
difficult space possible contingencies 
greater appropriate biases model learning 
easier essential keep mind biases 
wrong completely 
learning take longer biases 
trying subset important 
ones tried 

example biases came prior experience 
same kinds learning processes 
type biasing happen model 
learning networks interested kinds biases 
present start learning 
assume evolution built years good set 
biases human brain facilitate ability learn 
world essential role genetic 
learning emphasized 
neural network learning algorithms characterized 
completely blank learning systems 
reflect dynamic genetic biases 
based experience learning difficulty genetic 
biases typically obvious easily 

useful contrast kinds genetic 
important biasing neural network learning 
typically discussed psychologists 
emphasize genetic contributions 
emphasized think terms 
people specific knowledge representations 
knowledge solid things generally 
neural network terms require 
building detailed pattern weights relatively 
implausible information 
contain difficult expressed 
biologically development contrast biases 
areas generally connected areas 
biases fast area learns compared 
inhibition presumably relatively 
easily encoded expressed actions 
factors development subtle 
differences biases lead important differences 
learning easy exact nature 
biological biases shape learning see 
general aspects biology networks specifically role 
inhibition biology learning specifically associative 
hebbian character serve important biases model learning 

role biases learning long 
statistics goes name 
variance bias 
rely biases strongly 
learn actual experiences end 
getting wrong model think experienced 
important new favor familiar 
ones hand rely experiences 
strongly assuming 
experiences end different models reflect 
lot model 
variance assuming real underlying state 
world means lot wrong models proper 
biases experience true 
generally optimal solutions 

history research contains examples biases 
critical sense phenomena 
otherwise best 
known examples bias science negative ones 
ability understand world 
truly science provides example 
nature biases 

biases science 
parsimonious bias 

favor simplest explanation 
phenomenon primary practical advantages 
developing parsimonious models world results 
greater generalization application 
models novel situations think way 
bunch specific facts world trying 
extract simpler essential regularity underlying facts 
novel situation 
relevant example encode 
situation 
away part 

part see model 
learning favor developing relatively simple 
general models world ways related 
role associative learning inhibition 

positive correlations exist elements 
feature line model learning represent 
correlations 

main idea model learning 
based learning correlations environment 
understand useful pay attention correlations consider 
individual pixels picture elements image 
line see pixels 
active line present input producing 
positive correlation activities correlation 
reliable present different input images extent 
reliable world tends 
produce lines edges objects model learning 
pay attention correlations general seems 
world things relatively stable features 
tree individuals face 
eyes nose features 
reliable correlations input further 
model strongest 
reliable features components correlation matrix 
well see next section hebbian learning 
cause units represent strongest correlations 
environment 

detailed analysis hebbian learning 
well explore simple case shown 
simulation exploration 
see single unit hebbian learning rule 
explained detail learns represent correlations 
present pixels line 

begin open project 
input layer single receiving hidden unit graph log 
showing weight values addition usual windows 
present single right diagonal line 
see effect hebbian learning hidden units 
weights lets look initial weights hidden unit 
selecting variable view network 
window clicking hidden unit see random 
looking pattern weight values click back 
control panel 
see activation right diagonal line 
click back see units 
weights learned represent line environment 
click again see entire 
random weights line representation looking 
weights 

simple point exploration hebbian 
learning tend cause units represent stable things 
environment particularly simple case 
single thing environment serves point 
well explore interesting cases lets 
try understand mathematical basis hebbian 
learning 

general mathematical framework understand 
hebbian learning causes units represent correlations 
environment called principal components analysis pca 
name suggests pca representing major principal 
structural elements components correlational structure 
environment arbitrary pca 
refers principal 
components correlation sequential order 
strongest last term 
refer strongest components focusing 
principal components correlational structure framework holds 
developing reasonably parsimonious model 
provides useful mathematical level overall analysis 
understand effects learning further 
see pca implemented simple associative 
hebbian learning rule mediated nmda synaptic 
modification described 

follows develop particularly useful form hebbian 
learning performs version pca rely combination 
top mathematical bottom 
weights adapted find nice 
levels analysis order 
fundamental computations clear start simplest form 
hebbian learning problems 
algorithm actually simulations 
important keep mind different 
versions developed common 
extracting representing principal components correlational 
structure environment hebbian learning 

lets focus simple case single linear receiving unit 
gets input number input units imagine 
environment produces patterns input units 
certain correlations input units 
lets consider simple case environment 
line shown 
repeatedly presented set input units 
linear receiving units activation function weighted 
sum inputs see diagram 
usual input units 
reasons clear subsequent 
equations chapter variables implicitly 
function current time step input pattern writing 
variable things read finally 
lets assume weights unit learn time step 
input pattern according simple hebbian learning rule 
weight change pattern depends 
activities pre postsynaptic units 
follows 
learning rate parameter index 
particular input unit expression weight change enables 
update weights follows 

want know going happen weight value 
result learning input patterns easily 
expressed sum time 
again time different input patterns 
now lets learning rate 
set value total number 
patterns input arbitrary constant anyway 
turn sum average 
notation indicates average 
expected value variable patterns 

equation formula 
index sum inputs linear function 
activities input units find 
bit weight changes function 
correlations input units 
new variable 
correlation matrix input units 
correlation defined here expected value average 
product activity values time 
familiar 
standard correlation measure 
away mean values variables 
taking product result 
ignore additional 
time assuming simplicity 
need assumptions later activation variables zero 
mean unit variance 

correlations computed via 
simple hebbian learning algorithm 

main result 
changes weight input unit weighted average 
different input units correlation 
input units particular input unit see 
diagram 
computation strong correlations exist input 
units weights units increase average 
correlation value relatively large interestingly run 
learning rule long weights dominated 
strongest set correlations present input 
strongest set next strongest increasingly 
large simple hebbian rule learns 
strongest principal component input data 

demonstration simple hebbian algorithm 
units perfectly correlated 
completely input pattern 
computed change 
weights added next time step 
computed 

simple concrete demonstration learning rule shown 
input units 
single linear output unit different input patterns 
units perfectly correlated 
perfectly units zero mean 
assumed notice correlated units 
determining sign magnitude 
hidden unit activation ensures weights 
keep increasing ones weights 
run patterns 
notice weights correlated ones increase 
rapidly remains 
small due 

mathematically say simple 
hebbian learning rule weights towards strongest 

correlation matrix seen 
notation 
simplification weight itself 
expected value assuming changing relatively 
slowly matrix serves update function 
simple linear system state variables represented 
weight well known results 
state variables dominated strongest 
component update matrix 

problem simple hebbian learning rule weights 
large learning continues obviously 
good thing relatively simple 
weight updating weights remain 
end exactly algorithm 
similar explain influential version hebbian 
learning achieves weight algorithm 
proposed developed following modified 
hebbian learning rule away portion weight 
value order keep large 
learning rule complex see 
considering simple case input 
pattern avoid need average multiple patterns 
look stable value weight learning long 
time pattern order simply need set 
equation equal zero tell 
equilibrium weight values reached 
note same 
find equilibrium membrane potential 
weight input unit end representing 
proportion inputs activation relative total weighted 
activation inputs keep weights 
bound finally primarily based 
same correlation terms previous simple hebbian 
learning rule rule computes principal 
component input data 
involved 

problem rule fact simple hebbian 
pca well linear 
activations work properly point neuron 
activation function thresholded subject 
effect large activations 
substantial problem pca 
extension case multiple hidden units consider 
happen added multiple hidden units same 
activation function learning rule unit analyzed 
end learning exact same pattern weights 
strongest principal 
component input correlation matrix algorithms 
find 

general ways problem 
involve introducing kind interaction hidden units 
order different things problem here 
form weight update rule 
overall activation dynamics hidden units 
important relationship 
activation dynamics learning level 
perfect sense unit behaves activation dynamics 
going affect learns see repeatedly 

approach explored solving problem 
redundant hidden units introduce specialized lateral 
connectivity units configured ensure 
subsequent units end representing sequentially weaker 
components input correlation matrix 
explicit imposed hidden units 
unit ends representing principal component 
strongest correlations next unit gets next strongest 
etc call sequential principal components analysis 
spca solution reasons 
level computational principles available data 
neurons encode information 

sequential pca spca performed small 
images natural scenes principal 
component blob upper left subsequent 
components following right square 
large grid shows grid receiving weights hidden 
units common layer input units figure 

important level computational issue 
spca assumes input patterns share 
common set correlations represented 
principal component individual patterns 
sequentially distinctions 
represented subsequent components amounts 
assumption hierarchical structure 
central tendency shared environment 
individuals special cases overall principal 
component contrast seems world 
lots separate categories things 
exist roughly same level seen 
shows spca algorithm applied 
small images natural scenes 
scenes contain lots line 
different orientations positions sizes 
largely images world see 
principal component big blob 
average lines individual image pixel 
different lines correlations present 
line away completely thing 
left general correlation close pixels 
general close pixels tend similar values 
blob subsequent components essentially divide 
blob shape sub representing residual 
average correlations exist away big blob 

conditional pca cpca performed small 
images natural scenes results 
properties simple cells early visual cortex 
respond bars light specific orientations 

way visual system seems represent images 
images line different 
orientations sizes etc neuron responding 
small coherent category line properties 
neuron fire strongly short lines 
degree activity graded fashion 
lines differ preferred line examples 
weight patterns shown 
see description network 
produced weight patterns 

problem spca computes correlations 
entire space input patterns meaningful 
correlations exist particular subsets input patterns 
example somehow restrict application 
pca hebbian learning rule images lines roughly 
particular orientation size length etc present 
end units encode information essentially 
same way brain units represent 
correlations pixels particular subset lines 
way expressing idea units represent 
conditional principal components 
pca computation subset input cases 
consistent ideas regarding 
importance relatively sparse representation 
unit active subset input cases see 
related ideas 

issue precisely specify conditions 
unit perform pca complex 
avoid moment simply assuming activity 
receiving units determined external source turns 
units input contains things representing 
turns see later inhibitory 
competition play important role conditionalizing 
process task based learning further weights 
learned pca procedure itself conjunction 
inhibitory competition result organization self 
representations selective weights cause units active 
features represent present input 

section develop version hebbian 
learning specifically purpose performing 
conditional pca cpca type learning see 
resulting form learning rule updating weights 
similar normalized pca learning rule presented 
emphasized critical difference 
learning rule itself activation 
dynamics determine individual units participate 
learning different aspects environment 
developing cpca learning rule develop slightly different 
way objective hebbian learning better 
suited cpca idea problematic 
assumption linear activation function previous versions 
pca cpca rule consistent notion individual 
units hypothesis detectors activation states 
understood reflecting underlying probability 
existing environment 

cpca learning rule starts taking conditionalizing idea 
assumes want weights input unit 
represent conditional probability input unit 
active receiving unit active 
write 
form simplified notation continue 
call learning rule achieves 
weight values cpca algorithm learning objective 
analyzed slightly different context competitive 
learning algorithm 

important characteristic cpca represented 
weights reflect extent 
input unit active subset input 
patterns represented receiving unit typical 
characteristic aspect inputs weights 
large typical small 
think weights reflecting kind 
correlation input unit receiving unit 
conditional probabilities correlation 
terms conditional probability means zero correlation 
equally likely receiving unit active 
values larger indicate positive correlation likely 
receiving unit values 
indicate negative correlation likely 
receiving unit note least 
important difference conditional probabilities depend 
direction compute general 
whereas correlations come same way regardless way 
compute 

fact cpca computes correlation 
input receiving unit interesting pca based 
computing correlation input 
receiving unit pca receiving units linear 
activation directly reflects input activations 
output input correlation ends reflecting correlations 
different input units see cpca sensitive 
input correlations capable reflecting 
important conditionalizing factors determine receiving 
unit active competition amongst hidden units 

following analysis show 
following weight update rule achieves cpca 
conditional probability objective represented 

again learning rate parameter 
equivalent forms equation shown order emphasize 
similarity learning rule normalized pca learning rule 
shown showing simpler form 
main difference 
latter square activation times 
weight activation times weight 
expect produce roughly similar weight 
changes difference way works 
note activations cpca positive 
probability values difference activation 
affect sign weight change 

form emphasizes 
following interpretation form learning 
weights match value 
sending unit activation close possible 
difference weighted 
proportion activation receiving unit 
receiving unit active weight occur 
effectively receiving unit care happens 
input unit itself active receiving unit 
active lot input units 
activation set weight match 
individual weight changes slow learning 
rate averaged weight come approximate expected value 
sending unit receiver active words 

next section shows weight update rule 
shown implement 
conditional probability objective 
mathematically analysis 
significantly ability understand 
follows 

analysis based 
same technique understand normalized pca 
learning rule work backwards weight update 
equation equation setting zero 
solving equilibrium weight value show 
weights converge case conditional probability 

need relevant variables 
terms activations sending receiving units 
assumed represent probabilities corresponding 
units active consistent analysis point 
neuron function terms bayesian hypothesis testing presented 
expression 
represent probability receiving unit active 
particular input pattern presented 
represents corresponding thing sending unit 
total 
weight update computed possible patterns 
probability pattern occurs 

set zero order find 
equilibrium weight value solve resulting equation 
value results following 
now interesting thing note here 
actually definition 
joint probability sending receiving units active 
patterns 
similarly 
probability receiving unit active patterns 
equation 
point clear fraction joint 
probability probability receiver 
definition conditional probability 
receiver right 
end 

noted cpca hebbian learning rule 
essentially same learning 
rule competitive learning algorithm 
found 
pca soft competitive 
learning 
style networks see 
information 
algorithms commonly form 
hebbian learning simulation models addition 
pca idea number different ways 
effects learning rule 

competitive learning algorithm causes weight 
move towards input data clusters 
showed 
informative 

addition presenting conditional probability analysis 
learning rule roughly 
argued useful 
weights towards input data clusters 
shown idea 
clusters data important represented 
units network easy see strongly correlated 
input patterns tend form clusters sense 
way looking pca idea 
incorporates additional assumption multiple 
separate clusters different hidden units specialize 
representing different clusters 
conditionalizing idea unit learns 
patterns somehow relevant cluster competitive 
learning conditionalizing occurs via simple take 
competition described 
unit allowed active input pattern 
units weights come represent cluster 
likely cluster automatically 
resulting specialization units subsets input 
according clusters process 
organizing self learning 

mentioned previously problem simple 
competitive learning network single active hidden unit 
provides localist representational basis 
powerful distributed representations 
require feature cluster present input 
time clearly limits 
algorithm kwta 
activation function see 
allows multiple units representing input 
pattern unit representing features 
present simultaneously input same time 
sufficient competition units appropriately 
responses meaningful subset input patterns 
indicated inhibitory competition plays important role 
conditionalizing units learning 

important analysis hebbian learning related 
pca idea idea information 
idea here model learning develop 
representations amount information 
input patterns turns principal component 
correlation matrix information possible 
single unit causes units output 
amount variance input patterns variance 
information idea 
information placed context certain constraints 
taken extreme result development 
representations capture information present 
input result 
relatively parsimonious representations 

better consider role hebbian learning 
context tradeoff information 
complexity representations 
tradeoff represented framework known 
minimum description length 
clear inhibitory competition results 
specialization helps produce parsimonious models 
overall information capacity hidden layer 
works balance information objective 
emphasized form hebbian learning cpca 
extracting principal component subset 
input patterns receiving unit active 
learning rule itself addition inhibitory competition 
pressure develop relatively parsimonious model 
input patterns principal component relatively 
informative far capable representing information 
present input patterns 

different ways hebbian 
learning studied 
context cpca learning 
rules viewed 
weights effectively divided 
factor amounts probability 
receiving unit active case cpca due 
conditional probability form possible 

weight order control 
found 
necessary getting model hebbian learning 
early visual cortex produce receptive fields 
individual neurons specialize representing inputs 
initially receiving inputs see 
adding contrast enhancement factors cpca 
learning rule provide benefits 
increasing selectivity representations 
main limitation weights end 
bound whatever placed 
turns useful practice 
graded intermediate weight values necessary cases 
simple cases studied earlier 
version leabra algorithm combination 

turns combined 
soft weight bounding necessary keep weights 
graded learning rule implements 
cpca algorithm described 
due 
effects soft weight bounding algorithm 
perform well 

described beginning chapter likely biological 
mechanisms underlying weight changes cortex showed 
generally support hebbian associative type learning 
cpca learning rule slightly complex simple 
product sending receiving unit activations requires 
further explanation see account 
general characteristics weight changes cpca 
learning rule same basic 
mediated nmda ltp mechanisms described previously 
easier reference cpca equation 
pass seeing biology implement 
equation lets assume weight value 
well consider effects different weight 
values moment assumption general 
categories weight changes produced cpca 
sending receiving units strongly active 
weight increase ltp 
easily account associative nature mediated nmda 
ltp 
receiving unit active sending unit 
ltd occur explained 
nmda channels open function postsynaptic 
activity small amount presynaptic 
activity causing small zero level calcium 
lead ltd possible postsynaptic 
activity activate gated voltage calcium channels 
provide weak concentrations calcium necessary 
ltd presynaptic activity 
receiving unit active likelihood 
magnitude weight change goes zero explained 
nmda channels 
lack activation gated voltage calcium channels 
lead postsynaptic calcium weight changes 

finally effect cpca learning different values weights 
summarized follows weight large 
further increases happen 
likely larger weight smaller 
magnitude decreases show opposite pattern 
conversely weight small increases likely 
larger magnitude opposite holds decreases 
general pattern exactly observed empirically amounts 
ltp ltd upper 
lower respectively thought form 
soft weight bounding upper lower 
case soft manner slowing weight 
changes return 
issue context task learning later section 

exploration revisit simulation ran 
beginning chapter see single unit learns 
response different patterns correlation activity 
set input patterns conditionalizing 
activity receiving unit shape resulting weights 
emphasize feature present subset input patterns 
find need introduce additional 
factors learning rule order emphasis 
effective factors important 
organizing self case explored subsequent section 

begin open project 
want weights hidden unit 
learns select variable view network 
window click hidden unit now select 
control panel choose 
environment window events right 
diagonal line left 
sets correlations exist simple environment 
manipulate percentage time receiving unit 
active conjunction events order 
conditional probabilities drive learning cpca learning 
algorithm simple case explored earlier 
right line presented setting probability 
left lines probability 

view probabilities frequencies associated 
event locate parameter view window 
upper left hand side select thing 
displayed event display 
right side window see 
event means receiving 
unit active time conjunction 
right diagonal line time 
left note completely irrelevant 
times receiving unit inactive conjunction 
patterns weight change 
case frequencies 
conditional probabilities associated event 
events arbitrary probability 
appearing environment parameter 
control panel determines frequencies events 
environment event set 
lets set value 
ahead iconify environment window continuing 

important keep mind exercises 
single receiving unit multiple 
receiving units looking same input patterns want 
unit specialize representing correlated features 
environment lines case 
manipulate specialization conditional probabilities 
weighted towards event 

now press button control panel run 
network sets epochs randomly event 

event event value 
cpca hebbian learning rule 
applied event presentation weights 
updated see display weights 
network window updated epochs 
graph log display value right 
weights red left 
weights orange notice 
learning weights units active 
central unit present events 
weight weights event 
expected cpca learning rule causes 
weights reflect conditional probability input 
unit active receiver active experiment 
different values verify holds 
different probabilities 

parameter control panel corresponds 
cpca learning rule 
determines rapidly weights 
updated event change 
affect general character weight updates displayed 
network window explain happens explain 
importance 
integrating multiple experiences events learning 

set parameter back explored 
different values effectively 
selective receiving unit type 
event taking advantage 
conditional aspect cpca hebbian learning effectively 
conditionalizing representation input environment 
frequency 
events occurred environment think 
frequency receiving unit active 
events 

now want compare conditionalizing aspect cpca 
pca algorithm lets assume event 
equal probability appearing 
environment setting 
simulate effects standard form pca 
receiving unit effectively entire 
environment cpca receiving unit active 
lines present environment 
result lead weights weight pattern 
suggest existence separate diagonal line features 
existing environment compare 
blob solution natural scene images discussed 
shown 

set simulate hidden unit 
controlled way come 
right diagonal line input left 
result lead weights explain 
result informative case explored 
previous question extend architecture 
training network represent environment fully 
way explain answer 

simple environment weve far realistic 
assumes mapping input patterns 
categories features typically want represent 
lets switch environment pressing 
environment notice now different versions 
left right diagonal lines upper 
lower addition original center lines 
environment spread types 
right lines 
general category right lines 
left lines 

set see 
right lines weights left 
lines weights result illustrates 
couple problems cpca learning algorithm 
units represent categories features single instances 
weights end due fact 
receiving unit active different input patterns 
conditional probabilities individual pattern 
relatively small happens receiving 
unit perfect selectivity category 
features right left case 
crucial early phases learning reasons 
clear later differences weight magnitude 
input units selected category right 
categories left small sense cpca 
algorithm actual conditional 
probabilities emphasize selectivity 
receiving unit small overall weight values reduce 
dynamic range weights end 
inconsistent weight values produced task learning 
algorithm described later next section shows problems 
fixed come back simulation again 
see work practice 

cpca algorithm results normalized weight values 
saw previous section tend selectivity 
dynamic range introducing correction 
factors way weights 
taking account sparse expected activity level 
sending layer way 
contrast weak strong weights correlations 
computing effective weight value sigmoidal 
function underlying linear weight value cpca 
computed changes affects basic 
underlying computation performed cpca simply 
effective further 
continuum standard cpca weights 
weights effects clearly understood 

note exploration next section clear 
effects correction factors better job 
verbal arguments provided here 
fully understand section 
continue exploration reading section 
lot sense 

automatically expected activity level 
layer represented variable compute 
net input projection see 
automatically cpca learning rule 
idea expected activity level reflects expected 
conditional probability receiving unit active 
input unit chosen random active correct 
fairly sparse activity levels lack 
correlation input receiver produce weights 
values expected activity level 
correct idea 
conditional probability indicates lack correlation 
larger values indicate positive correlation smaller values 
negative correlation procedure 
weights standard range 

turns best way accomplish renormalization 
simply increase bound upper weight increases 
form cpca update weight equation 
simple verify form 
equation equivalent form 
understood analogy membrane potential update equation 
excitatory 
conductance term drives weights towards 
reversal potential term 
inhibitory conductance drives weights towards 
reversal potential correct 
sparse activations excitatory reversal 
potential greater increase range 
weight values produced 
new maximum weight value purposes 
learning weights standard range 
resulting potential resolution value 
conditional probability happens practice 
linear relationship 
true underlying conditional probability equilibrium weight 
value turns 
obvious way expected activity levels 
input produces relationship weight 
actual conditional probability learning 
things appear correlated actually set 
correction factor following equation 
expected activity level same 
standard cpca smaller values produce relatively larger 
weights 

finally order convenient specify general way 
correction factor want 
introduce parameter simulator 
close actual 
starting expected activity 
level compute effective activity level 
follows 
compute 
renormalization occurs weights 
fit range 

renormalization amounts relatively simple modification 
cpca learning rule contrast enhancement requires 
significant changes level 
simpler contrast enhancement important 
contribution well worth additional complexity 
subsequent simulations show further important keep 
mind implementation here involves different 
weight values 
introducing contrast enhancement effects 
learning rule itself essentially amount 
form soft weight bounding 

implement contrast enhancement sigmoidal function 
provides obvious mechanism contrast enhancement function 
gain parameter determines gradual 
function enhance contrast weight values 
linear relationship weight values 
turn conditional probabilities computed cpca algorithm 
sigmoidal relationship mediated gain parameter 
biologically speaking amount sensitivity 
weight changes weight values middle range opposed 
plausible directly supported 
data far know 

note kind contrast enhancement weights 
equivalent effects standard gain parameter 
activation values changing activation gain unit 
sensitive differences total net input sending 
activations times weights contrast changing contrast 
enhancement weights affects weight value separately 
allows unit sensitive individual input 
level total input put way weight 
contrast enhancement unit sensitive 
detecting patterns inputs whereas activation contrast 
enhancement net response sensitive 
threshold increase quality signal coming 
unit 

simulator implement weight contrast enhancement 
introducing effective weight computed 
underlying linear weight following sigmoidal 
function 
weight gain parameter 
simulator controls extent contrast enhancement 
performed note function derived same 
form repeatedly 
text 
difficult gain parameter equation 
latter form see plot 
function standard parameter 

effective weight value computing net inputs 
units standard value simulator 
linear weight value simulator 
internal variable computing weight changes basically 
sure averaging events results appropriate 
conditional probability values 
difficult measure linear weight value biologically 
effective weight value measured result activating 
sending unit excitation produced 
receiving fact said possible 
computations effective weight value 
computationally simpler keep values 

effective weight value function underlying 
linear weight value showing contrast enhancement correlations 
middle values conditional probability 
represented linear weight value note 
function offset parameter 

add additional parameter function 
controls offset acts threshold 
useful higher threshold correlation order 
further enhance contrast different features present 
input offset parameter 
simulator introduced effective weight value equation 
follows 
values greater higher threshold 
underlying linear correlation values weight value 
results value shown 

important note point threshold 
dividing line 
contrast enhancement process values 
sigmoidal 
introduced important 
different conditional probability values relative 
contrast enhancement threshold renormalization 
parameter plays important role addition obvious 
importance parameter see 
following simulations 

think effective weight function bias 
learning algorithm favors extreme 
weight values controlled parameter 
bias requires particularly strong correlations strong 
weights controlled parameter see 
biases appropriate range different learning 
tasks 

finally way thinking contrast enhancement 
context effects soft weight bounding 
property cpca algorithm weights constrained 
range approach 
slowly difficult units develop 
highly selective representations produce strong activation 
input patterns weak activation contrast 
enhancement limitations 
advantages soft weight bounding 

cpca 

open already project 
view weights hidden unit going 
explore renormalization weights taking account 
expected activity level input layer 
line stimuli units active units 
input layer value set lets explore 
issue environment features zero correlation 
receiving unit see renormalization results 
weight values case 

control panel pick 
environment see contains horizontal lines 
presented equal probability 
line features represent zero correlation case 
occur receiving unit same probability 
expected activity level input layer words 
expect same level occurrence simply 
input units random overall activity level 
input network see 
due behavior cpca algorithm 
reflecting conditional probabilities weights 
end learning 
weights terms standard meaning conditional probabilities 
represents zero correlation conclude 
input units correlated receiving unit 
know correct sparse activity levels 
input 

now set note variable name means sending 
average activation 
parameter control panel 
value means now applying 
full correction average activity level sending 
input layer network again observe 
weights now correct value 
expressing lack correlation ability fully 
correct sparse sending activations useful 
want particular prior expectation 
individual input patterns represented 
hidden unit set appropriately 
level corresponds roughly prior expectation 
example know units relatively 
selective representations input features 
unit want set 
full correction input layer result 
larger weights features relatively weakly correlated 
compared expected level selectivity units expected 
represent number input features value 
issue 

now lets explore contrast enhancement function 
effective weights parameters 
control panel control gain offset 
function lets set 
result substantial contrast enhancement see shape 
resulting effective weights function 
button control panel press file 
selection comes see sigmoidal function 
plotted graph log window bottom screen try 
setting different values 
effect shape function 

order see effects learning lets 
baseline comparison see 
blob representation right lines bit 
strong left lines now set 
see right lines represented relatively 
strong weights contrast enhancement allows network 
represent reality distinct underlying left right 
categories features selective 
features important organizing self 
learning see 

now lets parameter encourage network 
pay attention strongest correlations 
input set 
see affects effective weight function 
back forth couple times able 
see difference 

set network 
change results compared case 
explain occurs find value 
central non overlapping non units right lines 
units lower left units 
upper right weights 
resulting weights accurately reflect correlations present 
single input pattern imagine representation 
useful cases 

alternative way accomplish effects 
value described units 
selective weak correlations 
high weight value 

set back set 
effect learned weight values 
compare parameter found previous 
question 

last question shows contrast enhancement 
big effect changing amount 
correlated activity necessary achieve value lower 
correlated inputs parameter large 
smaller values towards zero causing unit 
essentially ignore inputs interactions 
contrast enhancement renormalization play important role 
determining unit tends detect 

already times ability network 
hebbian learning inhibitory competition 
organize self internal model environment context 
conditional pca learning algorithm organization self 
amounts competition set receiving 
units way conditionalizing responses units 
unit active extent 
strongly activated current input pattern units 
happen weights unit 
sufficiently well tuned input pattern cpca 
learning algorithm causes tuning weights input units 
active receiving unit active 
effectively positive feedback system initial selectivity 
set input patterns learning 
algorithm producing greater selectivity 

positive feedback systems potential 
positive feedback saw bidirectional 
excitatory connectivity previous chapter 
organizing self learning case individual receiving units 
end representing number input features 
receiving units represent features 
important phenomenon happens 
learning causes units tuned 
subset input patterns unit ends representing 
set patterns causes unit likely 
activated ones example consider case unit 
represented right diagonal line 
explorations appropriate contrast enhancement parameters 
learning unit caused weights decrease 
left diagonal line increased right diagonal 
line unit likely respond 
left diagonal lines allow unit 
competition case resulting good representations 
types lines 

continue lines exploration 
set hidden units environment consisting 
horizontal vertical lines input retina start 
opening project see number 
windows lets focus network input projects 
hidden layer hidden units fully connected 
input random initial weights usual select 
view weights units viewing pattern 
weights hidden units primary 
network learns special grid log window displays 
weights hidden units see press 
control panel display 
weights grid log window lower right side screen 
scale larger grid same 
network grid elements 
smaller grid representing input units showing weights 
unit clicking hidden units network window 
able verify 

now lets see environment network 
press control panel bring 
window showing events representing different combinations 
vertical horizontal lines events 
unique combinations type line 
real correlations lines reliable 
correlations pixels particular line 
putting way line thought appearing 
number different randomly related 
lines pretty obvious computed 
correlations individual pixels images 
equally weakly correlated 
learning conditional particular type line 
order meaningful correlations see 
simply organize self 
interactions learning rule kwta inhibitory 
competition note lines present 
image network require least active hidden units 
input assuming unit representing particular line iconify 
environment window done examining patterns 

return viewing network window lets 
step bit processing network default 
network button turned turn 
clicking button upper left hand network 
window now locate process control panel 
weights grid log adjacent network 
window press button hit 
present single pattern network see 
event patterns containing lines input network 
pattern roughly active hidden units hidden layer 
based average kwta inhibition function parameter set 
see parameter 
control panel function allows variability actual 
activation level depending actual distribution excitation 
units hidden layer units 
active units fairly equally activated input 
pattern due random initial weights selective 
important effect weaker additional activations 
enable units bootstrap stronger activations 
gradual learning end reliably active 
conjunction particular input feature particular line 
case 

single press button 
process control panel want turn 
epochs different events 
environment learning network 
weights grid log updated epochs 
weights came clearly reflect lines 
present environment individual units developed 
selective representations correlations present 
individual lines random context 
lines happened result individual units developing 
initial weak selectivity function random weights 
caused inhibitory competition subset 
images contained elements selective 
turn conditional pca learning 
increase selectivity etc 
units initially selective multiple lines 
units better able represent lines 
competition back representing line 
dynamics inhibitory competition critical 
organizing self effect finally case 
representations multiple 
hidden units encode same line feature 

net result organizing self learning nice 
combinatorial distributed representation input pattern 
represented combination line features present 
obvious way represent inputs 
network 
representation complex organizing self 
learning procedure see representation action turn 
network back 
general units strongly activated 
input pattern extra activation reflecting fact 
lines coded multiple units flexibility 
based average kwta function allows observed flexibility 
network patterns same time providing 
sufficient competition force specialize 

main thing notice weights shown grid log 
units obviously selective 
units aka units reliably activated 
input feature experience learning 
typically important units 
organization self requires learning 
units stable correlational features 
hidden units increases large 
range initial random organization self 
process consequence need units 
necessary end 
presumably problem brain worse 
algorithm didnt work unless precisely right 
number units idea 
brain neurons actually needs 
units activated new features 
later presented network diagonal lines act 
kind 

looking weights informative 
measure well networks internal model matches 
underlying structure environment measure 
plotted graph log network window network 
learned shows results unique pattern statistic 
simulator number 
unique hidden unit activity patterns produced result 
network different types horizontal 
vertical lines presented separate 
testing process epoch learning goes 
tests network lines resulting hidden unit 
activity patterns kwta parameter set 
critical due flexibility based average kwta 
function number unique patterns 
measure line encoded least 
distinct hidden unit show unique pattern 
units encode lines 
good model environment 
result unique representation lines resulting 
measure lower extent statistic 
internal model produced network 
fully capture underlying line 
lines 

seen run model eventually produced 
perfect internal model according statistic well 
analysis weight patterns order better 
sense well network learns general run batch 
training runs starting different set random initial 
weights time press button 
repeatedly pressing begins 
new random setting random initial weights 
updates end training run graph log 
updated training runs text log 
window upper right screen show summary 
statistics average maximum minimum unique 
pattern statistic last column contains count number 
times perfect perfect 
runs 

now lets explore effects parameters 
control panel lets manipulate 
parameter affect contrast 
selectivity units weights 

set run 
network statistics number uniquely represented 
lines obtain ways final weight patterns 
shown weight grid log different default case 
explain findings related specific 
reference role selectivity organizing self learning 

set back change 
run batch statistics obtain case 
change weight patterns compared 
default case explain results terms effects 
function contrast 
enhancement mechanism again explain important 
organizing self learning 

now lets consider parameter controls 
amount renormalization weight values based 
expected activity level sending layer value 
parameter weights increase rapidly 
driven larger maximum value see 
value result smaller weight increases described 
smaller values appropriate 
want units selective representations larger 
values appropriate general categorical 
representations smaller value parameter 
help prevent units developing selective representations 
multiple lines default value 
parameter 

set back set 
case explain result terms weight 
patterns observed weight grid log 
training run compare general effect parameter 
parameter 

set back set 
sets initial random weight values mean value 
batch run network pay particular attention 
weights see 
units unit now line 
illustrates interesting details organizing self 
learning effects learning overall 
weights general cpca rule causes weights 
increase input units active decrease 
amount weight decrease large 
relative amount increase input patterns 
active units start relatively large 
initial weights unit active pattern 
likely later similar 
identical pattern pattern containing lines 
common previous pattern weights 
overall increased happens 
increase initial random weight values results 
units useful effect 
network sufficient numbers units 
ends based task learning 
see later 

finally lets manipulate learning rate parameter 
set back set 

increase learning rate 
effect network explain case hint 
representations integrate multiple patterns 

exercise feel dynamics 
organizing self learning importance contrast 
enhancement order cpca algorithm effective 
generally now extent 
parameters provide appropriate biases 
learning process benefit produce 

basic process organizing self learning explored 
characteristic number models including competitive learning 

networks primary advantage model 
weve here traditional approaches comes 
kwta form inhibitory competition allows 
arbitrary combinations multiple units represent input 
pattern obviously critical exploration 
able learn powerful distributed 
representations organizing self manner 

intermediate case single wta kwta 
based neighborhood networks 
organizing self learning applied 
neighborhood activities active point 
enables organization self representations 
networks studied 
developed 
seen extension earlier ideas role 
lateral connectivity interacts hebbian learning 
producing representations influential 
hebbian learning modeling organization self early visual 
system developed showed hebbian 
representation input correlations produce unit activities 
early visual system extent 
model linear shown parameter sensitive 
model useful 
potential benefits hebbian learning 
cover issues detail 

slightly different approach organization self 
based primarily activities receiving units 
cover space input patterns 
example type learning algorithm 
ensures unit 
active essentially same percentage time effective 
features similarly distributed 
environment good match number 
units hidden layer number features 
environment plausible assumptions 
real world human brain respectively 

useful place upper lower limits 
average activities units layer order prevent 
active 
done simulating effects accommodation 
sensitization accommodation effect 
difficult activate neurons active 
time sensitization effect causes 
neurons easily excited active 
recently saw effects 
point neuron activation function 
see later improve learning cases 

finally important organizing self learning 
models called models based idea 
recognition 

idea here top image 
based internal model world 
learn based difference generated 
actually advantage learning mechanism 
requires internal model fit precisely possible 
actual input patterns principle lead 
representations correlations units 
problem produce worse fit 
details specific patterns further models 
easily understood terms bayesian statistical framework 
likelihood term plays important role 
framework essentially model 
extent hypothesis internal model 
produced data actual perceptual input 

problems models 
problem encourage network capture 
information present input image goes 
important bias requires away lot 
information order represent general underlying structure 
example cpca algorithm away 
principal component correlation matrix subset 
patterns represented unit question addition 
biological reality process clear 
algorithms typically bottom top 
weights set weights time 
clear weight switching occur biologically 
alternative implementation units activity reflects 
difference states predicts 
expected states result activity 
appear true brain expect 
reduced activity expected states problematic 
models require 
processing neurons areas higher 

short kinds processing required models run 
ideas developed previous chapter 
processing brain viewed involving 
bidirectional propagation information performing 
multiple constraint satisfaction information relevant 
levels processing resulting activation 
state constraint satisfaction model including 
top bottom processing critical explaining 
cognitive phenomena later chapters 
processing principles 
model based learning biological 
data consistent hebbian learning relatively simple 
easy implement form think brain 
pretty good learning algorithm turns 
models performance advantages further 
think hebbian learning naturally implements number 
important valid biases described 
learning 
framework anyway 

developing internal model environment 
useful survival environment certain 
learning solve actual tasks useful now 
focus task learning neural networks simple 
general means solve task produce 
specific output pattern input pattern input 
context contingencies demands task 
output appropriate response reading text 
correct answer addition problem 
straightforward examples output input mappings 
learned see subtle 
ways tasks learned 

cpca hebbian learning rule developed 
model learning good learning solve tasks 
need learning algorithm perform 
important kinds learning lets begin seeing well 
simple output input mappings 

exploration based simplest form task 
learning set input units project single output 
unit task terms relationships 
patterns activation input units corresponding 
target values output unit type 
network called pattern associator 
objective associate patterns activity input 
output 

lets begin opening project 
output units receiving inputs input units 
set feedforward weights locate 
control panel press button select 
output input relationships learned 
task simply input units 
left output unit active units 
right output unit active relatively easy task learn 
left output unit develop strong weights 
units ignore ones right right 
output unit opposite note kwta inhibition 
output layer parameter 

network trained task simply input 
output units corresponding values events 
environment performing cpca hebbian learning resulting 
activations see action locate 
control panel upper right side network window 
continue press 
presented random order end epoch 
events see activations updated 
result testing phase run epoch 
training testing phase events presented 
network time output units clamped 
updated according weights input units 
clamped testing phase 
actual performance network task 
test 

results testing displayed grid log lower 
right hand side screen row represents 
events input pattern actual output activations shown 
right column 
squared error sse simply difference 
actual output activation testing 
target value clamped training 
sum output units actually computing 
thresholded sse absolute differences 
zero means unit 
activation correct side order zero error 
think units essentially 
binary activation value expressing 
likelihood underlying binary hypothesis true 

single training epoch output unit likely 
errors now turn button upper left 
hand side network window press button 
control panel see grid log 
update epoch showing pattern outputs 
individual sse errors graph log 
immediately network provides summary plot epochs 
sum thresholded sse measure events 
epoch shows referred learning 
curve network rapidly 
zero indicating network learned task training 
automatically network correct epochs 
row sure learned problem 

order see learned press button 
control panel plot 
final output activation patterns testing window 
turn network back 
see actual unit responding see network 
learned easy task turning left output 
patterns right next now lets take look 
weights output unit see exactly happened 
click network window left 
output unit see expected weights 
left units strong right units 
weak complementary pattern hold right 
output unit 

explain pattern weights cpca hebbian 
learning algorithm 

now lets try difficult task press 
hard task overlap amongst input 
patterns cases left output 
right output unit 
somehow figure task relevant 
input units set weights 
problem hebbian learning apparent 
concerned correlation conditional probability 
output input units learn sensitive 
inputs task relevant unless happens 
same output input correlations case easy 
task hard task complicated pattern overlap amongst 
different input patterns cases left output 
inputs correlated extent 
middle strongly correlated cases 
left output right overlap considerably 
last event containing 
highly correlated inputs network 
attention correlations tend respond last case 
shouldnt 

lets see happens network task 
button 
produce new set random starting 
weights viewing 
weights left output unit network window 
network learns see weights learned 
middle units highly correlated output unit 
expected training 
network getting right different runs produce 
slightly different results case middle 
events turning right output unit 
last turning left output producing weaker 
activation output units relatively equally 
excited looking weights right output unit show 
strongly represented correlation input 
unit pattern output responses reason 
weight right output unit stronger 
left output unit middle inputs 
different overall activity levels different input patterns 
difference affects renormalization correction 
cpca hebbian learning rule described 
renormalization set constant 
different events help network solve task 
level detail 

task 
network ever solve task report final 
end training run 

depending answer last question now 
difficult impossible 
hebbian learning solve task now turned 
algorithm solve task 
impossible single layer units learn mapping 
big deal turns 
pattern weights lead correct solution 
see algorithm better suited task learning 
conclude hebbian learning limited 
task learning works correlational structure 
task structure 
number subsequent simulations well 

finally experiment parameters control 
contrast enhancement cpca hebbian learning rule 
see playing important role 
networks behavior 

find combination contrast enhancement parameters 
default 
default reliably 
networks performance multiple runs 

problem hebbian learning rule task learning 
care network actually performing 
task representing correlations conditional 
probabilities sending receiving units evident 
explorations network learned 
correlational structure ignored fact producing 
wrong outputs find specific ways 
hebbian learning work specific tasks 
form learning moment develop 
based task learning algorithm principles 

central principle base task learning 
directly goal producing correct output activations 
order need kind measure tell 
close network producing correct outputs 
way measure weights obvious 
measure squared error sse statistic described 
want extend sum events 
resulting 
again target value confused 
event index actual output activation 
implicitly function time event zero 
outputs exactly match events 
environment training set larger values reflect 
worse performance goal task learning 
error measure serves 
objective function driven error learning 
objective learning 

illustration computing derivative sse 
respect weights shows configuration 
single input output neuron target activation 
input activation assuming linear activation 
rule means weight needs 
task right shows sse function 
weight weight smaller change 
sse increasing weight derivative negative 
adjust weight negative increase 
weight towards true weights larger 

standard direct way minimize function take 
derivative respect free parameters 
weights network adjust parameters according 
negative derivative sense 
derivative change function want 
direction changing function 
negative shows illustration 
simple case single input output units 
linear activation function input activation fixed 
output activation equal weight value 
target say weight starts 
smaller value increase gets 
conversely weight larger 
shows 
derivative sse weight values negative 
negative results expected weight 
increase derivative positive weights larger 
taking negative results weight 

mathematically take derivative sse function 
produce expression tell exactly 
sse line function changes individual weight values 
network taking negative resulting expression 
weight update learning rule called delta rule 
input stimulus unit activation 
known least mean 
time essentially same equation 
rule 
conditioning 

look form learning rule sense 
adjust weights reduce error basically 
weights change function local 
error individual output unit 
activation sending unit sending units 
active big error receive 
error example output unit 
active shouldnt 
negative weights input units 
active hand output unit 
active weights increase 
input units active next time 
units activation closer target value 
error reduced 

illustration credit process 
activity unit represented 
output unit active 
positive 
weights increase proportion activity 
sending units active sending units 
good same principle holds output 
unit active 

process weights proportion sending unit 
activations called credit 
appropriate name 
illustrated 
important computational property driven error learning rules 
similar level pca hebbian learning rules 
view representations rule delta driven error 
learning reflecting results multiple credit 
satisfaction mechanism 
credit process output input pattern integrated 
weight changes 
entire training set reflecting strongest 
correlations case hebbian learning weights here 
reflect strongest solutions task hand 

now see delta rule works show 
derived directly derivative squared sum error 
measure respect weights 
input units essential reader understand 
details mathematical details 
important thing understand effects 
terms credit explained 

issue fact weights 
appear directly clear 
enter computation appear 
order proceed need actually specify exactly 
computed function weights 
expression derivative depend function 
sense know exactly changing weight 
affect activations figure change 
weights produce target activation values 

things simple start linear activation 
function 
now chain rule take derivative 
respect appear error equation 
take derivative respect weights based 
linear activation function chain rule expression written 
follows 
thing term thing 
next term 

chain rule series terms case 
figure 
terms separately end taking 
term 
notice outputs events 
considering change weights particular output unit 
particular event learning rule local 
sense depends single output unit single 
pattern 

take next term linear activation function 

notice elements sum 
involve particular weight appropriate full 
derivative case linear activations follows 
derivative want negative 
learning rule ignore introduce factor 
error measure arbitrary 
learning rate constant anyway delta 
rule shown previously 

issue bias 
weights learn recall bias weights provide constant 
additional input neuron see 
proper bias weight values essential producing 
useful representations example allowing units represent 
weaker inputs turns 
isnt straightforward way train bias weights 
hebbian learning model 
correlational information correlation unit 
itself perfect perform driven error task 
learning delta rule algorithm put bias weights 
good 

standard way thinking train bias weights 
consider weights coming unit active 

bias weight change 

sense bias weight adjust try 
decrease error example extent unit 
active shouldnt bias weight change 
negative positive cause bias weight 
decrease causing unit active bias weight 
learns correct relatively constant errors caused 
unit generally active inactive 

problems 
rule delta based task learning mechanism larger 
deal later problem computing 
derivative point neuron activation function respect 
weights simple linear activation function 
problem delta rule 
natural range weight values take 
positive negative values magnitude 
biological psychological reality target output 
values turns reasonable solutions 
problems 

solution problem involves steps 
assume approximate point neuron function 
sigmoidal logistic function showed back 
reasonable thing see 
later able avoid approximation 
necessary time see 
different error function appropriate 
valued binary underlying representations case 
analysis point neuron activation function terms 
representing unit detected 
derivative logistic activation function ends 
mathematically 
need know problem 

comparison entropy cross squared sum 
error sse single output target value 
larger output 

new error function called entropy cross 
distance measure probability 
defined 
actual output activation target activation 
probability variables range 
target itself entropy 
variable defined log entropy cross 
entropy variables think variance 
variance squared error function zero 
actual activation equal target increasingly larger 
different squared error 
function treat entire range 
value 
large whereas values 
produce error way function 
takes account underlying binary true aspect 
variables see comparison sse 

define functions unit sigmoidal 
logistic activation function here useful break 
activation function net input term 
weighted activations sending units 
logistic activation function operates net 
input convenient write function 
intended 
sigmoidal character function 

now take derivative error function respect 
weight again chain rule time extend 
chain rule include separate step derivative 
activation function respect net input 
net input respect 
weight 
again well break component terms 

note here notation indicate 
derivative sigmoidal logistic function 
cases 
equation single variable simple 
derivative partial derivative respect 
multiple variables shouldnt 
trying figure derivative 
involved finally derivative net input term 
linear activation 

put terms see 
term derivative logistic 
function resulting exactly same delta rule 
implement version delta rule learning 

problem ignore fact 
weights naturally learning algorithm allow 
adapt additional 
problematic least reasons biologically 
implausible excitatory neurons weight take 
positive negative values function learning know 
cortical neurons excitatory inhibitory 
switch 
time inhibitory 
neurons directly implemented leabra 
effects simulated kwta activation function 
meaningful combine subsequent 
driven error algorithms cpca hebbian learning rule 
later unless weights produced driven error learning 
same natural imposed cpca recall 
cpca computes conditional probabilities weight values 
naturally range reasons 
following mechanism bounding driven error weights 
weight change computed driven error 
algorithm 
otherwise 
opposite otherwise 
same kind soft weight bounding cpca algorithm 
naturally weights approach 
slowly note equation same 
general form form cpca hebbian weight update 
rule equation 
updating membrane potential 
functions effect producing 
weight value series individual weight 
changes equal magnitude opposite sign corresponds well 
hebbian interpretation reflecting lack positive 
negative correlation similarly positive weight increases 
negative ones weight value increases 
decreases negative changes 
positive 

finally regarding problem simplest 
interpretation reality target value think 
activation state corresponds 
experience explicit signal 
external source actual observed outcome event 
world produced activation state thought 
response internal expectation 
outcome refer activation 
states different phases call 
expectation phase minus phase outcome 
target phase plus phase reasons clear 
later discuss issue depth now 
assume target values actually experienced 
network activation states rule delta learning 
involves taking difference target actual 
produced activations further need assume somehow 
forms information present form 
learning again discuss greater length 
now actually 
exactly equation equation 
described reduces equation 
conditions revisit 
pattern associator task 

open default initial parameters 
pick switch rule delta updating 
weights start training network need 
understand different activation phases work 
simulator sure 
network window locate control 
panel press now pressing 
button usual need increase resolution 
press button perform phase 
processing processing entire event 
locate parameter process control panel note 
button case lower 
value currently set change 
means press 
button result settling activation updating 
process associated phase processing 

now hit button see 
network actual activation produced response input 
pattern aka expectation response minus 
phase activation now hit again see 
target aka outcome plus 
phase activation tell target exactly 
note activations 
units easily produce activations typical net 
input values learning occurs phase activation 

now lets monitor weights clicking 
left output unit process control panel 
complete training task network 
learning task perform multiple 
variable relative hebbian case switch 
learning algorithms reflects critical 
driven error learning basically 
output unit performing task correctly learning effectively 
stops whatever weight values happened 
contrast hebbian learning weights reflect 
conditional probabilities task least results 
roughly same final weight values regardless initial 
random weights return issue later 
discuss benefits combination hebbian 
driven error learning 

now real test pick 
press see network learns 
task apparently difficulty delta rule 
performs learning function well network actually 
adapt weights specifically order solve 
task 

compare contrast weight values learned delta rule 
task learned hebbian rule 
explain delta rule weights solve problem 
hebbian ones sure include bias weights 
perform multiple delta rule case pressing 
weights describe aspects solution remain constant 
runs changes finally explain 
relatively general terms delta rule able learn task 
hebbian rule 

experience think delta rule 
powerful choose 
notice input unit active equally output 
active inactive difficult 
input unit end output 
input patterns 
same seems humans learn solve 
task fairly attention overall 
patterns activation lets see network press 
delta rule learn appears 
relatively simple task conclude powerful 
necessary 

clearly delta rule better hebbian learning 
task based learning limits interestingly 
limitation played large role early 
neural network models book 
contained mathematical 
limitations delta rule learning algorithms 
problems impossible case tried 
researchers neural networks 
simply powerful model human cognition field 
towards metaphor computer based models 
sufficiently apparent people limitation 
applies networks layers input output layer 
pattern associator models turns delta 
rule relatively directly extended generalized deal 
networks hidden layers input 
output layers results algorithm commonly called 
backpropagation learn impossible task 
previous example algorithm 
learn function uniquely input patterns output 
patterns hidden units 

critical advantage hidden units enable problems 
represented ways easier solve 
way problem represented plays critical role ability 
solve familiar insight problems essentially 
amount new useful representation 
answer apparent example try figure 
following sense 
words here 
chosen context sense 
able represent words context 
see perfect sense 
example comes simple turned 
representing part 
tree cognition based development 
appropriate representations input patterns think 
important issues study cognition 
importance developing learning mechanisms backpropagation 
produce representations series hidden layers 
return issue repeatedly 
text already discussed 
context transformations discussed previous 
chapter 

illustration basic processes standard 
backpropagation algorithm shows feedforward 
propagation activations layer same 
sigmoidal logistic activation function 
shows error backpropagation step challenge 
figure update weights input units 
hidden units already know adjust hidden 
output weights delta rule 

took time 
idea 
times 
backpropagation algorithm 
simple extension delta rule continues 
chain rule hidden units 
weights lets take standard case network 
input hidden output feedforward connectivity sigmoidal 
logistic activation functions see chain rule 
propagate error signals back output layer 
hidden layer finally weights input units 
hidden units see 
illustration 

assume moment units standard 
logistic activation function well write activation output 
unit hidden unit input 
stimulus unit net input unit 
unit 
function written 
equal 

note weights hidden units output 
units trained simple delta rule derived 
real problem training weights input 
hidden chain error function 
continue entropy cross error function weights 
input hidden layer follows 

lets break smaller 
lets look derivative terms 
take expression 
contribution hidden unit activation overall error 
term interesting tells change 
activation hidden unit affect resulting error 
value output layer imagine 
weights hidden unit output 
units way hidden unit influences 
outputs already expression 
similar chain rule delta rule 
steps difference 
delta rule took derivative net input output 
unit respect weights equal sending 
unit activation whereas here want take derivative 
same net input respect sending unit activation 
equal weight 
relevant term net input taking 
derivative terms 
versa vice 

expect overall derivative 
delta rule weight end activation 

computation interesting suggests hidden 
units compute contribution overall output error 
errors output units project 
errors strength units 
contribution output units return equation 
bit now continue chain 
backpropagation compute remaining 
actually similar already computed consisting 
derivative sigmoidal activation function itself 
derivative net input hidden unit respect 
weights know sending activation 
overall result entire chain 
negative 
adjust weights way minimize overall error 
layer network backpropagation delta rule 
output units weights 
hidden units weights 

illustration computation backpropagation 
learning standard feedforward layer network 
generic delta notation error values 

lets take moment understand properties 
backpropagation 
computation right way clear 
introducing new variable unit variable 
represents contribution unit towards overall error 
output layer derivative 
error respect activation unit 
hidden units computation 
derivative 
error respect net input unit output 
units 
recall due entropy cross error term derivative 
activation function respect net input 
here hidden units 
part comes equation terms 
new variables output layer 

turns same equation applied 
regardless number hidden layers network 
hidden layer takes weighted sum delta terms hidden 
output layer derivative 
activation function 
derivative activation function understood terms 
difference amount weight change actually going 
activation value unit unit 
sensitive middle range function weight change 
big difference activation derivative 
maximum conversely unit 
weight 
change relatively difference consistent 
derivative small 
learning rule effectively focus learning units 
think versus 
want focus 
chance 

generic delta backpropagation clear 
strong similarities feedforward propagation activation 
layer computing same basic function 
activations preceding layer backpropagation error 
layer computing same basic function 
errors subsequent higher layers clearly 
shows backpropagation error error essentially 
term algorithm 
provides illustration 
generic backpropagation 

terms write completely 
generic expression change weights terms 
sending unit error variable 
receiving unit interestingly looks 
simple hebbian learning rule product 
presynaptic sending postsynaptic receiving variables 
presynaptic term activation value fact 
postsynaptic term terms means 
isnt hebbian well see next section 
write equations completely terms 
activations come learning rule 
hebbian biologically plausible hebbian learning 
rule major problem backpropagation 
procedure biological reality error 
variable far certain necessary mechanisms 
network required equations 

finally note weight update rule bias weights 
according 
set sending activation value constant 

backpropagation procedure implemented 
biologically plausible way based important ideas 
algorithm developed 
ideas generalized 
restricted case 
algorithm resulting 
generalized algorithm generec 
providing complicated development 
ideas present key derived 
algorithm context generec algorithm 
end based task learning algorithm rest 
text 

illustration generec algorithm 
bidirectional symmetric connectivity shown 
minus phase external input provided input units 
network settles record resulting minus phase 
activation states plus phase external input 
target applied output units addition 
input units network again settles 

generec notion activation phases 
mentioned previously implementation delta rule 
network itself responsible setting activations 
output units expectation response minus 
phase environment responsible providing 
target outcome output activations plus phase 
generec operates allowing network symmetric 
bidirectional connectivity settle updating 
activations network minus phase settle 
again plus phase see 
illustration indicate 
phase plus variables indicate phase minus variables 
equations 

key idea way generec compute error 
terms activation values error signals 
seen examining way net input computed 
bidirectionally connected network standard layer 
network analyzed net input hidden units 
sum weighted activations input output 
units minus phase 
net input plus phase 
observe due bidirectional connectivity activation 
hidden unit reflects influence output activation 
states lets assume hidden unit somehow measure 
difference input signals received output units 
different phases difference related 
error output units seems 
bidirectional activation propagation hidden unit 
able measure output error 

see works mathematically lets 
net input terms different phases 
perform simple 
recognize last form equation 
similar expression derivative error respect 
hidden unit activations 
shows key insight 
algorithm difference 
net input terms compute error 
contribution hidden unit putting way 
start looks 
difference net input terms 

compare equations important 
subtle difference apparent net inputs 
equation terms feedback weights output units 
hidden units whereas equation terms 
feedforward weights hidden units output units 
order hidden unit accurately compute 
sending contribution output error 
weights hidden units output units based 
receiving input output units 
weights output units back hidden units 
assume weights same value 
recall needed symmetric weights 
order understand constraint satisfaction properties 
networks well discuss biological 
assumption now need least insight 
working algorithm 

difference activation states 
difference net input states times derivative 
activation function 

key insight algorithm 
compute error terms activation values hidden units 
net inputs greatly simplifies things see 
works lets equation variable 
hidden unit 
key insight algorithm 
term difference 
net inputs hidden units output units 
phases recall expressed 
derivative activation function 
turns resulting product 
difference sigmoidal activation values computed 
net inputs 
difference hidden units activation values 
equivalent difference net inputs times 
activation function net input values 
long linear approximation 
activation function reasonably 
actual net input units 
inputs coming output layer 
inputs relatively constant phases 
helps ensure phase differences relatively small 
linear approximation reasonable 
illustrated 
clear differences axis 
equal differences axis times function 

simplification differences activation states 
major advantage addition simpler 
eliminate need explicitly compute derivative 
activation function derivative implicitly 
computed difference activation states big 
advantage allows based biologically point 
neuron activation function kwta inhibition 
entire layer units layer 
derivative difficult compute 
computing resulting complicated 
derivative need imagine neuron itself 
compute derivative 

weight updates computed generec algorithm 

terms hidden units computed 
difference activation phases nice result 
learning rule same units network 
essentially delta rule see 
generic learning rule 

generic receiving unit activations sending unit 
activation backpropagation 
learning rule applies units regardless hidden 
layers network activation differences 
continue reflect appropriate backpropagation error 
exercise interested reader show 
means locally available activation states pre 
postsynaptic units perform driven error learning 
need biologically error backpropagation 
mechanism different normal propagation activation 
network 

rule bias weights 

summary difference phases activation states 
indication units contribution overall error 
signal interesting thing bidirectional connectivity 
ends naturally signal 
network needs done difference 
drive learning activation states local space 
sequential time synapse weight changes 
occur see weight changes happen 
biological synapse important consequence algorithm 
error signal occurring network 
drive learning enables different sources 
error signals addition form learning 
bidirectional connectivity known exist 
cortex actually requires connectivity order 
drive learning effectively 

finally noted generec approximation 
actual backpropagation procedure bidirectional network 
potentially complex settling dynamics propagation 
phases activation values separately 
difference generec same 
difference itself backpropagation approximation 
holds well deep networks performing 
complicated learning tasks 

done problem basic generec learning 
rule shown symmetric 
weight changes computed unit unit 
same computed unit unit units 
rule end symmetry 
weights learning rule work properly 
place way 
updating weights known method average 
minus plus phase activation sending unit 

method method symmetry 
sum 
ideas change weights combined result 
following 
interestingly additional generec 
algorithm equivalent called 
hebbian learning algorithm chl aka mean field 
learning algorithm 
locally available activation variables perform driven error 
learning connected networks algorithm 
derived originally networks activation states 
described distribution 
context chl amounts 
reducing distance probability arise 
phases settling network algorithm 
extended case 
restricted cases 
derived 
distribution continuous energy 
function require 
problematic assumptions conclude 
chl networks 
chl 
directly backpropagation algorithm via generec 
basis ability learn difficult problems 
further generic form chl aka generec 
remaining problems largely 
learning rule context kwta inhibition function 
conjunction hebbian learning rule showed 
problems chl effects 
purely driven error learning bidirectionally 
connected network well explore issues 

short providing further biological psychological 
potential reality generec algorithm 
human cortex least done mathematical side 
things summarize adjust 
weights network subject soft weight 
bounding procedure described previously 

generec approximate error backpropagation locally available 
activation variables fact variables available 
locally plausible learning rule 
real neurons based activation signals 
opposed error variables increases 
relatively straightforward map unit activation onto 
neural variables averaged time membrane potential spiking 
rate see main features 
generec algorithm potentially problematic 
biological perspective weight symmetry plus 
minus phase activation states ability activation 
states influence synaptic modification according learning 
rule 

recall generec requires symmetric weights order units 
compute sending error contribution based receive 
back units ways biological 
weight symmetry generec 
addressed show exact symmetry critical 
proper functioning algorithm rough form 
symmetry required biology show 
least rough form symmetry actually present 
cortex data consistent arguments summarized 
here 

order point noted symmetry 
learning algorithm chl combined weight 
automatically lead symmetric weights 
start way assumes units 
connected place difficult 
case connection 
chl learning algorithm 
algorithm effective connectivity 
input non units 
possible connections 
ways error signal 
information obtained hidden unit obtain 
error signal directly output units 
connections hidden units note absence 
connection different presence 
connection symmetric non weight value form 
found problematic analysis 
case subset error information 
available latter case result specifically 
wrong information due influence 
symmetric non weight due automatic property 
chl latter case problem 

terms biological evidence symmetric connectivity 
indication cortex least roughly 
connected level anatomical 
cortex areas visual cortex 
connected area projects 
area area receives projection area 
level cortical columns 
prefrontal cortex connectivity 
symmetric interconnected 
neuron received 
projections neurons 
neurons detailed level individual neuron 
symmetric connectivity difficult assess empirically 
least evidence exist further 
evidence least rough symmetry detailed symmetry 
critical demonstrated 
chl connectivity 
long way information 
subset symmetric connections via neurons 
same area 

based phase activations central 
generec algorithm 
aspect driven error learning cortex 
signal come emphasized 
generec signal phase plus activation state 
suggests signal state 
experience network state 
actual outcome previous conditions 
minus phase thought expectation 
outcome conditions example 
words sentence expectation develop 
word likely come next state neurons upon generating 
expectation minus phase experience 
reading actual word comes next subsequent 
demonstrate 
generated state 
activation serves plus phase idea 
brain constantly generating subsequent events 
subsequent 
driven error learning suggested 
psychological interpretation 
backpropagation learning procedure particularly 
generec version backpropagation activation 
states requires additional mechanisms providing 
specific signals effects experience 
neural activation states via standard activation propagation 
mechanisms 

different forms error signals 
simple layer network right layer figure 
represents state adjacent layer left 
subsequent time step additional layer explicit 
signal output 
implicit based expectation error signal 
input triggers subsequent expectation 
experienced implicit expectation based motor output 
consequences motor action expected 
experienced implicit single input 
layer note here 
layers different input representing different 
states time 

provides 
kinds based expectation signals arise 
conditions shows case 
people typically assume think driven error learning 
signal explicitly provided 
word explicitly 
shows similar kinds error 
signals arise result implicit expectation 
word pronounced followed actual 
experience word pronounced ones 
reading 
show different implicit 
generated including outcome motor output 
actual outcome actual 
input received 

addition ways error signals 
based outcome expectation differences evidence 
electrical activity behavioral 
tasks cortical activation states reflect 
sensitive example studied widely 
going positive occurs 
stimulus onset considered measure 
subjective determined preceding experience 
short long term terms 
showed 
determined amount prior resolved 
processing event nature 
consistent idea represents plus phase 
activation following relatively short time activation 
minus phase specific properties 
itself due specialized neural mechanisms 

presence suggests possibility neurons 
neocortex experience states activation relatively rapid 
corresponding expectation 
corresponding outcome 

weight change according chl 
cpca hebbian learning rules conditions 
minus plus phase activation values cpca 
care phases assumed take place plus 
phase plausible biological mechanism produces combination 
rules shown column 

suggested minus plus phase activations follow 
rapid remains shown 
activation states influence synaptic modification manner 
largely consistent chl version generec equation 
discussed capable 
implementing critical aspect chl recall 
already showed 
biological mechanisms consistent cpca hebbian learning 
rule extent chl inconsistent cpca 
possibly argue biology 
argue biology consistent 
combination cpca chl 

shows chl cpca learning 
rules differ direction weight change 
different values minus plus phase 
sending unit receiving unit note cpca 
based phase variables further specify occurs 
plus phase see learning rules 
row table row rules differ 
predict 
different weight change rule predicts weight 
change predicts weight change 
combination rules ends 
further see 
combination driven error 
hebbian associative learning generally beneficial solving 
different kinds tasks 

already shown cells 
accounted biology 
cpca hebbian learning 
remains shown lower hand left cell table 
chl learning rule predicts weight decrease ltd 
occur according biology refer cell 
error correction case occurs synaptic 
larger minus phase plus phase 
strong expectation associated units 
minus phase actually experienced outcome 
plus phase important contribution driven error 
learning enables network correct 
expectation output otherwise hebbian learning capable 
right things shown table 

explain error correction case relationship 
calcium ion concentration direction 
synaptic modification proposed 
discussed shown 
idea 
synaptic modification 
level calcium higher 
high threshold leads ltp level 
lower high threshold lower 
threshold leads ltd basic idea here minus phase 
synaptic activity followed similar 
greater levels plus phase synaptic activity 
lead level 
threshold threshold resulting ltd 

mechanism plausible consistent known data 
directly tested further requires kind 
additional mechanism indicate activations considered 
plus phase seems likely phase plus 
signal produced same kinds dopamine 
mechanisms described 
modeled 
discuss 
brain dopamine 
systems apparently fire whenever 
expectation outcome specifically case reward 
studied general 
known dopamine efficacy ltp 
appropriate phase plus learn now signal summary 
available data consistent directly supporting 
biological mechanism enable driven error task learning 

now lets put theory work see generec 
scale small task learning problems same 
problems pattern associator case 
time extra hidden layer units inputs 
outputs theory enable network solve 
impossible task 

start opening project 
associator major exception introduced 
hidden layer units increased learning rate 
takes time solve impossible 
problem lets start network problem 
select choose default 
learning rule set now press 
network displays sse error measure epochs 
training testing grid log lower right now updated 
epochs training shows states 
hidden units 

training network stops automatically 
gets entire training set correct epochs row note 
correct 
solutions happen due noisy behavior 
network learning shape 
learning curve reason noisy behavior 
relatively small change weight lead large overall 
changes networks behavior due bidirectional activation 
dynamics produces range different responses input 
patterns sensitivity network property 
networks networks bidirectional 
connectivity typical feedforward networks 
feedforward backpropagation network learning same task 
learning curve people 
nature learning networks 
share backpropagation find 
benefits bidirectional connectivity dynamics far 
learning curve further turns 
larger networks exhibit learning 
sensitive small weight changes 

epochs take network learn 
press times order sense fast 
learns general provide general 
results rough average 

learning done explain hidden units represent enable 
network solve impossible task weight 
values network display explain 
terms hidden unit activated sure 
multiple runs extract general nature networks 
solution data try explain general terms 
hidden units let networks solve difficult problems 
otherwise solved directly 

adding hidden units enable 
network solve problem button 
select couple times 
observe complete hebbian learning work 
well hidden units simple tasks see 
network learn couple epochs added 
end sure now select 

fast generec learn task compared 
hebbian rule sure run times good 
sample try explain occurs 

last exercise important 
hebbian learning faster reliable 
driven error learning reasons explored 
greater depth clearly interacts nature 
task tasks network 
typically going easy sense 
hebbian driven error learning see fact 
combination types learning works best 

developed explored different kinds learning model 
task obvious question arises relationship 
forms learning think part 
cortex performing model learning task learning 
extent issue addressed 
field assume case people 
assume sensory processing proceed largely entirely 
basis model learning higher output oriented 
areas task learning general 
division important consider alternative view 
suggests task model learning performed 
integrated fashion areas cortex good reason 
considering idea biological mechanism synaptic 
modification discussed 
suggests case 

number functional computational level 
reasons think model task learning work well 
generally levels processing benefit 
representing relevant structure world model learning 
developing representations useful solving tasks 
task learning example seem reasonable 
relatively early areas visual system develop representations 
relevant task distinctions 
important survival accurately represent 
subtle perceptual features correspond different 
states imagine based task 
learning provide necessary emphasis distinctions 
collapsing distinctions task 
relevant different trees 
seems likely left own model learning 
incapable determining subset huge number 
reliable distinctions actually represented 
case statistical structure 
clusters correlations tree features stronger 
likely represented model learning 
features case pretty shape 

form argument well model 
learning generally facilitate task learning cases 
solving tasks requires developing representations underlying 
elements task likely things 
space time appear correlational 
structure model learning representing further 
likely correlations particular input patterns 
particular output patterns meaningful task learning 
model learning enhance representation 
correlations model learning enable large 
deep hidden layers networks learn rapidly 
reliably critical typically hidden 
layers cortex perception motor output 
saw previous chapter previous exercise demonstrated 
easy task hebbian learning generally 
faster reliable learning important 
speed local nature hebbian learning 
rely driven error learning possibly 
weak error signals order weights learn appropriately 
note generec driven error learning algorithm 
locally error signals drive learning 
communicated backwards network output layer 
reliable truly local correlations 
hebbian learning represents 

saw exercises hebbian model learning 
produces reliable weight patterns random initial 
weight values represents correlations 
conditional probabilities available activity patterns 
important 
degrees network otherwise 
case learning actually constrained 

hebbian model learning saw generally capable 
learning tasks view combination 
types learning learning 
process primarily task learning task 
learning constrained model learning levels 
processing respect view model learning 
particularly form term 
commonly learning describe 
additional biases introduced further constrain 
otherwise types learning common example 
weight small portion 
weight value weights updated 
network weights reliably 
contributing solution otherwise weights 
zero 

finally note hebbian learning 
addition driven error learning inhibitory 
competition kwta functions represents 
important additional constraint learning process 
substantially performance network cases 
inhibitory competition appears significant overall 
contribution additional hebbian learning 
inhibitory competition form long 
model learning context task 
learning context error difficult 
compute unless generec algorithm 
computes implicitly form 
competition kwta allows distributed representations 
develop essential interesting tasks 
generally 

architecture 
gating units soft wta competition aka 
output contribution 
corresponding problem 

context inhibitory competition 
task learning situation 
framework illustrated 
here soft wta 
take competition takes place special group 
units called gating units provide 
modulation outputs corresponding groups 
units specialize solving 
different parts overall task group 
distributed representations same limitations wta 
algorithms operate individual units apply here well 
example extreme nature wta competition 
network active time limits extent 
different solve problems 
algorithm individual units 
powerful same limited overall dynamics wta 
systems contrast think effect kwta 
algorithm learning task context better kind fine 
specialization 
different units different aspects task allows 
powerful distributed representations 

implementation combined model task learning 
straightforward amounts simply adding weight 
changes computed hebbian model learning computed 
driven error task learning additional parameter 
simulator controls relative 
proportion types learning according 
following function 
learning components 
cpca hebbian rule 
chl generec driven error rule 
respectively parameter typically smaller 
values larger 
relatively small amount hebbian learning needed 
primarily error signals typically smaller 
magnitude bit training hebbian 
learning constantly same kind pressure 
learning error signals change depending 
network getting right wrong consistency 
hebbian learning larger effective impact well 

important note parameter useful 
means relative importance hebbian versus 
driven error learning different simulations possible 
single value parameter modeling 
different areas cortex hand different areas 
differ parameter way 
genetic biological biases influence development 
specialization different areas see 

note hebbian rule computed phase plus activation 
states sense computationally want move 
towards activation states sense learning 
statistical structure biologically assuming 
learning actually plus phase see 

benefit combining task model learning bidirectionally 
connected networks comes generalization domain 
central cognition specifically 
generalization viewed system 
representations capture underlying structure 
regularities environment simple example 
know count generalize sequence 
produce further instances sequence long 
structure 
systematic mapping onto digits enables produce 
understand domain numbers clearly way thinking 
generalization central understanding humans exhibit 
rule systematic behavior 

purely driven error task based learning typically highly 
constrained actual tasks saw case 
delta rule pattern associator task large range weight 
values network solve task 
true cortex neurons 
number possible combinations weight values truly 
likely task based learning 
capture essential structure environment 
weights reflect large contribution 
initially random values seen hebbian model 
based learning better job extracting structure 

constrained task feedforward driven error 
networks typically generalize well due 
ability networks treat novel input 
graded based similarity novel input 
known input patterns hidden unit provides robust 
graded estimate similar input pattern trained 
examples representing same level 
exists output hidden mapping well further 
important hidden units participate 
representation input residual 
hidden units representation averaged 
performed output units 
hidden units 

illustration bidirectionally connected 
interactive networks small initial differences 
settling resulting large final differences known 
effect shown initial points 
dimensional state 
space feedforward network processing 
takes place allow differences 
significantly contrast interactive network requires 
updating allowing differences 
greatly 

problem constrained nature weights 
purely driven error network show 
introduce bidirectional connectivity know 
computationally important 
feature cortex essential generec 
algorithm bidirectional connectivity complex 
settling dynamics lead small 
differences input patterns opposite 
graded feedforward network treating 
novel pattern familiar patterns closely 
interactive network likely treat 
resulting generalization performance 
illustrates phenomenon 
understood terms effect effect 
name idea interactive 
dynamics 
lead later small 
initial differences considerably complex dynamic 
systems 

extent bidirectionally connected network operates 
activation space processing similar 
inputs network settles 
time interactive networks exhibit 
dynamics similar inputs result roughly same final 
activity state thought 
dynamics theory lead 
better generalization effect sensitivity 
behavior same behavior 
expressed network depends nature weights 
define relative activation 
space settling 

bidirectional network sensitive noisy 
weights lead activation space 
feedforward constrained nature weights 
driven error network result worse generalization case 
suggests way solve problem eliminate 
weights exactly hebbian model learning 
produces reliable final weight values 
seen addition inhibitory competition present 
cortex model causes individual hidden units take greater 
representing specific input patterns 
causes weights closely correspond important 
distinctions different inputs further inhibition 
effect activation dynamics network see 
reduces 
effects 

explore simple example effects combined task 
model learning generalization simple oriented lines 
environment explored model learning section 
begin open project 
notice network now output layer 
output units corresponds different vertical horizontal line 
task learned network simple activate 
appropriate output units combination lines present 
input layer task simple provide 
good demonstration pure hebbian learning 
actually capable learning task time 
later 
next section task provides particularly 
clear relevant demonstration benefits adding hebbian 
learning otherwise purely driven error learning 

control panel contains parameter 
learning compared driven error learning see 
main parameter 
order compare purely hebbian model learning 
purely driven error task learning 
combination 
turn learning 
bias weights pure hebbian learning well 
select purely 
driven error task learning 

see network trained lets step 
events sure network 
window locate control panel right 
next network window 
event press again see plus phase 
output units reflect lines input 
position left bottom press 
button process control panel turn 
network 

network graph log updated epoch 
training error statistic epochs important test 
statistics sse training error statistic 
count number events 
error again threshold unit 
unit activation right side event 
counted measure plotted red line 
graph log test statistics same unique pattern 
statistic see 
measures extent hidden units represent 
lines plotted yellow 

statistic plotted green measures 
generalization performance network previous 
cases network trained total 
patterns remaining testing generalization 
lines presented subset lines 
training set possible recognize 
produce correct corresponding output unit novel context 
lines testing set network recognize 
underlying regularity environment line line 
line regardless context appears 
generalize well produce relatively number errors 
testing items green line plots number testing 
events network gets wrong smaller value 
better generalization performance 

addition graph log weight grid log updated 
epochs showing weights change 
learning 

well purely driven error network generalization 
task final generalization error 
epochs training explain performance terms 
unique pattern statistic weight patterns hidden 
units reference constrained nature purely 
driven error learning 

order determine particular result 
run batch training runs press button 
overall control panel see new text log show 
place graph log present summary statistics 
training runs addition bring text log 
weight grid log shows final results 
training run 

report summary statistics batch run 
indicate earlier generally applicable 

now lets see improve performance adding 
hebbian learning overall control panel 
select see 
value control panel now 
now control panel bring back 
graph log training run 
run 

additional hebbian learning change results 
compared purely driven error learning report results 
batch run explain results terms weight 
patterns unique pattern statistic general effects 
hebbian learning representing correlational structure 
input 

finally lets see well pure hebbian learning task 
select changes 
notice network learns rapidly 
depending goes network perfect 
performance task itself generalization test 
runs network fails learn perfectly 
due hebbian learning correct errors 
gets better find case rapidly press 
number epochs 
press again task obvious 
correlational structure well suited hebbian learning 
algorithm clear hebbian learning helps 
here network reliable driven error learning 
see next task hebbian learning helps 
correlational structure particularly obvious 
pure hebbian learning completely incapable learning 
apply range different generalization tests 

critical benefits combined model task learning 
training deep networks hidden layers 
explained previously additional hidden layers enable 
representation problems ways easier solve 
clearly true cortex visual system 
example original retinal represented huge number 
different ways build upon 
hidden layers 

revisit issue 
develop model visual object recognition multiple hidden 
layers 

examined issue example 
problem benefits multiple hidden layers known 
family trees problem task 
network learns family relationships 
capable relationship instances 
trained representing intermediate 
hidden layer individuals family represent 
specifically functional similarities individuals 
enter similar relationships represented similarly 
happens deep network example difficult 
purely driven error network learn learning time actually 
decreases network single hidden layer 
original deep network generalization 
completely finally depth 
scaling driven error learning worse bidirectionally 
connected networks training times long 
feedforward case times longer 
combination hebbian driven error learning 

illustration analogy driven error 
learning deep networks 
difficult hebbian 
organizing self model learning adding 
layer 
learn useful representations constraints inhibitory 
competition viewed restricting flexibility motion 

useful analogy understanding driven error learning 
work well deep networks bidirectional 
ones easier balance 
placed 
moving base 
back forth direct effect single 
effects indirect higher 
bottom 
increasingly effects higher nature 
effects depends position lower 
similarly error signals deep network increasingly 
indirect effects layers further away 
training signal output layer network nature 
effects depends representations developed 
layers output signal increased 
non associated bidirectional networks problem 
gets worse 

way problem easier 
internal greater 
self least partially balance themselves 
idea model learning provide exactly kind self 
learning local produces potentially 
useful representations absence error signals 
slightly abstract level combined task model learning 
system generally constrained purely driven error 
learning algorithm degrees adapt 
learning thought range 
motion easier balance 
explore ideas following simulation family 
trees problem subsequent chapters 

family tree structure learned family 
trees task english 
indicates 

now lets explore case learning deep network 
same family trees task 
structure environment shown 
task network trained 
produce correct name response 
presented activating 
name units input layer 
conjunction units input layer 
training network produce correct 
unit activation output layer 

open project 
notice network input 
layers output layer bottom 
network layers localist representations 
different people different relationships means 
similarity input patterns 
people 
localist representations richer distributed 
representations able support based similarity 
generalization performance see finally 
central layer responsible performing mapping 
representations rise correct 

locate control panel press 
training events help understand task 
presented network ahead scroll events 
list see different events presented 
network click look particularly interesting 
see represented now lets see works 
network itself locate control panel 
right next network press button 
followed button activations network 
display reflect minus phase state event press 
network train events rapidly network 
graph lower right displays error 
count statistic training red average number cycles 
took network settle orange 

learning curves family trees task showing 
combination hebbian model driven error task learning 
leabra results faster learning deep networks compared 
pure driven error learning backpropagation network 

network train epochs initial 
default parameters take press 
network learning curve load network 
network window select menu upper left 
epochs learn indicated file name 
graph log select menu choose 
epochs total train network completion 
network window 
name want come back later 

epochs took network learn problem indicates 
relatively rapid learning deep network 
example shows comparison 
typical learning curve leabra algorithm weve versus 
standard backpropagation network 
took epochs learn required large 
learning rate compared standard leabra 
network interested here raw learning speed 
own fact additional biases 
constraints imposed combining model task learning obviously 
learning networks hidden layers deep 
networks 

order see contribution hebbian learning run 
network overall control panel 
select 
network again want train 
load trained pre network log time select 
train network way 
different name hebbian learning clearly learning 
deep networks takes longer epochs case 
compared learn repeated runs networks 
different parameters effect further 
compare pure driven error network backpropagation 
network shown 
connected bidirectionally driven error chl network 
kwta activation constraints takes 
epochs learn clear kwta activation 
constraints playing important role well 

now lets see pure hebbian learning task 
select run network 
epochs isnt going 
improve load network trained epochs 
selecting 
log file 
hebbian model learning useful driven error learning 
simply capable learning tasks own 

order compare cases load 
identify lines based 
epoch end 
interesting note orange 
line average settling cycles fairly well correlated 
training error combined error network achieves 
significant cycles note pure 
case starts settling epochs 
learning 
turns contribute generalization performance 
networks see later simulations 
training patterns containing elements item 
question required achieve good generalization whereas 
individual appear times training 
set here 

notice general shape learning curve 
sse epochs compared 
leabra network ran pay 
special attention epochs learning 
primary differences cases 
network inhibitory competition via kwta 
function possible importance 
learning based results note network 
larger learning rate now compare 
sse learning curves red lines start 
different case 
suggest role hebbian learning hint error signals 
smaller network learned 

cluster plot hidden unit representations 
prior learning learning 
combined hebbian driven error learning trained 
network corresponding different 
clusters organized generally according 
generation 

finally examine representations 
network performing cluster analysis hidden units lets 
comparison initial clusters 
network learned task press 
process control panel weights press 
tests patterns cluster window appear 
load trained network default 
again results look 
note ways 
people appear related 
think initial plot 
final plot sensible structure terms overall 
difference coming clusters 
individuals generation 
overall clusters 

far considered relatively discrete kinds 
tasks output response expectation etc depends 
input pattern world real tasks 
extend time obvious example language 
meaning currently reading 
depends sequence words sentence 
language words themselves constructed sequence 
distinct sound patterns phonemes 
examples including tasks driving 
work seem sequential tasks 
exception 

consider here general categories temporal 
sequential temporally delayed continuous 
sequential case sequence discrete events 
structure aka grammar sequence 
learned temporally delayed case delay 
event production 
output consequences example 
corresponding 
smoke fire etc ones 
slow coming benefits degree 
here issue 
learning particular sequence events 
determining relationships set possible 
events delayed 
obviously mutually cases issues 
apply task important type learning 
applied temporally delayed problems called 
reinforcement learning based idea 
temporally delayed reinforcement backwards time 
update association earlier states 
likelihood causing subsequent reinforcement 

finally case focus 
timing continuous case relevant 
information detailed temporal evolution 
best described continuous system discrete set 
events obviously important motor control 
perceptual tasks discrete tasks 
detailed timing matter meaning 
sentence depend significantly rate 
read case possible 
sequence taking set regular 
long 
order capture relevant information 
difference sequence 
important processing continuous depends 
critically detailed temporal response characteristics 
neurons biological parameters weights likely 
critical further available mechanisms learning 
continuous 
far 
biologically plausible tend work well 
relatively simple tasks 

learning sequential temporally delayed tasks 
challenging area particular neural 
network models reason adding 
time steps output input contingencies lot adding 
extra hidden layers network way learning 
time backpropagation additional time step 
equivalent adding new hidden layer input 
output layers 
advantages model learning biases specifically hebbian learning 
inhibitory competition leabra algorithm weve 
learning deep networks expect 
useful learning temporally extended tasks example 
explore indicates likely case follow 
interesting complex cognitively relevant 
tasks 

explore context representations deal 
learning sequential tasks move 
reinforcement learning temporally delayed learning 

central problem learning sequential tasks developing useful 
context representations capture critical information 
previous events needed produce appropriate output 
interpretation later point time simplest 
case called case prior context 
necessary immediately previous time step case 
particularly convenient problems 
information away 
arise context include information prior time 
steps interesting note non environment 
context 
representation prior state representation include 
sufficiently rich amount information contain 
necessary contingencies case 
context representation contains copy 
prior states 

simple recurrent network srn context 
layer copy hidden layer activations previous 
time step 

neural network model incorporates style context 
representation developed 
known simple recurrent network 
srn called network 
context layer network input layer 
activity state set copy prior hidden 
version output version unit activity state 
see hidden state based context 
layer network considerable flexibility 
context representations learning 
representations hidden layer 

adopt basic srn idea main way 
sequential tasks later chapter 
simple limited context 
srn introducing additional mechanisms introduce 
greater flexibility represent 
context exploring basic srn couple issues 
addressed nature context layer 
representations updating further 
computational perspective need examine biological 
representations explore 
simulation basic srn 

standard srn context layer 
copy hidden layer computationally 
convenient necessary basic function performed 
layer context layer 
information transformation hidden layer 
set weights going context 
hidden layer adapt fixed slowly updating 
transformation hidden layer representations context 
layer important possible biological 
implementation context discussed 

essential context layer copy 
hidden layer essential context layer updated 
controlled manner example alternative idea 
implement context layer seem easier imagine 
biology implementing kinds network dynamics described 
additional layer 
free context units presumably connected amongst 
themselves enable sustained activation time somehow 
maintain information prior states special 
operation context units communicate 
hidden layer via standard bidirectional connections 
couple problems situation 

basic tradeoff context units 
information prior hidden state hidden units settle 
new state new input updating new state 
hidden units units need 
stable 
generic activation functions well related problem 
driven error learning procedure generec take 
account activation units 
settle learning based final activation states 
reasons simple context representation work 
simulations easily demonstrate 

simulations simply 
operation update context representations equation 
update context unit 
parameter determines extent 
context unit gets updated new input 
typically set determines 
reflects previous state 
basic srn constants typically 
additional gating mechanism place control 
special set units network 

simulator following steps need taken 
context layer needs set 

units projection projection 
hidden layer context layer needs 
tells context unit hidden unit activation 
standard full projection going opposite 
direction finally needs 
completely network start trial 
context layer previous hidden context layer 
activations done setting parameter 

default ensure network starts settling 
state context state variables 
processing preserved 
state state 

obvious brain area 
context representation frontal cortex 
discuss greater detail frontal 
cortex frontal pre cortex pfc seems involved 
planning extended temporally behaviors example 
people frontal lesions incapable 
sequence behaviors tasks 
perform individual step 
perfectly well appear specific deficit 
steps 

addition frontal cortex appears important 
maintaining representations time discuss 
argued 
demonstrated neural network models internal 
maintenance system important called context 
example example ambiguous words 
require kind context meaning 
writing implement argued pfc 
responsible maintaining necessary internal context 
words context established 
information presented earlier text further showed 
people pfc functioning 
context appearing immediately ambiguous word 
context appearing previous sentence 

findings internal 
context maintenance points directly srn context 
layer provides context necessary sequential 
temporally extended tasks further data 
show context representations 
produce sequential behavior 
sequentially presented information language see 
model 

addition behavioral data frontal cortex 
lot evidence neurons brain area exhibit 
sustained firing relevant task delays sequences 

frontal cortex receives projects 
areas posterior frontal non cortex 
connectivity produce appropriate context representations 
context representations affect ongoing processing 
posterior system considered hidden layer 

finally likely basal ganglia 
play similar role 
frontal cortex lower levels temporal extent 
relevant motor control perception rapidly 
changing stimuli speech sounds basal ganglia neurons 
firing patterns pfc context representations 
lesions areas 
produce motor deficits argue 
possible basal ganglia 
interact frontal cortex via thalamus implement 
selection mechanism described aspects 
context update point time 

good example task srn works 
reber grammar task modeled 

psychologists implicit learning 
explored people press buttons 
corresponding letters appear sequentially screen 
subject letters followed regular 
grammar subjects pressing buttons faster 
faster sequences letters 
significantly faster sequences follow grammar 
shown evidence learning grammar implicitly 
explicitly knowledge grammar 
asked 

simple state grammar reber 
string grammar produced starting 
start generating letter link followed 
chosen random probability 
string ends end reached example 
string 

grammar shown 
figure 
state fsa state grammar 
string letters letter corresponding 
link takes 
sequence produced follows 
starts start letter 
equal probability next chosen 
letter 
process going 
generating letter link continues end 
corresponding letter reached 
connectivity regularities present 
grammar 

srn learn 
reber grammar training network predict next letter 
sequence output network prior 
input link fsa unique letter 
task relatively easy input uniquely 
identify location fsa different 
same letter kind internal context necessary 
keep based prior history 
grammar context layer srn 
next letter actually chosen random possible 
best network activate 
pick random backpropagation 
network 
pick items random produces blend 
possible outputs see detail 
leabra network pick multiple 
possible output patterns essentially random take 
advantage simulation 

start reber grammar simulation directory 
see network log windows addition process 
control overall control panel 
usual begin exploring network click 
observe connectivity note particular context layer 
units single receiving weight hidden units 
context units connection determine hidden unit 
update weight random value 
reasons otherwise network fully 
connected notice 
simply display purposes shows possible valid 
outputs compared actual output 

lets view activations see trials learning click 
sure 
locate process control panel 
right minus 
phase beginning sequence pass fsa 
grammar starts letter context 
units network produce random expectation 
letters coming next note noise 
unit activations helps pick unit 
possible ones random again see plus 
phase see possible subsequent 
letters strongly activated unit indicates 
letter actually came next sequence network 
ever learns possible subsequent letters 
trial chosen random learn 
possible outputs integrating experience 
different trials things 
challenging task learn 

interesting aspect task network 
done good possibly roughly 
errors ends discrete 
output come next right time 
reason cause problems learning introduced 
systematic error signal constantly increase decrease 
bias weights problem unit 
active inactive 
overall net error zero note allowed 
units active case 
units active introduce 
net negative error large magnitude negative bias weights 
eventually activation output units 

possible network pick output random 
allowing somehow means 
network actual response 
time actually 
case hidden layer representation remains essentially 
same outputs reflecting identity 
change actual output presented 
plus phase higher level internal representation 
possible outputs level lower output 
representation randomly situation 
important later consider networks represent 
multiple items see further discussion 

order monitor networks performance learning need 
error statistic zero network learned 
task perfectly case standard sse 
new statistic error 
output unit possible outputs shown 
layer labeled 
log displays 

now continue minus phase next event 
sequence see now units 
updated copy prior hidden unit activations verify 
click show plus phase 
activations previous event point continue 
reset sequence want 
network runs special type environment 
called dynamically new sequences 
events epoch whole bunch 
training examples underlying fsa advance 
line implements reber grammar fsa 

takes train network 
time load trained pre network training log 
file load network network 
window select load log file 

select 

network take epochs learn 
problem point gets zero errors epoch 
range random networks ran trained pre network 
took epochs zero trained longer 
epochs total point row 
representations robust 
noise network errors 
extra training epochs amounts different sequences 
epochs amounts sequences set sequences 
epochs case faster 
backpropagation network 
took 
sequences epochs scheme 
able train backpropagation networks larger hidden layers 
units learn epochs 
evidence advantage additional constraints 
model learning inhibitory competition task 
leabra networks generally learned faster backpropagation 
networks required larger learning rate 

now test trained network see solved 
problem see well 
letter locate testing 
process control panel 
test network sequence letters results 
shown grid log right note network display 
updated cycle see 
possible outputs network producing 
correct outputs indicated column 
fact pattern matches 
pattern due noise 

better understand hidden unit representations need 
sequence reasonable length events 
longer sequences fsa due 
selecting tell 
representation individual total number 
events sequence events counted 
again find sequence events need 
find sequences 
epoch 

running sequence events press 
bring cluster plot hidden unit states 
event interpret cluster plot clusters 
events zero distance terms 
hidden states letter labels 

now back process control panel change 

produces random sequence letters obviously network 
capable letter come next 
lots errors network 
detector determine string fits grammar 
sense network fsa structure itself 
own representations 

context layer srn provides means 
immediately preceding context information cases 
need able learn temporal contingencies 
time steps specifically need able solve 
temporal credit problem recall discussion 
driven error learning credit 
problem units responsible 
current error signal temporal credit problem 
similar events past 
responsible subsequent outcome see 
temporal credit problem solved similar way 
structural form credit 
earlier context driven error learning 
based time form driven error learning 

primary means solving temporal credit 
problem temporal differences learning algorithm 
developed based similar earlier ideas 
model phenomenon reinforcement learning 
reinforcement learning 
based idea relatively global reinforcement signals 
reward drive learning 
enhance reward avoid kind learning 
goes conditioning 
form learning solve temporal credit 
problem closely related psychological 
biological relevant phenomena fact recently shown 
detailed properties algorithm close 
relationship properties subcortical brain areas 

start discussion biology reinforcement 
learning standard algorithm 
show notion activation phases generec 
algorithm implement version 
leabra algorithm relationship 
standard driven error learning apparent 
explore simulation learning action 

primary brain areas appear specialized 
reinforcement learning midbrain 
ventral area substantial 
cortical subcortical areas control firing 
neurons neurons midbrain areas project neurotransmitter 
dopamine widely frontal cortex basal ganglia 
action dopamine likely learning 
areas things areas provide 
relatively global learning signal brain areas relevant planning 
motor control see properties firing 
neurons consistent temporal differences 
learning rule 

midbrain play role 
reinforcement signal brain areas required 
control firing signal see key idea 
reinforcement learning computing future 
reward complex task performed areas frontal 
cortex basal ganglia project control midbrain 
evidence basal ganglia neurons 
representing reward provided neural 
summarized 
evidence areas frontal cortex 
related structures involved 
patients lesions 
area show ability predict future comes control 
behavior based 

control distribution 
midbrain signals basal ganglia frontal 
cortex shows basal 
ganglia control firing 
sends dopamine back entire basal ganglia shows 
parts frontal cortex control ventral 
area sends dopamine back entire 
frontal cortex 

shows relationship 
controlling areas midbrain areas 
dopamine signal case basal 
ganglia system fairly well 
established areas called 
control constitute distinct subset 
basal ganglia dopamine signal coming 
affects basal ganglia signal controlled 
specialized subset areas notion distinct 
system essential aspect learning framework 
called established well 
plausible frontal cortex exist well 
certain areas play role 
controlling dopamine signals entire frontal cortex 

finally data firing properties neurons 
documented simple 
conditioning tasks particularly 
shows start conditioning 
task neurons fire reward presented 
experiences repeated trials reliably predicts 
reward neurons start fire onset 
seem fire whenever reward reliably 
early learning reward 
actually presented learning comes 
note neurons fire fire 
reward itself actually pattern firing 
essential computation performed algorithm 
see note firing 
predicts predicts 
reward called order conditioning 
neurons learn fire onset new 
subsequent actual reward 

now see biological properties discussed 
provide good fit temporal differences algorithm 
developed basic framework 
algorithm reinforcement learning algorithms 
produce actions 
environment environment produces 
upon delayed effects actions 
goal naturally produce actions result 
maximum total amount reward 
come long ways future 
typically interested future 
expressed mathematically 
factor 
determines ignore future reward 
obtained time time considered relative 
event beginning training trial 
expectation repeated 
trials note power future 
time increments gets smaller smaller times 
further future unless 

plays role 
squared sum error sse entropy cross error 
driven error learning algorithms objective 
learning called objective function 
particular function called value function 
term thought expressing value 
things point time whereas error driven learning 
goal minimize objective function goal here 
rapidly apparent 
function going difficult 
value point time depends happens 
future same issue temporally delayed 
weve discussed now see showing 
objective function 

approach taken algorithm problem divide 
specifically problem basic 
components component learns 
estimate value 
current point time based currently available information 
component actions 
take components map onto components 
basal ganglia frontal cortex previous section 
job 
resort value function 
different alternative actions order select action 
perform next right reward receive 
compared going left todo clear 
focus 

sensory estimate value 
call value distinguish 
actual value needs 
learn sensory predictive reward sounds 
happens conditioning obviously ever 
reward actually receives 
propagate reward information backwards time point 
reliably sensory cue 
point time 
next point time adjust 
current point time words looks 
ahead time step updates estimate predict 
ahead look value initially learn predict reward 
immediately time step reward happens 
next time able predict 
reward predict backwards 
time reward 

note propagation takes place repeated trials 
trial error backpropagation 
procedure error way network 
point error received means 
require remember information 
point reward required 
order propagate reward information back time 
point reward 

see happens mathematically start noting 
written terms 
follows 
same relationship hold estimates 
now define error tell 
update current estimate terms ahead look 
estimate next point time computing 
difference represented value 
estimate according 
current estimate 
note expected value notation 
changes slowly time compute expected value 
hebbian learning rule note equation 
based notion future reward 
consistent time time 
time error signal measure 
residual said learning 
able temporal delays notion consistency 

computes total expected 
future reward based current stimuli 
learns weights minimize difference 
estimate value based time step ahead look 

last thing specify exactly 
computed directly external stimuli 
error signal adapt estimates 
expect neural network computes 
based weights representations stimuli 
potentially processed hidden layers see 
illustration error 
train weights network computes 
treating same error signal 
squared sum error entropy cross error 

extent stimulus environment 
reliably produce correct value 
network learn function error 
reliable stimulus exists reward remain 

stages learning simple conditioning 
experiment showing error function time 
shows initial trial reward 
shows estimate 
reward gets earlier earlier shows 
final trial onset completely predicts reward 

now lets see learning works practice 
same kind simple conditioning experiment shown 
case reward 
fixed time figure 
shows function time reward coming 
starting shows 
happens trial learning large 
error reward occurs completely 
refer 
means 
note set case 
weights produce increase 
value larger next time say effects 
reduce value next time 
propagate reward backwards time step 
equation time 
includes shows 
propagation occurs way back finally 
shows final state network 
learned propagate further back 
predictive stimulus earlier time 
network occurs 
reward follows provides nice fit 
neural data shown 
computing error well explore example 
further 

finally need specify 
learning error signal easily 
train network produce actions increase total 
expected reward see lets imagine network 
produced action time action 
leads directly reward leads previously 
increase future 
positive adjust weights 
network similar way network 
increase likelihood action produced again 
similar possible action 
time step led greater reward produce 
larger weight changes weaker reward 
clear error signal provides 
useful means training system itself 
reflected biology fact dopamine 
signal represents error projects 
areas control dopamine signal itself 
areas considered network see 

noted different learning 
exist general category 
reinforcement learning algorithms generally 
mathematical analysis performed showing algorithm 
converge correct result 
particularly important 
called basically averaged time 
activation value learning 
activation value role analogous 
hysteresis parameter srn context 
units value 
parameter represented 
form parameter case 
implicitly considering 
included activations 

able activation differences implement 
driven error learning relatively straightforward 
learning looking 
consider value represent minus phase 
activation output unit 
represents plus phase activation difference 
phase values error error 
generec based phase driven error scheme weights 
rest network automatically updated 
reduce error standard generec learning 
phases activation completely 
introduce learning overall leabra framework 

issues need addressed regarding way 
phase values actually computed lets consider 
happens network experiences reward case plus 
phase activation equal reward value plus 
additional expected reward current reward 
simpler 
consider additional term zero 
unit reward value plus phase fact 
reinforcement learning entire network 
reset reward achieved new trial 
referred reward 
reward assumption reward value 
plus phase well see moment minus 
phase 

now lets consider happens isnt reward 
obvious thing here allow output unit 
settle value minus phase representing current 
future reward state problem 
plus phase explicit reward 
future 
opposite todo provide high level 
settle plus phase obtain 
set minus phase value next 
time step previous plus phase value 
assuming zero unless plus phase clamped 
value plus phase exactly 
detail parameter well consider 
moment next set plus minus phases 
current time now set 
value computed plus phase estimate next 
time steps value network 
actually value expected future reward 
next time step current 

relationship time steps 
based phase algorithm plus phase unit predicts 
value function next time step consider 
unit phase ahead stimulus represented 
forward units relative stimuli 
think plus phase actually minus phase 
next time step learning based phase difference 
stimulus activations previous time 
step 

performed context standard generec driven error 
learning based phase scheme learning effectively 
offset phase illustrated 
output unit settling 
plus phase time computes expected value next 
time step experiences actual outcome form 
reward figure clear coherent 
think plus phase unit minus phase 
next time step consistent idea minus phase 
next minus phase actually plus phase 
time step unit clamped value 
computed 

unit computing based stimulus activities 
time error updating actually 
computed next time step weight 
computation sending activations time 
error minus plus phase difference time shown 
important note 
time based phase implementation 
aspect algorithm 
requires future states adapt 
prior estimates contingencies time 
step allows network propagate information 
future back time 

implementation explained here 
biologically plausible combining context 
representations srn model explain further 
error signal 
control context representations updated 
context representations simplifies issues time 
discussed version algorithm 
actually think brain implementing biologically 
plausible otherwise seem 

finally note implement parameter 
set minus phase activation 
reflects idea actual value computed 
prior plus phase actually 

activation computed settling typically 
value simplifies things considerably 
appropriate isnt problem 
future states 
point receive next reward well 
discuss effective greater 
delayed achieved 
simultaneously performing learning multiple time scales 

explore learning rule based phase implementation 
described simple conditioning task 
discussed network learn stimulus 
reliably predicts reward stimulus reliably 
predicts need 
algorithm context nature stimulus 
representations network 

recall said delta rule aka 
rule provides good model conditioning 
needed issue 
timing timing stimulus relative 
response fact rule equivalent delta 
rule happens time step 
match sensitive 
timing relationship importantly purposes modeling 
timing provides particularly clear simple demonstration 
basic properties learning 

problem simple demonstration involves 
representation timing basically stimulus 
representation distinct unit stimulus point 
time unique units weights 
learn representation complete serial 
proposed 
see exactly 
works look model imagine brain 
involves error signal control gating 
stimulus information active memory context 
representation srn stimulus 
identity setting set neurons range 
different activation different time 
periods subset timing units active 
point reward learning come 
represent expected delay stimulus onset 
reinforcement well explore type mechanism further 
simpler time 

open project lets 
start examining network input layer contains 
units represent 
different stimulus columns represent points time 
single unit receives weights inputs 

lets see works action locate 
process control panel 
happen stimulus reward present 
monitor time steps value 
displayed bottom network view time 
step sequence events simulator continue 
steps input activation represents fact 
stimulus stimulus row came continue 
active time steps notice 
stimulus unit activated 
reflects fact reward received phase plus 
activation unit clamped reward value here 

now lets see reward weights 
zero begin click unit 
notice weights increased unit representing 
stimulus last position went 
reward caused unit minus phase 
plus phase updated weights 
based sending activations previous time step 
discussed previous section seen 
graph log 
minus plus phase difference unit 
function time step clearly shows 
continue goes back 
maintaining reward active end entire sequence 

now switch back again 
recall weight unit increased 
activation unit expected 
due thresholded nature units click 
continue trial shown 
bottom network time step due 
weight changes previous trials weight 
unit now strong activate threshold 
look graph log see now positive 
time step network now 
reward time step earlier 
effects click note weight 
previous time step now increased result 
positive lead trials 
reward earlier now 
reduced magnitude now 
click back network let process play 
process control panel see 
forward weights graph 
log ultimately resulting activation unit 
stimulus comes same process 
shown represents 
algorithm 

point standard phenomena 
conditioning explored model well look 
order conditioning occurs 
stimulus longer predictive reward 
ability predict reward appropriate 
simulate simply turning reward appears 
locate overall control panel 
contains parameters determine nature 
stimulus input reward 
controls look field controls timing 
stimulus representing time 
stimulus comes long 
fields provide variance points 
zero explore own later reward 
known conditioning 
stimulus view timing parameters 
stimulus mechanism control probability 
coming want manipulate 
control probabilities contained 
field button 
press select indicates 
stimulus presented 

now graph log trial 
happened point reward occur 
explain happened equations 
network describe occurs next terms error 
signals plotted graph log explain 
network done learning again stimulus 
expectation reward 

thing 
procedure weights reduced back zero 
reduced bring unit threshold 
effects threshold applicable real brain 
appears unit constantly active low 
level additional inputs driving resting 
potential threshold effectively closer 
simulation expect weights 
reduced bring unit threshold 
behavior suggest complete 
least situations kind threshold effect 
work 

now lets explore order conditioning 
network stimulus association press 
onset stimulus clearly driving expectation reward 
note happens faster due 
now turn stimulus starts 
time steps see field 
control panel selecting 
already trial back look 
weights essentially stimulus acts 
reward positive allows 
stimulus learn predict stimulus 
stimulus 

point feel free explore parameters available 
see network responds note change 
parameters sure press button order 
new environment based new parameters 

final regarding limitations 
representation example learn 
conditioning order 
saw properly configured representation allow 
learned question 
time zero right point trial 
properly finally requires stimulus 
manipulate 
last problem points important issue algorithm 
learn temporal requires 
stimulus representation support 
see problems 
resolved allowing system control updating 
context representations 

form ltp mechanism cortex based 
nmda receptor allows calcium ions 
enter synapse response conjunction presynaptic 
neural activity excitatory 
neurotransmitter glutamate postsynaptic activity 
sufficiently excited membrane potential 
calcium synapse triggers complex sequence chemical 
events ultimately results modification synaptic 
efficacy weight available data suggests pre 
synaptic neurons strongly active weight increases ltp 
due relatively high concentration calcium weaker activity 
results weight decrease ltd due lower 
concentration calcium called associative 
hebbian learning important model learning 
addition important way calcium 
low concentration lead ltd transient neural activity 
turns important biological mechanism 
driven error learning taken biology ltp suggests 
combination hebbian driven error learning mechanisms 
work consistent computational advantages 
combination 

goal model learning develop internal models 
information present world difficult 
relatively limited relevant information via senses 
possibly represent information 
experience appropriate 
biases world order 
organize experiences simple parsimonious 
models critical successful model learning strong bias 
towards representing correlations appropriate 
reflect reliable stable features world parsimonious 
representation correlations involves extracting 
principal components features dimensions correlations 
simple form hebbian learning perform 
principal components analysis pca modified 
fully useful importantly 
individual units represent principal components 
subset input patterns implemented 
network property inhibitory competition described 
previous chapter results distributed representations 
informative principal features 
features sub input unit effectively representing 
strongest principal component input items 
best adapted specialized represent biological 
processes accommodation sensitization 
important different types 
input patterns well represented appropriately specialized 
units 

hebbian learning number ability learn 
produce particular output patterns function particular 
input patterns seen simple 
pattern associator task 
error target output pattern 
actual output pattern drive learning networks 
learn successfully idea delta 
rule error weights function 
derivative error order learn complicated 
output input relationships functions networks least 
intermediate hidden layer necessary 
networks require generalized form delta rule 
error signals chain rule 
propagate error derivative back multiple layers 
units evidence cortical neurons 
communicate error signals directly possible 
difference activation states phases compute 
essentially same error derivative activation 
phases corresponds expectation production 
particular output pattern corresponds 
experience actual outcome long 
network bidirectional weights difference 
activation states units network 
reduce error layer idea 
generec algorithm seen generalization 
earlier algorithm order biological 
synapses compute weight changes necessary algorithm 
perform ltd expectation state unit 
outcome state ltp otherwise assuming relatively 
rapid expectation outcome activation 
phases expect ltd transient expectation 
consistent biology ltd see 
expect ltp sustained activity expectation 
outcome phases according purely driven error learning rule 
biological mechanism suggests ltp occur case 
biology resolved assumes 
hebbian driven error learning taking place 

sound functional reasons believe hebbian model 
learning driven error task learning taking place 
cortex see later chapters types learning 
required account full range cognitive phenomena 
considered further model learning provide important 
constraints biases development representations 
otherwise purely task learning context understood 
terms variance bias well known 
phenomenon statistics result representations encode 
important statistical features activity patterns 
same play role solving particular 
tasks network perform viewed way task learning 
seen important way conditionalizing model learning 
combination appears produce better performance cases 
compared simply shaped task demands 
better performance based pure model 
learning relevant solving 
tasks 

sequence temporally delayed learning require proper 
development maintenance updating context representations 
specify location sequence appropriate 
factor subsequent outcome biological 
mechanisms facilitate forms learning including 
connectivity cortex basal ganglia 
thalamus modulation learning neurotransmitter 
dopamine specialized aspects frontal pre cortex 
pfc computationally mathematical framework 
reinforcement learning useful understanding 
type learning work provides possible means 
understanding important roles reward motivation 
learning computational mechanism active gating 
useful controlling maintenance updating context 
representations dynamic manner required complex tasks 
biological evidence gating mechanisms operating 
pfc locations 

book 

last chapters competitive 
learning 

lots 

part text presents models cognitive 
processing taking place cortex brain areas building 
upon principles basic mechanisms developed part 
text chapter overview general function 
large scale organization brain areas presented focus 
functional computational 
observed different brain areas emphasize 
specialization understood common 
principles hold areas 

goal chapter provide useful coherent 
framework specific models subsequent chapters 
related framework supported 
existing data reflects common thought number 
researchers years remains 
certain aspects suggest reader view 
follows broad framework take 
established fact 

central organizing principle framework notion 
achieved achieving achieving 
tradeoff different functional 
tradeoffs identified specialization 
understood means separately 
otherwise system try achieve 
turns mechanisms principles 
developed part play important role tradeoffs 
help explain aspects large scale organization 
brain 

begin brief summary general functional 
computational principles underlie cognitive models 
provide brief overview different anatomical areas 
cortex relevant aspects subcortical 
describe functional organization areas terms 
following specialized systems posterior perceptual 
motor cortex pmc frontal pre cortex pfc 
hippocampus related structures hcmp organization 
based underlying tradeoffs rate learning 
nature resulting representations cortex pmc 
pfc slow integrative integrating instances 
hcmp fast keeping instances 
separate ability maintain representations 
active state delays face interference 
ongoing processing active maintenance pfc appears 
specialized active maintenance plays important role 
active memory controlled processing 
executive control cognition taken common 
underlying mechanisms broad provide framework 
cognitive architecture different cognitive 
phenomena explained terms interactions 
specialized systems addition common principles 
mechanisms applicable areas finally end 
chapter address number general problems arise 
framework 

divide discussion general properties 
cognition structural dynamic aspects 
structural aspects describe ways information processing 
system determined overall 
patterns connectivity relationships representations 
different levels stages processing dynamic aspects 
describe nature processing time determined 
activation flows processing levels 
achieves useful overall outcome 

number reasons detailed follows thinking 
processing information cortex 
generally levels 
hierarchical fashion different 
specialized pathways operate 
emphasize different aspects overall sensory input motor 
output intermediate processing number benefits 
rich interconnectivity different pathways number 
different levels think completely distinct 
parallel pathways 
highly interconnected clear 
processing information embedded same 
underlying neural distributed potentially 
wide range different processing pathways number 
important consequences elaborated upon subsequent 
sections 

begin considering basic building model 
cognitive architecture detector neuron presented 
here individual neurons viewed 
relatively stable representations detect difficult 
define complex set conditions inputs saw 
taken layer detectors 
perform transformation input patterns 
emphasizes distinctions patterns 
saw transformations 
shaped learning represent important 
structural statistical properties environment enable 
tasks solved 

cognition viewed hierarchical structure see 
sequences layers 
transformations operating sensory inputs ultimately producing 
motor outputs responses useful internal states 
provide interpretation environment 
important subsequent behavior discussed 
sensory input contains large 
low quality information highly processed 
sensible responses interpretations example 
sensory signals viewing same object different 
directly common 
overlapping activations typically sense 
interpret object same 
potentially elaborate sequence transformations 
sensory input stages detectors neurons 
distinctions collapsing form 
useful generally abstract internal representations 
invariant differences 
input selective sensitive 

same process performing transformations emphasize 
dimensions aspects collapse operates 
levels processing example representations underlie 
words emphasize 
features properties define word collapsing 
irrelevant ones example relevant feature 
physical size irrelevant 
whereas notion reality central 
affect 

generic hierarchical system showing specialized 
pathways streams processing pathway 
connectivity 

important consequence hierarchical structure 
existence specialized distinct processing 
pathways streams necessary layer 
processing hierarchy requires specific types 
transformations performed previous layers 
accomplish particular job addition typically 
potential transformations input irrelevant 
transformation sense group 
relevant transformations coherent stream 
continue example visual object recognition layer 
processing visual representations letters digits 
appropriate categorical representations digit 
needs processing stream 
provide already invariant 
respect changes location image retina 
spatially invariant visual information 
transformation need 
possible digit location retina 
lead generalization knowledge 
novel retinal locations transformation 
necessarily require information 
useful described 
sense similar transformations visual 
stimuli overall visual form object processing 
stream appears cortex done 

same types specialization seen operating 
types hierarchical structures example 
functions typically divided 
different specialized processing streams 
details division typically 
details 
lot division 
level higher division 
developed summarizing 
level lower details levels analysis 

completely separate hierarchical structure 
effective flexible 
different specialized processing streams 
levels now 
connections lower levels mutually constrain 
processing different pathways better deal partial 
noisy novel particularly complex stimuli example 
explore idea visual form 
pathway interacts levels spatial processing pathway 
important result attention focused 
spatial scales depending confusions arise 
visual form pathway confusions level 
occur things way 
top hierarchy system deal rapidly 
appropriate level detail 

further general hierarchical structure comes 
higher levels processing pathway 
areas likely receive input different pathways 
own whole hierarchical notion 
here see 
better association 
areas associate level higher 
association areas collection 
perform constraint satisfaction processing 
described 

principles converge idea knowledge 
associated item distributed widely number 
different brain areas specialized processing 
pathways level higher association areas 
illustrates version 
general idea proposed ones 
representation item distributed 
different specialized systems semantic 
memory system evidence generally distributed 
model 

notion distributed representation large scale similar 
notion fine distributed representations 
discussed terms processing taking place 
layer pathway again kind nature 
brain scale small structure larger 
scale 

widely distributed view knowledge 
computer metaphor tend favor 
idea single representation 
knowledge item features stored 
convenient location bias towards assuming 
representation leads problems discussed last 
section chapter show distributed model 
problems 

important properties general structure 
knowledge processing processing 
transformations performed specialized 
dedicated systems neurons transformations reflect 
experience knowledge via learning 
transformations applied situation determined 
directly specific stimulus activation pattern 
processed easy system treat different stimuli 
specific properties consequences saw 
specialization important layers 
integrated elaborate processing stages 
subsequent stages come depend particular types 
transformations input layers turn reliably 
provide specific transformations subsequent layers 
representations enables rich set 
specific content associations built time 
connectivity different representations 

compare situation standard serial computer 
processing data knowledge explicitly separated 
processing typically operates whatever data 
arguments function advantage 
system relatively 
flexible function need written 
wide range different situations ability 
arbitrary variable binding case arguments 
function important flexibility 
well return later obvious advantages 
flexibility 
difficult treat different stimuli specific 
properties consequences resort sequences 
constructs elaborate representational structures 
clear exactly different 
stimulus compared 

basic tradeoff here specificity 
knowledge hand flexibility 
appears brain 
latter interesting 
view precisely deal 
type specific content real world knowledge led 
traditional computer metaphor based models 
human cognition seems 
case getting details right 
practical differences trees important 
world kind flexibility 
provided arbitrary variable binding demonstration 
point domain language provided 
following set due 
time 

specific world real knowledge necessary produce 
different interpretations words 
see 
flexibility favor specificity causes number problems 
resolved different ways discussed 

dynamic level view cognition result activation 
propagation interconnected hierarchical 
structure bidirectionally connected processing layers described 
via multiple constraint satisfaction property 
bidirectionally connected networks described 
network tend produce 
activation state response interpretation 
constraints imposed upon inputs 
learned weights stimuli familiar ones 
result straightforward rapid settling network 
relatively optimal state response stimulus 
require extended appropriate 
activity state 

case resulting activity state typically 
same time same stimulus presented due number 
factors learning sensitization 
importantly due influence maintained activation 
states prior processing provide additional 
constraints settling process internal states aka 
internal context dynamically interpretation 
response stimuli order provide coherent consistent 
set responses time 
simple example note following 
produce different interpretations word 

words preceding different internal 
context representations subsequent interpretation 

addition multiple constraint satisfaction 
amplification dynamics described 
pattern completion bootstrapping mutual top support 
etc play important role processing role mutual support 
providing form active memory elaborated further 
following section plays particularly important 
cognitive role relatively simple mechanistic basis 
particularly important aspect contribution inhibition 
described cognition 
form attention following section 

consequence bidirectional excitatory connectivity 
allows different representations mutually support 
important enables representations remain 
active absence derived excitation 
viewing stimulus rely 
internal excitation mutually supporting representations 
remain active time viewed form 
memory enables information time well 
call active memory 
based weight memory results changing weights new 
information see 
distinction active memory typically lasting long 
based weight memory active neurons 
ongoing processing active memory 
distinct advantage directly influence ongoing 
processing areas providing internal context 
described whereas based weight memories directly 
affect units weights see 
limits mutual support providing 
active memories active maintenance mechanisms 
needed robust flexible active memory system 
mutual support provides basic underlying mechanism 
main form active memory brain areas lack 
active maintenance mechanisms 

inhibition operating levels 
processing cortex discussed 
natural limitation amount 
activation number things 
simultaneously represented rise phenomenon 
attention aspects sensory input internal 
context ignored favor attention 
cognitive psychologists view attention 
discrete separable mechanism view emergent property 
constraint satisfaction limits inhibition 
similar described 
external environment internal context 
determine ignored further 
levels processing constrained inhibition levels 
mutually influence varying degrees via bidirectional 
connectivity attentional effects arise level 
abstraction important consequences processing 
levels functional level attention critical 
level processing representations 
different levels processing focused same 
underlying thing set things important solving 
problems discussed 

integrated basic principles mechanisms developed 
part book relatively coherent general framework 
thinking structural dynamic properties cognition 
now turn functional different brain areas 
relatively large scale analysis emphasis 
cognitively relevant brain areas including cortex parts 
system brain areas 

cortical associated functions 

human cortex generally organized 
contain number related specialized processing pathways 
level higher association areas general nature 
specialized functions described illustrated 
well following standard 
terminology describing locations aka 
upper aka ventral lower posterior aka 
towards back aka towards 
lateral towards towards 
middle 
lobe specialized visual processing area 
central posterior region receiving main visual inputs 
thalamus areas performing 
higher levels transformations well discuss areas 
detail 
lobe specialized functions including 
primary higher level perception regions 
level higher visual form object representations posterior 
regions see language 
processing lateral regions see 
term longer representations 
events regions 
regions hippocampus play important role 
rapid learning arbitrary information see 
obviously important 
relationship language speech 
perception contribute 
lobe specialized functions 
including spatial processing representing things 
located space see task 
specific perceptual processing organizing tuning reaching 
via visual perception 
regions primary level higher processing 
areas spatial nature consistent emphasis 
spatial processing areas temporal lobe 
important language left 
lobe specialized maintaining representations 
active state active maintenance 
executive control processing areas 
posterior regions 
contain primary level higher motor output representations 
consistent executive nature lobe 

group subcortical brain areas 
surface cortex called system consists 
principally hippocampus cortex 
contains thalamus 
areas 
mutually interconnected originally thought process 
information useful group 
system now known 
different individual roles better understood terms 
relationship cortex example hippocampus 
temporal cortex 
important rapidly learning new information different types 
interconnected 
temporal cortical association areas viewed 
top cortical hierarchy virtue 
connectivity located frontal 
cortex appears important tasks frontal cortex 
specialized including motor control action selection 
areas thought 
specialized cortical areas 
areas 
thought primarily specialized 
processing cognitive roles components 
system well documented 

thalamus subcortical area specialized 
provides sensory input cortex 
plays role attention provides source feedback 
basal ganglia group subcortical areas 
appear important sequence learning motor control 
see 
subcortical area important motor 
control timing representations cognitive role 
well finally midbrain small groups 
cells play important role state 
cortex relatively global fashion see 
systems 
control cortex play important role 
cortical regulation self controlled processing 

now attempt provide principled framework 
understanding different characteristics brain areas 
described based functional tradeoffs 
characterized systems brain areas tradeoff 
rate learning way interacts knowledge 
representations ability perform active 
maintenance delays face stimuli 
interacts ability graded distributed 
representations interconnectivity 

brain systems follows system 
posterior perceptual motor cortex pmc consists 
occipital temporal parietal motor sensory areas cortex 
areas directly responsible sensory 
inputs producing motor outputs higher level association 
areas serve integrate activities 
system frontal pre cortex pfc 
large pmc humans considerably smaller lower 
consisting frontal cortical regions forward 
motor cortex pfc appears specialized 
active maintenance information time particularly 
useful controlled processing responses 
mediated specific task constraints simply 
automatic responses stimuli system 
hippocampus related structures hcmp appears 
play critical role rapid novel information 

effect learning rate ability 
weight represent underlying conditional probability 
input unit activity output unit activity cpca 
hebbian learning objective conditional probability 
reflected case 
weight training example 
binary impossible represent overall 
probability apparent number individual 
examples 

functional analysis begins assuming cortical systems 
pmc pfc learning mechanisms described 
order develop representations 
important underlying structural statistical characteristics 
world effectively process perceptual inputs produce 
systematic useful motor outputs learning 
cortex necessarily slow order integrate 
individual experiences extract general underlying 
regularities environment 

shows simple example point 
taken exploration described 
slow learning 
rate enables weight converge actual underlying 
conditional probability event occurring environment 
somehow world provide underlying statistical regularities 
experience able away 
faster learning rate experience typically 
small noisy overall picture slow 
learning order blend 
note virtue integrating 
previous ones unique details specific 
remaining 

survival world demands rapid learning occur 
specific arbitrary information important 

saw family enter last notice 
rapid form learning memories individual 
separate integrating 
confused 
learning fast slow 
integrating basic tradeoff 
demands slow integration hand rapid 

seems brain resolved tradeoff allowing 
cortex learn slowly integrate experiences 
hippocampus provides 
strengths cortex complementary rapid 
learning system idea consistent large amount data 
people hippocampal lesions 
example 
known large 
temporal cortex including hippocampus removed 
prevent subsequently 
unable learn new information people 
events occurred etc able learn number 
relatively complex motor perceptual tasks showed normal term long 
priming effects recently read words example 
subsequently read rapidly preserved learning 
explained terms results slow integrative cortical 
learning mechanism ideas 
detail 

different type tradeoff understand difference 
pmc pfc time involving active memory 
maintenance information learning rate described 
active memories supported bidirectional connectivity 
important memory processing 
brain areas active memories 
conjunction highly overlapping distributed representations 
problems occur specifically 
distributed representations rely critically input order 
select appropriate subset distributed components 
activated absence selection 
information maintained system itself 
pattern overlap representations cause activation spread 
result original activity 
pattern 

illustration active maintenance 
via bidirectional excitation distributed representations 
value excitatory weights enable 
appropriate subset features maintained 
activating representations 
independent maintenance problem semantic 
features maintain 
level higher items terminal 

problem illustrated 
distributed representation encode different 
items terminal share 
total features monitor 
recall explored example 
inhibition needed prevent spread 
excitation overlapping distributed representations 
inhibition distributed representations 
active memories original input stimulus 
representations 
activity pattern difficult maintain unique 
subset features active activating 
activation spread unit via connections necessary 
maintain 
constructed avoid problem particular forms 
linear non inhibition weight modulation robust 
noise require further control mechanisms alternative 
shown panel figure independent 
isolated representations maintain themselves 
isolated representations lack distributed pattern overlap 
important representing similarity enabling based similarity 
generalization novel inputs easily maintain 
information active state similar 
representations explore phenomenon 

active memory highly overlapping distributed 
representations important again suggests 
tradeoff seems clear pmc areas require overlapping 
distributed representations order represent 
process complex visual perceptual inputs contrast 
large amount data supporting notion pfc 
uniquely capable robust active maintenance 

sense analysis pfc 
independent isolated representations 
isolated character representations function 
interconnectivity easily measured firing 
properties neurons solid evidence issue 
exist evidence discrete 
isolated connectivity pfc 
described 
data obtained idea remains 
basic point pfc perform robust active 
maintenance way pmc relatively well established 

finally particularly important additional advantage 
isolated representations pfc allow 
arbitrary combinations representations activated 
interference due 
kind combinatorial flexibility likely 
important factor achieving powerful solving problem 
executive pfc 

further insight specialization pfc active memories 
comes basic tradeoff ability 
maintain active memories face interference ongoing 
processing update rapidly new 
information needs maintained extent units 
simple bidirectional excitatory connectivity active 
memory interference 
bidirectional excitatory connections stronger prevents 
rapidly new representations 
activated existing ones updated 
inputs conversely 
weaker excitatory connectivity units sensitive 
inputs capable rapid updating enable 
sustained face interference tradeoff 
think pfc taken advantage midbrain 
systems provide gating mechanism controlling 
maintenance opened pfc representations 
sensitive inputs capable rapid updating 
closed pfc representations 
interference 
idea developed further 

diagram key properties principal 
brain areas active representations shown highly 
overlapping distributed representations 
overlapping non isolated separated weights 
active units shown solid lines active non 
units active representations shown 
thought feature values separate 
dimensions modalities pmc representations 
distributed embedded specialized specific 
processing areas pfc representations isolated 
combinatorial separate active units 
representing feature value areas pfc units 
capable robust maintenance self indicated 
recurrent connections hcmp representations sparse 
separated distributed conjunctive 
single representation active time corresponding 
conjunction active features 

central characteristics different brain systems 
described summarized form 
basic picture pmc 
learns slowly form integrative distributed overlapping 
representations interconnectivity exhibit short 
term active memory relatively easily new 
stimuli pfc learns slowly isolated 
representations dynamic regulation mechanisms enable 
maintain active memories longer delays face new 
stimuli finally hcmp learns rapidly form separated 
representations minimize interference similar 
memories taken systems 
basic principles earlier constitute description 
cognitive architecture brain aspects 
architecture general character well 
supported available data 

cognitive architecture described provides framework 
explaining cognitive phenomena explored detail 
subsequent chapters useful point elaborate 
issue cognitive control controlled processing 
ability adapt behavior 
different task demands generally act 
world issue obviously central 
understanding human cognition represents major 
problem computational models understand 
mechanistic basis cognition generally 
issues greater detail 
provide brief summary basic 
ideas here help models help 
elaborate important aspects general cognitive 
architecture 

controlled processing described contrast 
automatic processing 
thought 
involve limited capacity attentional system 
theories actually continuum 
controlled automatic processing 
view controlled processing 
graded defined extent certain 
dimensions task performance required particular 
controlled processing ability adapt 
behavior demands particular tasks processing 
relevant task information sources competing 
information relevant task behavior 
otherwise responses 

ways hcmp pfc contribute 
automatic versus processing controlled distinction bias 
provided pfc perform sustained 
processing facilitate processing learned weakly 
relatively tasks serve 
processing different areas binding 
provided hcmp rapidly learn 
information necessary perform novel tasks processing 
controlled processing involve 
contributions automatic processing performed 
independent 

aspects controlled processing 
terms hcmp binding pfc 
biasing 
illustrates central ideas account based directly 
pfc hcmp 
extent controlled processing task defined 
extent following conditions exist 

sustained learned weakly relatively 
processing required 
novel information rapidly stored 

extent automatic processing defined relative 
absence factors 

consequence robust dynamic active memory properties 
pfc relatively unique position 
provide sustained top activation bias 
processing posterior cortex brain systems 
dynamically update active representations according 
relevant task pfc bias processing 
rest system appropriate sustained pfc activation 
enable sustained focus extended temporally tasks 
additional top activation pfc facilitate 
learned weakly processing pmc otherwise 
dominated well forms processing 
task highly reading word relatively 
color pfc 
biasing processing different pmc 
areas different areas mutually constrain influence 
via located pfc representations 

hcmp hand contributes ability learn novel 
information rapidly interference typically 
binding familiar elements novel combinations 
relevant particular task particular 
combinations stimuli specific task intermediate 
states problem solutions etc think combination 
factors bias binding account 
distinction controlled automatic processing essentially 
automatic processing occurs via activation propagation 
pmc connectivity controlled processing 
activation propagation introducing dynamic relevant task 
constraints pfc hcmp 

appear account places large 
control pfc 
true control complex 
satisfaction constraint process interactions 
relevant brain areas likely specialized 
learning mechanisms associated dynamic regulation systems 
control gating updating pfc representations play 
important role point 
evaluate ideas implemented models exhibit 
relevant behavioral properties models 
described subsequent chapters constitute important building blocks 
add overall 
resulting system 

context issues cognitive control worth 
noting different systems cognitive architecture differ 
extent representations 
influential wide range areas opposed 
embedded specific processing areas 
influential view difference 
principally relative position overall hierarchy 
representations described hierarchy 
effectively defined far removed area direct 
sensory input motor output shows 
further removed areas located respect 
overall network connectivity resulting greater 
terms pfc hcmp top hierarchy 
influential 
general areas pmc 
essential emphasize view graded continuum 
distinction further pmc rich 
lateral connectivity areas same general level 
abstraction least levels sensory motor 
processing 

diagram purely hierarchical connectivity 
separate processing streams showing higher levels 
hierarchy virtue located 
overall network explicit 
equivalent standard view 
imagine actual connectivity 
purely hierarchical effect nonetheless relevant 

relate issues 
psychological distinctions explicit versus 
implicit versus view 
experience reflecting results 
global constraint satisfaction processing brain 
areas representations influential 
process greater 
general means highly influential areas 
pfc hcmp tend experience 
embedded areas pmc areas clearly 
associated explicit processing 
pmc subcortical areas associated implicit 
processing distinction add 
important big interactive system 
distinctions considered continuum 
notion 
assumed 
brain composed separable 
computations pass results 

next address number general problems arise 
general functional principles described people 
tend neural networks exhibit 
kind problem somehow models cognition 
example found case catastrophic 
interference found generic 
neural networks interference simple sequential 
list learning humans led conclude neural 
networks good models cognition 
showed 
actually tells important way brain 
works helps sense 
different kinds memory systems cortex hippocampus 
described 

important emphasize cases problems 
actually reflect documented limitations human cognition 
taking kind analysis 
approach argue human cognition perfect suggest 
reflects number tradeoffs 
fact neural network models seem provide useful insight 
nature human cognitive limitations real 
strength approach 

following problems lack flexibility 
resulting dedicated specialized representations 
indicated earlier tradeoff 
specialization flexibility dimension appears brain 
generally knowledge benefits 
specialized representations challenge 
problems understand measure flexibility emerge 
context system specialized 
dependent knowledge representations note 
largely examples based visual perception issues 
generalize aspects cognition 

multiple items 

illustration binding problem encoding 
terms separate features leads multiple 
items present input here red circle green 
square present input same representation 
activated green circle red square system 
know present illustrated 
representation 

commonly problems neural networks known 
binding problem arises whenever different features 
stimulus represented different underlying 
representations distributed representations 
multiple items need represented lets imagine 
set units encode color information red 
green blue set encode shape information 
circle square binding problem arises 
present red circle green square system 
know circle red square 
green way see 
illustration words system bind 
separate features applying same object 
see attention provides way 
problem place 
able restrict visual input image produced 
object time whatever activation results processing 
input automatically bound same object 

solution binding problem 
representations encode combinations input features 
color shape achieve greater representing 
multiple combinations show features 
objects 
feature separate representation shown columns show 
responses set representations encode 
separate features combination 
different conjunctions shown top column last 
red green blue square circle 

way binding problem represent feature 
information separate manner 
representations encode conjunctions different feature values 
red circle obviously impossible 
represent possible combinations way units 
required realistic numbers features individual 
units represent combinations conjunctions 
separate feature representations overall 
pattern activity units uniquely 
combination binding features present input 
shows works single 
additional conjunctions combination unit required additional 
unit responds red circle green square blue 
cases network 
otherwise confused basis separate features 
total number units required separate 
features conjunction unit 
units needed encode possible conjunctions 
scale problem advantages 
apparent example shapes 
conjunctive units required needed 
features plus conjunctions combinations scheme described here 
feature units conjunctive units 

different type solution binding problem suggested 
dynamics processing 
different feature elements idea type 
activations features 
same object encode binding information 
available 
evidence observed firing 
actually binding 
important observed 
likely natural consequence simple activation propagation 
spiking neurons brain neurons 
communicating tend drive spike 
roughly same time central problem 
brain achieve necessary 
kinds processing presumably taking place bound 
representations substantially addressed 
feature transient dynamic systems 
elaborate system dynamic processing 
established process binding information example 
unique consequence set associations red 
apply green blue 
information associated representation exists 
relatively temporal problem 
arise unique pattern activity set dedicated 
representational units case combination 
conjunctive representations scheme described 

finally essential cases people 
fail solve binding problem successfully 
provide important underlying representations involved 
example well known combinations 
visual features requires slow serial processing 
found fast parallel speed 
details complicated phenomenon consistent 
idea sequential mediated attention binding needed 
cases presumably sufficiently 
conjunctive otherwise distinctive representations relevant 
combinations 

extreme case binding problem occurs multiple instances 
same item clearly possible distinguish 
instances basis centered object features color 
shape least ways actual 
number items present accurately represented 
sequential application attentional mechanism 
items turn combined kind mechanism 
result appropriate representation situation 

unless items presented same location 
spatial location representations case 
item present case multiple instances 
same item important take account 
elements representations considering 
problem 

problem related representation multiple items 
arises whenever compare different representations 
attention share time same representational 
space different items work need 
actually compare items 
represented same time idea natural network 
dynamics inhibition pattern overlap result 
representation items common 
basis comparison recall exploration 
presenting multiple digits same network 

possible compare visible stimulus stored 
representation stimulus well 
visible stimulus activity pattern fits pattern weight 
values encode stored stimulus speed 
network settles see 
resulting activation state measures 
assess general fit stimulus stored assuming 
stored pattern obviously closest 
visible measure tell close 

idea posterior pmc 
frontal pfc representations comparison pfc 
thought redundant representations 
content pmc maintain information 
active form hold representation items 
represented posterior system viewing 
item comparison based overlap kind 
measure indicates consistent 
representations think large number 
different levels representations produced item 
cortex modalities 
different levels abstraction 
possible same idea implemented activating 
different pmc representations different items 
mutual consistency idea pfc somehow 
involved consistent existence same 
capacity limitations relative 
active memory 

number possible solutions problem 
aware implemented models demonstrate actual 
ideas 

emphasized notion structural hierarchy 
representations increasing levels abstraction 
forms hierarchical structure world need 
represented systematic fashion issue comes 
things components 
themselves component example 
face composed components eyes nose 
same time component body imagine 
spatially invariant representations visual 
aspects objects invariant representation nose 
structural relationship information parts 
components relative position nose face 
invariant representations 
invariant respect context object appears 
separate relationship representations somehow bind 
invariant nose representation containing face 
representation implemented neurons 

general solution problem imagine 
invariant representations active higher level 
system long representations share 
same units level lower visual feature 
representations encode structural information via same kind 
limited style conjunctive representations suggested 
solution binding problem well explore role kinds 
representations object recognition 
shared representation 
nose face invariant nose extent 
build same level lower features face 
representation conjunctive information 
relationship nose rest face 
invariant nose representation away context 

likely sequential attentional mechanisms 
focus different aspects objects 
state visual system focusing 
face whole different focusing 
nose higher levels system maintain invariant 
representations integrate sequential attentional 
lower levels attention way 
emphasizes information necessary process 
current focus attention focus nose 
emphasizes level lower features encode face 
encode nose job 
away context easier 

happens example face recognition particularly 
interesting issues appears 
representation face sum 
representations individual parts appear encoded 
objects 

challenge dedicated specialized representations 
problem 
same type processing different type 
processing needs performed middle 
processing step obtain result needed processing 
proceed serial computer current set state variables 
simply onto stored memory 
possibly same called 
appropriate arguments easy data processing 
separable particularly case same 
type processing performed different data 
previous data issue arises processing 
embedded bit 
imagine overall 
processing sentence composed number 
process turns 
tell people easily indicating 
limitation exactly type processing 

order model limited 
appear exist human cognition 
specialized systems rapid learning hippocampus 
active memory frontal pre cortex begin address 
issues 
aspect human cognition remains developed 

dedicated specialized representations appropriately 
recognize novel inputs generalization produce novel outputs 
problem major 
limitation neural network models cognition important point 
keep mind people actually particularly good 
knowledge learned context novel context 
clearly capable 
significant amount generalization 

important means achieving generalization 
learning associations appropriate level abstraction 
example lets imagine learns consequences 
visual image corresponding run away 
actual visual image 
retina clear learning took place 
level lower based representation 
generalize well subsequent situations image 
appear novel retinal location learning took 
place abstract spatially invariant representation 
images location retina trigger 
appropriate response same argument applies different 
levels representation system learn 
same similar representation 
instances appropriate generalize 
generalization problem 

think cortex organized according rough 
increasingly abstract representations abstraction 
solution generalization plausible further likely 
learning automatically tend form associations 
right level abstraction example invariant 
representation predictive correlated things 
associated whereas level lower representations 
learning task model 
sensitive automatically 
associations 

finally way thinking generalization terms 
distributed representations capture similarity 
structure novel item previously learned items here 
novel item represented terms combination known 
distributed features learned previously associations 
features provide basis correct responding novel 
item neurons naturally perform weighted average 
feature associations produce response reflects 
appropriate balance influences individual 
features see examples chapters 
follow 

interesting note problems caused 
set assumptions regarding nature 
representations involved example binding problem 
problem long 
representations generalization fine long 
knowledge encoded proper level abstract representations 
important question representational assumptions 
rise problem place 

finally emphasize solid 
cognitive architecture result powerful 
human cognition say properties 
sufficient simulate wide range cognitive phenomena 
challenging issues remain solved full scope 
human cognition understood mechanistic ultimately 
biological level 

chapter explore models visual perception 
level cortical representations abstract level high 
spatially invariant object representations demonstrate 
attentional effects emerge levels representation 
interactions different specialized processing streams 
focus studied upon 
senses similar principles likely apply senses 

discussed perceptual processing 
sequence transformations emphasize 
aspects perceptual input collapsing 
important learn visual 
system subjective 
experience constructed wide processing areas 
specialized extent particular aspect 
dimension visual world shape color motion 
depth location general structural principles 
specialized hierarchical processing streams discussed 
relevant here 

focus primarily pathway emphasizes object 
identity information aspects 
input irrelevant identity information examine 
role pathway emphasizes spatial location information 
interacts object pathway producing complex pattern 
spatial based object attention 
principles derived exploring subset phenomena apply 
generally aspects visual processing 

biology visual system obviously begins eyes 
completely clear ends increasingly abstract 
levels visual processing gradually cognitive 
processing considerable evidence 
visual brain areas activated person thinking 
semantic properties see 
section cover brain areas widely 
visual processing areas starting retina going next 
lateral lgn thalamus 
primary higher visual cortex occipital lobe 
etc continuing parietal 
temporal 

amount known visual system particularly early 
stages processing able provide sketch 
main findings here objective provide sufficient 
orientation empirical models described 
chapter focused presentation aspects 
visual processing particularly important object 
recognition aspects important 
processing motion color surface depth etc 
visual properties play role object recognition people 
good recognizing objects simple line 
indicates basic shape form information least 
sufficient object recognition 

addition representing basic form object 
general shape key properties necessary 
object recognition spatial invariance object 
recognized spatial location size etc 
models explore location size invariance 
best documented terms underlying neural 
representations well see visual system seems 
gradually build invariance subsequent stages 
processing 

retina light 
provides relatively highly processed signal cortical 
visual areas important understand nature 
processing models cases 
performing appropriate form processing pre actual 
images directly providing input representations 
roughly capture effects retinal processing 

general retinal processing 
performs contrast enhancement emphasizes places 
visual signal changes space tends enhance 
edges objects coding absolute 
values regions relatively constant 
beneficial result processing 
greatly representation visual scene 
pixel 
informative edges represented 

complex responsible retinal 
processing starts sensitive light 
cells selective color 
turn electrical signals subsequent 
stages retinal processing combine electrical signals 
local regions perform types 
different regions specialized 
specific retina mechanisms provide crucial 
contrast enhancement effect finally neurons 
provide output signals retina thalamus described 
next section 

center receptive fields computed 
retina bottom shows dimensional picture 
receptive field central surround regions upper 
show center dimensional 
receptive fields showing broad surround field 
central field combined form overall receptive 
field 

computed retina involve 
surround center receptive field receptive field 
neuron generally refers spatial distribution retina 
inputs light affect firing neuron 
later term generally refer set 
inputs activate neuron picture 
surround center target central region 
surrounding shows main 
categories surround center receptive fields center 
neuron active central portion 
receptive field surrounding portion 
center neuron active center 
surround 

illustrated figure center constructed 
tuned surround region 
tuned center region versa vice 
center individual tuning functions 
modeled gaussian normal shaped distribution 
function resulting surround center field called 
difference 

consider happen region light covered 
entire receptive field center neuron 
excitation inhibition leaving net 
effect light excitatory center 
center neuron inhibitory surround 
net excitation conversely light 
inhibitory center center neuron compared excitatory 
surround excited receptive field 
properties lead effect mentioned 
retinal output neurons cells fire 
change levels receptive fields 
constant focused 
coding retinal neurons 
center coding different color well 

different categories retinal cells 
identified meaning big 
meaning small generally speaking 
cells receptive fields color 
selectivity better motion selectivity better contrast 
sensitivity low light small differences compared 
cells high resolution small receptive fields 
better color sensitivity think 
cells uniquely contributing form processing cells 
motion processing correct types 
participate kinds processing participate 
varying degrees 

see subsequent sections basic retinal receptive 
field properties provide useful building blocks subsequent 
processing areas 

thalamus generally 
brain different sensory signals 
cortex places visual thalamus 
called lateral lgn 
information retina visual cortex 

brain area 
information resources 
retinal outputs directly visual cortex 
place 

increasingly people finding thalamus responsible 
number important forms processing 
existence fact remains basic 
center coding information retina 
relatively intact visual cortex 
complexity structural information 
visual signal reflects structure 
visual scene thalamus appears primarily concerned 
dynamic aspects information example good 
evidence certain neural lgn responsible 
temporal tuning properties neurons play 
important role motion perception 

dynamic aspect processing 
attention 
function comes large number projections back 
visual cortex back thalamus according 
estimates going forward 
projections factor projections back generally 
thought play role controlling attentional processing 
performed thalamus aspects regions visual 
scene dynamically focused 
thalamus 
visual input relatively 
structure uniquely suited implementing kind 
competitive activation dynamics entire visual scene 
result attentional effects similar argument 
regarding attentional competition modalities 
sensory modalities thalamus potentially 
further thalamus thought 
important levels versus 
controlling sensory input cortex 

explore models visual attention chapter 
models based mediated attentional processing 
attention similar principles likely 
apply focusing motion 
processing means models capture 
contributions lgn take advantage 
organization lgn organize 
center inputs models lgn different layers 
center cells different layers inputs coming 
eyes depth information 
reflected models 

next major processing area visual stream 
primary visual cortex known area 
back occipital lobe 
information subsequently processed subcortical 
brain areas well area builds input 
producing richer set representations provide basis 
subsequent cortical processing focus 
representations capture complex useful aspects visual 
form 

string center receptive fields represent 
edge surface goes 
kind edge represented 
dimensional shows lower values 
upper left higher values 
lower right 

considering center coding scheme provided 
retina lgn expect world 
points light 
combine basic receptive field 
elements represent basic building 
blocks visual form edges 
edge simply roughly linear separation region 
relative light showed 
neurons called simple cells encode oriented edges 
bars light proposed 
explain edge 
detectors constructed set lgn surround center 
neurons recent evidence consistent model 

edge detectors sense edges provide 
relatively way representing form object 
assumption region edges relatively 
visual system capture form 
information suggest 
visual system types 
neurons encode things color 
surface coding neurons summarizing 
surface properties region space well focus primarily 
visual form information encoded edge detector neurons 

imagine different kinds edges 
visual world edges differ orientation size aka spatial 
frequency low frequency means large high frequency means 
small position going light 
light light light light 
edge detectors exhibit sensitivity tuning different values 
different properties edge detector 
respond edge particular orientation size 
position responses properties 
optimal tuning example 
coarse coding visual properties 

different types edge detectors surface 
coding neurons dimensional 
visual cortex according topographic organization 
level neurons roughly organized according 
retinal position encode thought 
dimensional map organized according retinal space 
map number 
neurons encoding positions sensitive 
resolution high area center visual field 

scale large map surface coding neurons 
oriented edge detectors separated surface neurons 
structure called blob 
region edge detectors 
found region edge detectors appear 
organized according orientation neighboring neurons 
encode similar orientations relatively 
orientation found moving direction interesting 
occurrence orientations 
represented circle neurons 
middle circle orientation clearly coded 

extensively studied form topographic arrangement 
columns neurons respond 
input 

columns important 
depth coding present 
remains possible 

well see simulation key properties edge 
detector neurons topographic organization emerge 
cpca hebbian learning kwta activation function 
neighborhood interactions neurons 

diagram visual system showing 
pathway ventral pathway 
solid lines indicate full visual field projections 
whereas lines visual field 
adapted contains further 
details 

visual processing appears separate major 
streams initially 
described terms ventral pathway processing 
object identity information pathway 
processing spatial location motion information 
ventral stream goes lower 
part cortex occipital lobe temporal 
cortex stream goes upper part 
cortex occipital lobe parietal lobe reality 
brain areas bit complex 
simple story example likely pathway plays 
role visual information motor actions 
involves spatial location information certain 
kinds visual form information 
clearly pathways 
considered isolated processing 
well explore idea 
models described later chapter 

point thought distinction 
present retinal cells terms 
neurons now known types 
cells project processing streams likely 
cells play relatively unique role encoding motion 
information goes processed area called 
generally considered part processing stream 
now provide brief sketch processing streams 

ventral pathway representing visual form information 
object recognition thought stages lead 
increasingly spatially invariant representations addition 
complexity form information encoded representations 
increases stages receptive fields larger 

next area called appears contain number 
regions called specialized 
neurons emphasize different aspects visual information 
form edges surface properties color motion 
emphasized critical difference 
representations neurons 
exhibit feature selectivity range different 
positions neurons seen initial stages 
process ultimately produces spatially invariant object 
representations see 
kinds invariant partially receptive fields develop 
weights need configured achieve 

next major area receives inputs 
visual area appears primarily focused 
visual form processing object recognition here neurons 
continue process spatial invariance coding 
exhibit complex feature detection 

temporal cortex neurons finally achieve 
high level size location invariance measure 
invariance further neurons encode complex 
difficult properties shapes 
seems clear neurons provide distributed basis invariant 
object recognition face distributed object type 
details specific 
properties neural code see 

pathway represents spatial information 
information relevant action neurons pathway large 
receptive fields preferred motion fire 
neurons pathway information 
position eyes properties 
support processing information location 
objects pathway 

model explore level representations 
visual cortex area provide basis 
upon subsequent visual cortical processing builds 
important contribution model understanding 
properties representations described 
previous section explain particular types 
representations computationally useful nature 
visual world main benefits 
computational models cognitive neuroscience explain 
brain cognition certain properties 
provides level understanding 
properties 

computationally oriented way thinking edges 
represented terms correlational structure 
visual environment discussed 
reliable correlations pixels 
edge object objects reliably tend 
edges sense represent internal model 
environment basic pixel correlations 
represented edges subsequent levels processing 
represent higher level correlations arise regularities 
arrangement edges different kinds edge 
basic shapes etc 
higher higher levels visual structure see 
next model 

objective model described section show 
network learn represent correlational 
structure edges present visual inputs received via simulated 
thalamus recently showed network 
presented natural visual scenes 
manner generally consistent enhancement contrast properties 
retina develop looking realistic set oriented 
detector edge representations network based 
known biological principles intended 
demonstration idea sparse representations provide 
useful basis encoding world real visual 
computational models early visual 
representations developed 
aspects detailed properties representations 
recent 
models useful potential 
relationships biological computational properties 
resulting representations 

model present incorporates properties 
identified models important 
principled based biologically mechanisms developed part 
text relatively simple model standard leabra 
model hidden layer produces fairly looking realistic 
representations based natural visual inputs model 
cpca hebbian learning algorithm same processed pre visual 
scenes 
showed sequential pca algorithm 
spca produce appropriate edge detector representations 
blob representation 
shown 
reason believe based 
results conditional pca cpca 
hebbian algorithm organize self appropriate conditional 
correlational structure present image edges 

hebbian model learning case 
effectively assuming levels perceptual 
processing sufficiently removed particular task 
driven error learning play major role put way 
assume statistical structure input visual images 
sufficiently strong constrain nature 
representations developed purely model learning system 
need extra based task constraints model 
producing looking realistic receptive fields 
assumption extent see next model 
next layer network benefits model task 
learning 

model important properties 
representations orientation position size 
emphasized varying existing models 
properties dimensions model develop 
coarse coding representations cover space possible 
values dimension case actual neurons 
case orientation units preferred 
orientation respond weaker 
responses increasingly different orientations further expect 
individual units particular tuning value 
dimensions coding large edge light 
degrees location finally explore 
topographic arrangement dimensions neighboring 
units representing similar values enable subsequent 
processing layers representations 
explore role excitatory lateral connectivity producing 
topographic 

inputs model based center layers 
lgn project itself modeled 
single layer actually corresponds hidden layer 
area cortical layers input layer neurons cortical 
layer appear basically same 
center receptive fields lgn retina inputs 
cortical input layer lgn 
model lgn model here nice 
separation easier receptive field 
properties 

network presented images natural scenes 
etc processed pre 
effects contrast enhancement retina 
done spatial effects 
surround center processing units 
network positive negative 
weights activations biologically implausible 
separate center components 
separate components activations weights 
biological constraints presenting valued positive 
processed image pixels center input layer 
absolute values valued negative ones center 
input layer note absolute value reflects fact 
center neurons excited input 
negative images center 
surround center 
directly method advantage 
exact same input statistics 
comparison results 

critical aspect model relative scale 
model intended simulate roughly cortical hypercolumn 
relatively small structural unit cortex 
hypercolumn generally thought contain full set feature 
dimensions orientations sizes etc area 
retina hypercolumn neurons 
settle considerably reduced model hypercolumn 
main factor determining scale model raw 
number units patterns connectivity 
inhibition determines ways units interact 
extent process same different input 
patterns model seen representing hypercolumn 
basis following connectivity patterns units 
connected same lgn inputs actual neurons 
hypercolumn different individual connectivity 
patterns roughly same part lgn 
lateral connectivity relatively large portion 
neighboring units units common 
inhibitory system kwta layer kwta based average 
inhibitory function ensures units 
active time critical specialization 
units discussed 

consequence scaling small 
overall image presented inputs network 
hypercolumn similarly processes small overall 
retinal input units fully connected input 
layers contrast models 
require initial spatially topographic connectivity patterns obtain 
specialization units function 
subsequent models 
simulating larger visual cortex model 
want include spatially restricted topographic connectivity 
patterns 

mechanism topographic representations 
excitatory interactions neighboring units 
implemented circle connections surrounding unit 
strength connections gaussian 
function distance unit active 
tend activate closest via lateral excitation 
learning cause neighboring units active 
develop similar representations 
similar responses subsets input images 
discussed key idea 
networks explicitly 
specific activation onto units surrounding single 
active unit layer main advantage current 
approach kwta inhibitory function allows 
multiple activation multiple 
different features present input time further 
closer actual biology balance 
lateral excitation inhibition achieve topographic 
representations 

lateral excitatory connectivity 
edges unit right side hidden layer 
actually unit left side same top 
bottom shaped functional 
onto hidden layer important otherwise 
units edges 
middle activated 
problem cortex network huge edge units 
constitute relatively small percentage 

finally receptive fields need represent graded 
coded coarse tuning functions appropriate 
default weight contrast useful binary kinds 
representations results shown weight 
gain parameter weight contrast default 
weight gain weight offset interact 
higher offset needed gain offset 
default order encourage units represent 
strongest correlations present input see 

details 

open project directory 
notice network input layers 
size representing small center lgn neurons 
representing similar 
center lgn neurons specific input patterns 
produced randomly set 
larger pixels images natural scenes 
single hidden layer size lets examine weights 
network clicking hidden unit 
observe unit fully randomly connected 
input layers neighborhood 
lateral excitatory connectivity needed topographic 
representations 

select again network window locate 
order load single processed pre image training 
network images time memory required 
exploration now press 
observe activations network settles response 
input pattern 

observe center input patterns 
complementary activity patterns 
activity activity versa vice 
reflects fact center cell 
excited image middle edges 
receptive field center cell excited image 
edges middle true 
active image location important keep 
mind center units active positive 
activations extent image contains relatively 
region location coded unit actually 
negative activations encode 

hidden units initially random sparse 
pattern activity response input images note noise 
added processing units important 
satisfaction constraint settling process 
balance effects lateral connections 
feedforward connections input patterns 
useful here same reasons useful cube 
example studied noise rapid 
settling relatively equally good states 
network case lateral connectivity 
unit same lateral weights point hidden 
unit space trying activity noise 
needed break enable best 
activity level noise determined 
parameter overall control panel 

continue input patterns 
effects lateral weights particularly evident 
hidden unit activity patterns expect see 
weights playing dominant role determining 
activities hidden units now increase control panel 
parameter default 
continue increase effective strength 
lateral recurrent weights hidden layer 
change hidden unit activation patterns 

set back relatively subtle 
strength level lateral weights want network 
able multiple activity dominant 
important multiple different edges different 
orientations present image 
case 

let network run image 
develop set representations reflect 
correlational structure edges present input 
take load trained pre 
network point network window 
network trained epochs image 
total image 

select click upper left hidden unit 
see weights onto input layers 
indication vertical orientation coding vertical 
bar stronger weight values center 
bar middle input center 
bar adjacent left note 
network center bars adjacent 
locations same location complementary 
active same place arrangement 
center bars 
direction orientation coding line fact 
encoding edges change 
orientation edge 
order see clearly 
weights small magnitude click 
top color scale bar right network 
window good visual contrast weight display 
overall range values represented providing better 
contrast smaller weight values 

click next unit right next 
click back forth units 
observe center bar remains roughly same 
position center weights unit switch 
left surrounding central center bar 
reflects receptive field 
left unit organization 
center center region next unit 
organization center region 
center ones unit left goes back 
field center bar right further 
vertical orientation going right represents 
different related orientation coding compared 
vertical previous units looks 
smaller size 

individual units weight values 
aspects dimensions coded units 
topographic organization difficult overall 
sense units representations looking time 
single display receptive fields 
time view display press 
control panel select response file 
appears able press 
now see grid log presents pattern receiving weights 
hidden unit center values 
center ones single plot receptive field 
hidden unit positive values red going 
maximum yellow indicating center center 
excitation vice versa negative values blue going 
maximum negative magnitude receptive fields 
hidden unit correspond 
hidden units network verify look same 
units examined upper left 
grid log see same features described 
keeping mind grid log represents difference 
center center values clearly see 
topographic nature receptive fields full 
range different receptive field properties 

dimensions receptive fields different 
hidden units vary 

observe topographic organization different 
features causes neighboring units share value 
least dimension similar least dimension keep 
mind units far right 
similar far left etc 
observe range different values represented 
dimension space possible values combinations 
values reasonably well covered 

interesting aspects 
representations network well neurons 
orientation receptive fields 
systematically degrees topographic space 
units seen starting hidden unit located 
top column number noting orientation 
units circle units wide 
surrounding units circle 
orientations seen consequence 
varying neighborhood relationships unit similar 
distinct orientation coding expect 
proceed circle goes further away 
unit middle represents 
relate neighboring unit 
values occur relatively 
changes orientation mapping short distance hidden 
unit topographic space phenomena found 
topographic representations real neurons provide 
important source data models account 
order fully replicate real system 

directly examine weights simulated neurons 
model possible biological system 
indirect measures taken order map receptive field 
properties neurons commonly measure 
activation neurons response simple visual stimuli 
vary critical dimensions oriented bars light 
replicate kind experiment model pressing 
environment containing events represents edge 
different orientation position 

explain sense probe stimuli represent edges 
reference relationship 
patterns 

control panel 
observe resulting hidden unit activations 
compare based weight receptive fields shown grid log 
described displayed here select 
network window press 
panel continue next 
events note relationship case based weight 
receptive fields shown grid log 

groups hidden units activated probe events 
correspond well based weight receptive fields explain 

interested new patterns probe 
events present same procedure described 
particular interesting see network responds 
multiple edges present single input event 

finally order see lateral connectivity responsible 
developing topographic representations load set 
receptive fields generated network trained 
topographic organization resulting receptive field grid log 
indicating strength lateral connectivity provided 
neighborhood constraints patterns look 
similar networks trained lateral 
connectivity 

receptive field plot suggests interaction 
topographic aspect representations nature 
individual receptive fields themselves look 
different case lateral interactions 
kinds interactions documented brain 
sense 
computationally lateral connectivity important 
effect response properties neurons 
responsible tuning receptive fields place 
via cpca hebbian learning rule 

model illustrates hebbian learning develop 
representations capture important statistical correlations 
exist individual elements edges exist 
perceptual environment resulting representations capture 
important properties actual receptive fields meaning 
model provide computational explanation 
properties arise brain 

representations provide building blocks 
elaborate representations illustrated next 
simulation likely similar principles apply 
learning sensory pathways 

next model visual form object 
recognition pathway range important issues 
model edge detector representations level 
builds way spatially invariant representations 
enable recognition objects regardless appear 
visual input space range different sizes 
characteristic processing pathway 
brain transformations performed collapse different 
spatial locations sizes distinctions 
different objects forms invariance location 
size simulated here likely same mechanisms 
lead least degree invariance well note 
clear extent brain exhibits 
invariance representations 

important challenges spatially invariant object 
recognition problem binding problem discussed 
problem arises recognizing 
object encode spatial relationship 
different features object particular 
edge right left hand side object 
same time collapsing overall spatial location object 
appears retina simply encoded feature 
completely separately spatially invariant fashion tried 
recognize objects basis resulting collection 
features spatial arrangement 
features relative objects 
same features different example 
clearly 
problem solved encoding limited 
combinations features way reflects spatial 
arrangement same time recognizing feature 
combinations range different spatial locations repeatedly 
performing type transformation levels processing 
ends spatially invariant representations 
encode spatial features 

model depends critically learning hierarchical series 
transformations produce increasingly complex terms 
object features spatially invariant representations 
general principle hierarchical representations likely 
important aspects cortical processing ability 
learn representations task model learning 
provides key demonstration general principle 
researchers suggested object recognition operates roughly 
hierarchical fashion existing models implement 
specific versions idea 

important difference present model 
model versions 
separate process increasingly complex 
representations increasingly invariant 
representations different stages processing 
training easier well 
properties visual system appears achieve 
increased complexity spatial invariance same 
stages processing contrast present model 
constrained way develops aspects 
representation simultaneously 

model demonstrates hierarchical sequence 
transformations work effectively novel inputs 
generalization see new object learned 
relatively rapidly small set retinal positions sizes 
recognized large percentage time further 
learning positions sizes accomplished 
performing spatial invariance transformation common 
structural features shared objects higher 
levels network contain spatially invariant representations 
object features need associated 
unique combinations define particular objects 
assumes possibly large set underlying structural 
regularities shared objects ensure case 
model likely true objects real 
world seen composed different 
component shapes etc 
particular exists component shapes 
exactly 
structural regularities learning automatically 
find model 

general approach taken towards object recognition 
problem here consistent features actual visual 
system results effective solution 
size invariance ways people 
thought solving problem example 
suggested dynamically object recognition 
system spatial transformation system effectively 
object back 
position size orientation representation 
easily recognized simple pattern recognition system 
similar ideas suggested different 
general approach 
well supported known properties visual system 
effective implementation 
kinds model ideas proposed 

computer solve object recognition 
problem kind gradual 
hierarchical parallel transformations brain well 
suited performing 

finally important limitation current model 
processes single object time see later 
section spatial based object representations interact 
multiple levels perception complex potentially 
visual displays containing multiple objects 

objects object recognition model 

environment experienced model contains objects composed 
combinations horizontal vertical lines 
regularity 
objects composed same basic features 
critical enabling network generalize novel objects 
described network trained objects 
range positions sizes last testing 
generalization training positions sizes 
testing recognition performance 

whereas previous model represented roughly single cortical 
hypercolumn model relatively wide cortical area 
hypercolumns previous case 
means model reduced terms number 
neurons again connectivity patterns determine effective 
scale model need represent large cortical scale 
requires additional structure far 
particularly respect way inhibition computed 
idea structure inhibition acts multiple scales 
stronger inhibition neurons relatively 
close single cortical hypercolumn 
significant inhibition communicated larger via longer 
range excitatory cortical connectivity projects onto local 
inhibitory interneurons represented model 
levels kwta inhibition units single 
hypercolumn amongst themselves kwta activity 
level same time units layer 
larger kwta activity level reflecting range longer 
inhibition level unit level 
inhibition maximum computed 
kwta computations 

aspect relatively large scale model 
connectivity patterns previous model assume 
units hypercolumn connected 
same inputs further assumption neighboring 
hypercolumns partially overlapping 
offset inputs due overlap 
coding input area multiple hypercolumns 
basically different hypercolumns process different information 

hypercolumn units processing different parts 
input basically same thing extracting 
features multiple locations sizes part 
input different objects appear 
speed learning significantly amount 
memory required implement network 
hypercolumn units share same set weights 
weight weight sharing 
hypercolumn layers model weight sharing allows 
hypercolumn benefit experiences 
hypercolumns learning time 

verify playing substantial role 
resulting performance network control network run 
weight sharing network took memory 
longer train expected resulting representations 
overall performance similar network weight 
sharing explore here sense 
hypercolumn experience same input patterns time 
develop roughly same kinds weight patterns 
hebbian learning undoubtedly important similarity 
weight shared separate weight networks 
tends reliably produce same weight patterns purely error 
driven backpropagation network run comparison perform 
similarly cases type network tends 
high level variance learned solutions 
hypercolumns different things led 
worse performance terms spatial invariance properties 
resulting network 

network seen extension previous 
input lgn separate center layers 
represented objects bars light pixel wide 
showed bars activity center lgn input 
center input activity ends bar representing 
light end 
information important widely thought neurons 
early visual cortex kind information represent lines 
particular length example included 
center activity length bar 
redundant center representation 
required kinds bars center 
right center left lgn 
units previous simulation 
right unit left same 
bottom top bar features pixels long 
depending size object sizes 
corresponding pixel length bars lower left 
hand side objects located 
grid total different unique locations 
combined different unique images 
object 

previous model area processes lgn input 
simple oriented detector edge representations 
already demonstrated kinds representations develop 
response natural images want 
number units extra units 
required organizing self learning develop properly 
discussed simply fixed 
representations encode horizontal vertical 
lines possible locations receptive field 
trying combinations different 
center inputs set units encode bars 
center field encode bars center field 
receptive field size lgn input 
horizontal vertical bars center 
center total units hypercolumn 

next layer cortical object recognition pathway 
contains neurons larger complex 
receptive field properties assume units represent 
combinations edges larger 
receptive fields enable slightly spatially invariant 
representations encode same feature multiple far 
input locations next layer continues 
towards increasingly complex spatially invariant 
representations 

due limited size model representations 
visual input space produce fully invariant 
representations entire space larger realistic 
model cortex next layer 
processing temporal cortex fully 
invariant representations possible units 
able cover input space relative 
simplicity objects simulated environment enables 
representations sufficient complexity terms feature 
combinations distinguish amongst different objects 
effectively layers layer 

last layer network output layer enables 
based task driven error learning addition based model 
hebbian learning train network assumed correspond 
number possible task outputs example 
different objects different sounds 
network corresponding 
representation modalities feedback 
improve ability visual system identify 
different objects accurately predict 
similarly objects different physical consequences 
themselves etc 
serve digits letters case simply 
distinct output unit object network trained 
produce correct output unit image object 
presented input based task learning important 
successful learning network 

parameters network standard amount 
hebbian learning set connections 
lower amount hebbian learning here 
necessary due weight sharing same weights 
updated times input causing hebbian learning 
dominant learning rate 
epochs minimize interference 
effects learning likely brain 
exhibits similar kind slowing learning 

note simulation requires minimum 
run 

open project directory 
see network looks skeleton 
big network units connections project 
file build network moment 
skeleton important aspects network 
structure see lgn input layers 
see see 
layer grid structure grid 
elements represents hypercolumn units hypercolumn 
contain group units network built 
units connected same small region lgn 
inputs discussed neighboring groups connected 
overlapping regions lgn see clearly 
network connected addition connectivity groups 
organize inhibition layer described 
kwta level set units hypercolumn 
entire layer hypercolumns 
activity activity need distributed 
manner 

layer organized grid hypercolumns time 
size hypercolumn units again 
inhibition operates hypercolumn entire layer scales 
here units active hypercolumn entire 
layer hypercolumn units receives 
layer neighboring columns again overlapping 
receptive fields next layer represents single 
hypercolumn units units single inhibitory 
group receives entire layer finally output 
layer units different objects 

seen organization layers skeleton 
view lets build network press 
overall control panel see 
network units connected now 
switch click units layer 
see hypercolumn units receives 
input layers neighboring hypercolumns receive 
overlapping clicking units see 
connectivity patterns similar form larger 
size 

now lets see network trained back viewing 
input image shapes shown 
random location size pressing 
different input patterns look 

takes network trained 
load weights trained network network trained 
epochs object inputs epoch object 
took roughly epochs 
object performance approach 
corresponds network seen object location 
size locations sizes object objects 
assuming perfect distribution 
case clear considerable amount 
generalization training due weight 
sharing weight sharing help 
size invariance example 

network relatively large actual weight 
values themselves file 
weights network window 
menu left hand side selecting now 
back control panel press 
trained network perform task see 
plus minus phase output states same meaning 
correctly recognizing objects presented record 
performance text log window lower 
left screen column shows error 
pattern see error column 
errors associated 
sizes low end resolution 
network feature detectors 
correspond objects real world 

particularly informative 
patterns activity layers network response 
different inputs further units 
directly connected input view weights 
easily see representing important 
technique activation based 
receptive field hidden units network 

activation based receptive field shows units activity 
correlated layers activity patterns measured 
large sample patterns example want know 
units activity lgn patterns activity 
unit reliably responds input pattern resulting 
activation based receptive field input pattern 
unit responds equally set input patterns result 
average patterns corresponding 
mathematical expression receptive field 
corresponding lgn unit example 
activation unit receptive field 
computing unit example 
activation unit layer computing 
receptive field lgn example index 
input patterns usual note similar 
cpca hebbian learning rule computes saw previous 
simulation lgn example 
activation based receptive field procedure compute weights 
receptive field values layers unit directly 
connected example useful look units 
activation based receptive field averaging output images 
see object unit representing 

now lets take look activation based receptive fields 
different layers network press 
overall control panel select press 
file selection window input receptive field 
units center lgn layer file selection window 
appear move examine 
current display 

weight sharing need look 
hypercolumns worth units displayed 
scale large grid window grid 
elements grid representing weighted average 
input patterns activation based receptive field 
unit note units lower left hand 
hypercolumn layer receive corresponding 
lower left hand region input receptive fields 
emphasize region 

notice units looking receptive fields 
appear represent 
orientations positions tuned 
explain level corresponds terms 
activation based receptive field computed 

taking advantage answer previous question describe 
characteristics receptive fields observe here 
terms selectivity particular input features 
particular kind evidence see conjunctive 
representations bind different features 
relevant different objects 
see evidence level spatial invariance 
single units respond features range different 
positions explain characteristics 
receptive fields overall computation performed 
network 

press file selection window bring next 
receptive field display again put next 
moment shows input receptive fields center 
lgn input layer observe clear 
receptive field patterns center inputs 
indicating neurons encoded linear 
represented center fields lines 
end represented center fields 

now press next file selection window bring 
next receptive field display shows output layer 
receptive fields same units enables see 
objects units participate representing notice 
appear correlation input output 
selectivity units highly selective input coding 
representation objects versa vice 
expected highly selective input tuning units highly 
selective objects represent fact 
representing shared features objects participate 
representation multiple objects 

images objects shown 
same configuration 
output units explain units particular 
output representation based features shown input 
receptive fields hint pick unit particularly selective 
specific input patterns specific output units 
things easier see 

press bring next window showing input 
receptive fields units sure notice scale 
shown bottom window tells large 
maximum values window 

describe receptive fields indicate selectivity 
units specific input patterns terms spatial 
invariance property units 

finally press view output receptive 
fields units again sure notice scale shown 
bottom window want manipulate scale 
new numbers pressing control buttons bottom 
right match scale grid log receptive fields 

entire objects parts different objects explain 
answer question now compare 
relative selectivity units particular output 
units objects compared units units 
selective explain hint 
determine number objects unit 
participate tell relative 
complexity input features units selective 
different layers note complex 
representation objects complex 
combination features 

probe stimuli test response properties 
units model 

perspective units encoding 
visual inputs probe stimulus technique observe 
units responses previous simulation 
present case record sum units responses 
display grid log based activation receptive 
fields examined 

press overall control panel select 
press grid log window appear 
file selection window appear again move 
examine current display log displays 
responses hypercolumn units different probe 
stimuli consists adjacent lines lines 
object space view 
responses display presented 
possible locations lower left set 
lgn locations plotted units responses 
probe grid shown display 
position probe corresponding shown 
responses 
locations lower left hand probe figure shown 
lower left cells overall grid upper 
right probe figure upper right cells 
overall grid unit 
probe locations highly 
spatially invariant representation show display 
solid yellow active color corresponding 
grid 

subset features compared significantly 
respond least activity 
conclude selectivity units 
number different locations unit active 
area pixels display say 
units least degree spatial invariance 
results correspond based 
activation based receptive fields examined previously 

press next file selection window bring 
probe receptive fields units 

range different feature levels 
spatial invariance indicated figure 
specifically units respond feature 
locations single yellow 
unit respond probe stimulus tend 
relatively spatially invariant probe stimuli seem 
units explain units 
sensitive terms features present probe 
stimuli see 
explain response properties unit appears 
time 

similar way testing representations network 
actual object stimuli systematically 
positions input different sizes record 
statistics resulting activity patterns important 
statistic number different unique patterns occur 
layers different positions object 
recall position lower hand left 
object different sizes 
average different patterns compared 
different object images conclude 
time representation fully invariant object 
interestingly true novel objects 
network seen critical 
generalization test described detailed report 
analysis found file 
simulation directory 
results object item size object size 
spatial errors correlations 
subsequent representations number unique activation 
patterns 

techniques obtained insight 
way network performs spatially invariant object recognition 
particular appears build invariance gradually 
multiple levels processing similarly complexity 
representations increases increasing levels 
hierarchy simultaneously stages 
multiple levels network able recognize objects 
environment depends critically detailed spatial 
arrangement features apparently 
binding problem described previously 

addition receptive field measures 
networks performance perform behavioral test ability 
generalize spatially invariant manner objects 
numbers 
presented network training now train 
objects restricted set spatial locations sizes 
assess networks ability respond items novel 
locations sizes indication work 
test showed network produced 
spatially invariant responses layer novel stimuli 
presumably network needs learn 
association representations appropriate 
output units good generalization result 
spatial locations 

addition presenting novel objects training 
present familiar objects otherwise network 
catastrophic interference see 
discussion 
issue following procedure trial 
chance novel object presented 
chance familiar presented novel object 
presented location chosen random grid 
center visual field possible locations 
total roughly locations size 
chosen random pixels 
possible sizes sizes familiar object 
presented size position chosen completely random 
procedure repeated epochs 
objects epoch learning rate 
network getting novel objects correct epochs 
longer training new knowledge well 
testing 

testing performed analysis training 
new objects detailed results contained file 
attention column shows errors 
function total item object overall 
results object roughly errors testing 
possible locations sizes object 
considering trained possible 
input images good generalization result note 
evidence interference training 
objects observed detail comparing 
file 

looking specifically size generalization performance object 
generalized novel sizes show sizes 
results file level errors 
different trained sizes similar results 
object errors novel sizes 
compared trained ones good 
evidence network able generalize learning 
set sizes recognizing objects different sizes 
seen 

determine learning primarily occurred examine 
difference pre generalization training 
weights load weight differences network 
viewed clicking 
click objects output layer see 
magnitude weight changes units 
now compare output units 
layers lower network clear 
primary learning occurred evidence 
weight change units accounts 
observed interference kind interference 
learning constantly weights 

important general result test demonstrates 
hierarchical series representations operate effectively 
novel stimuli long structural features 
common familiar objects present case novel 
objects built same line features objects 
network learned represent terms increasingly 
complex conjunctions increasingly spatially invariant 

role driven error hebbian learning missing properties 
size missing dimensions color depth 
etc need generalization training prevent 
interference items 

described attention plays 
important role solving binding problem restricting focus 
processing related set features object 
enables resulting pattern distributed representations 
sense apply features actually 
related opposed random combinations features 
different objects aspects environment 
problem previous model presenting 
simulation object time 

object multiple binding problem different 
feature multiple binding problem previous 
model multiple objects necessarily want form 
conjunctive representations encode multiple objects 
simultaneously objects completely 
possible objects commonly seen 
eyes face network same 
kind hierarchical conjunctions feature spatial invariance 
solution multiple object binding problem 
feature multiple 

section next develop closely related 
models attention end result 
model extension object recognition model 
network able multiple objects 
environment sequentially focusing attention 
turn focus case attentional effects 
object recognition specifically role spatial representations 
controlling attention principles apply generally 
processing pathways begin 
general issues regarding nature role attention 
proceed explore models 

need emphasize point 
attention separate mechanism 
framework emergent property activation 
dynamics representational structure network 
particular inhibition responsible limitation 
total amount activity set representations 
layer constraint satisfaction operating network 
representations active context 
emergent property 
convenient refer attention mechanism 

addition thought distinct mechanism attention 
associated types mediated spatially 
effects modeling here mechanisms 
underlie attention cortex same 
kinds issues discussed here context object recognition 
apply generally well see following models 
particularly special mechanisms 
representations model attentional 
spatial object processing pathway 
attentional contribution well 

attention obviously represents limitation processing 
providing important functional benefit 
enabling system take advantage powerful distributed 
representations confused limitations 
course possible brain 
large number redundant specialized processing pathways 
own system distributed representations 
things processed parallel limitations 
imposed attention issue actual 
benefit processing objects time ultimately 
resulting representations fashion 
ongoing processing perform specific task 
sense focus sequentially individual objects 
particular ongoing processing activation 
common object processing pathway directly interact 
representations aspects processing 
object pathways 
relevant particular case 
learn appropriate set weights processing pathway 
capable appropriate ways 
processing pathways 

words assuming objects likely 
relevant time well processing 
object time place possible 
imagine tasks objects simultaneously relevant 
generally seem case objects 
relevant solving task example large 
number objects pieces potentially relevant 
considered properly 
relationships effects pieces ones overall 
etc cases specific properties 
individual objects relevant useful consider 
objects same time otherwise 
important exception general 
point case objects discussed 

functional benefit attention 
fact need act coherent focused manner 
order effectively solve tasks important 
survival attention forces 
processing lead inevitably limited 
resources time energy etc end basic 
tradeoff parallel processing sequential 
processing different advantages 
case brain appropriate 
balance forms processing way 
benefits 

models follow see structural principles 
developed specialized 
processing pathways lateral interactions critical 
determining ways functional 
resolved specific case interactions spatial 
object representations specifically parallel processing 
takes place lower levels system higher levels 
greater focus single objects locations space 

representations space object 
interact via level low spatially feature 

simpler model begin explore ways 
spatial representations interact kinds based object 
processing developed previous model parietal lobe 
cortex contains different types spatial 
representations lesions lobe cause deficits spatial 
processing discussed 
assumptions nature functions parietal 
representations basic spatial properties 

fundamental assumption underlying beneficial role spatial 
representations object processing objects tend 
spatially useful group 
focus processing upon subsets features spatial 
region simple idea spatial 
object representations interact via top effects 
spatial feature map provides inputs 
subsequent levels processing 
important problem idea positive feedback 
characteristic spatial representations focus top 
attention particular spatial locations 
ability activity coming spatial 
locations attention new 
location system able switch attention 
new location top projections spatial system 
relatively weak weak resulting level 
spatial attention incapable sufficiently focusing 
object processing pathway object 

spatial representations attentional 
effect via direct connections object processing system 
spatially organized lower levels 

alternative model explore here shown 
includes lateral interconnectivity 
spatial pathway lower spatially organized levels 
object processing pathway areas object 
model described previous section emphasized 
figure influences spatial processing object processing 
stronger opposite direction bottom 
pathway stronger top efficacy 
spatial modulation object processing limited need 
keep spatial system sensitive bottom input model 
include multiple spatial scales processing object 
pathway case previous model 
spatial pathway 

important note model includes top projections 
object pathway lateral projections spatial 
system object pathway allow 
possibility based object attentional effects 
considerable debate field exact nature 
based object attention essentially attention 
spatial region object located specifically 
attention particular features object 
model allows potentially complex interactions 
pathways processing rise 
interesting effects explore issue 
model well developed object pathway 

posner spatial attention task cue 

attention region space reaction times detect 
target faster cue valid target appears 
same region invalid 

simplest influential studying 
spatial attention posner task 
attention region 
space box side display 
subsequently affects speed target detection object 
recognition 
attention same region target 
subsequently appears subjects faster detect target 
attention opposite region 
interaction spatial object processing 
captured general framework discussed shown 
specifically activation 
spatial processing pathway processing objects 
appear part space processing 
objects parts space time competing 
virtue inhibition activating corresponding feature 
representations object pathway observe dynamics 
model todo positive feedback model capture 

model effects lesions parietal cortex 
performance posner spatial task spatial 
processing effects object recognition generally 
argued lesions 
parietal cortex lead specific deficit ability 
disengage attention particular spatial location 
showed apparent disengage 
deficits accounted terms model spatial attention 
common explore following 
explore effects 
parietal lesions associated 
posner task note 
model form shown 
account 
specific parameter needed work model 
provide robust richer 
basic points virtue lateral interactions 
spatial object processing pathways discussed shown 
modeling exercise 
provides example based biologically 
principles understanding cognition rise different 
functional cognitive phenomena 
specific disengage mechanism model 
argued basis traditional box 
model thing 

model spatial attention effects parietal lesions 
generally consistent 
associated parietal damage typically 
right patients lesions show 
side visual space reference 
seen address 
model account 
terms damage side representations 
visual space difficult patients 
focus attention damaged side 

start opening project lets step 
network structure see network basically 
mutually 
interconnected pathways 
layer contains spatially feature 
simple case assuming object represented 
single distinct feature organized 
single spatial dimension row units represents 
objects feature serves cue stimulus 
different locations row represents objects 
feature serves target same locations 

object processing pathway sequence increasingly 
spatially invariant layers representations unit 
collapsing adjacent spatial locations layer 
spatial processing pathway similarly represents adjacent spatial 
locations object pathway sensitive 
particular features units location spatial 
pathway todo 
represent distributed representations likely 
present brain useful 
effects partial damage pathway select 
network window click object spatial units see 
function via connectivity patterns note output 
layer taken level object pathway 
reaction time detect objects 
settling whenever target output object gets 
activity happen settling stops 
cycles 

locate overall control panel contains 
number important parameters 
values determine relative strength pathways 
network pathways default strength 
relatively weakly activated visual inputs compared 
top projections object spatial pathways allowing 
dominated spatial object attention 
spatial system bottom inputs 
influences object pathway relatively strongly compared 
relatively zero non impact spatial pathway 
specialized pathways influence lower layers note 
apply projections spatial system 
back apply top projections 
object system taken 
spatial system dominant activation object 
pathway inputs parameters show 
relatively slow settling adding noise 
processing simulate subject performance 

now lets see network responds perception multiple 
objects presented simultaneously function spatial 
location objects provide introduction 
kinds interactions spatial object processing 
happen relatively simple model lets begin viewing 
events present network press 
overall control panel select 
event different objects features present different 
spatial locations note target object slightly higher 
activation result 
reliable selection object next event 
same objects presented different locations 
finally last event different objects same 
spatial location 

imagine events look actual 
visual display complex objects different 
letters events terms relative 
speed recognizing target object 
answer 

now lets test model locate 
followed present event 
network settling updating networks 
activations cycle time target units activation 
exceeds threshold remaining 
events graph log lower right shows settling times 
event todo settling time represents 
settling times match 

terms interactions excitation inhibition 
flows network run batch runs 
pressing panel report 
resulting average settling times text log 
immediately left graph log 
original results 

observed spatial representations facilitate 
processing objects attention object 
key contrast objects 
same location spatial attention longer separate 
leaving object pathway try process objects 
simultaneously see complex realistic example 
effect next model 

now lets see model posner spatial task 
overall control panel select 
task represented groups events 
shown here correspond cue cue 
case presentation target object 
left location valid case event cue 
presented left followed target event target 
left activations 
primary groups events opposed 
list separate events todo specify 
finally invalid case same cue event target event 
target showing right opposite side space 

again 
process control panel note network responds 
conditions posner task continue 
cases press 
control panel run batch runs 

subsequent processing target valid invalid cases 

typical reaction times human subjects task roughly 
valid invalid appears 
difference side 
effects attentional focus general 
pattern results obtained fit data 
precisely add constant offset roughly 
correspond factors task 
included simulation course relationship 
cycle processing human reaction time 
automatic simply 
particular experiment 

now lets explore effects 
parameters networks performance try reducing 

based comparison results default network 
effects spatial processing object processing 

set back 

reduce value 
turn network display back 
invalid target case explain results 

set back continuing 
manipulate parameter controls strength 
input determine role top modulation 
activations via spatial system fixed strength 
increase value 
effectively reducing influence top projections set 
find longer associated invalid trials 
reduced indicating attentional modulation 
important contribution overall behavior 
model todo important 
set back continuing 

additional manipulation visual 
distance cue target presented closer 
expect attentional effect 
tested control panel 
selecting case 
see overlapping set spatial representations 
activated run 
effect invalid case relative case 
strong valid trials switch environment back 
continuing 

mentioned earlier showed patients 
lesions parietal cortex 
exhibit performance invalid trials 
posner spatial task specifically 
cue presented side space processed intact 
lesion target 
processed lesioned patients showed 
difference valid invalid cases 
roughly compared control subjects 
showed roughly invalid valid difference valid 

model data need change mapping cycles 
settling model reaction time patients generally 
exhibit overall slowing reaction times due patients 
generalized effects damage addition 
specific effects recall cycles settling 
model normal subjects 
reaction times add 
normal scale 
cycles settling model control subjects 
valid divided normal valid 
models results 
example found cycles difference 
valid invalid trials scaled 
appropriately controls provides good match 
observed data 

apply scaling procedure patients data 
results fit well take valid 
patients divide same 
invalid valid difference 
factor significantly smaller 
invalid valid difference patients 
patients additional slowing ability disengage 
attention region space accounted 
overall reaction times 

attempt replicate additional slowing similarly 
lesioning spatial processing pathway network 
resulting reaction times posner task press 
control panel select 
lesion levels spatial representations 
location right space select 
units back units right 
weights 

resulting scale invalid valid difference 
overall slowing 
patients responses compare patients data 
intact models performance turn network 
display back control panel explain 
lesioned model invalid trials terms 
activation dynamics network 

found replicate apparent disengage 
deficit specific mechanism 
determine importance presenting target lesioned side 
space run posner task locations cue 
target clicking 

compare previous lesioned results intact 
network explain 

interesting additional lesion data comes patients 
parietal lesions 
called interestingly exhibit 
level attentional effects posner task 
smaller invalid valid difference emphasized 
data provides important argument 
disengage explanation parietal function 
posner predict 
problems invalid trials naturally type 
model weve exploring simulate 

lesioned network explain 
results differ lesioned 
network 

finally explore effects lesion 
parietal spatial representations provide better model 
known typically referred 
described results 
lesions parietal cortex right 
cause patients generally lesioned 
side space left side space due 
visual information cortex simulate 
similar lesion 
lesion 
units location specifically 

now back case 
object lesioned side space network 
attention intact side specifically 
lesioned side case causes 
network activate cue object representation 
network settling resulting full cycles settling 
now choose 
again see similar phenomenon 
network completely incapable switching attention 
damaged side space order detect target again 
resulting full cycles settling invalid case 
interestingly case 
attentional effects completely settling 
times same conditions roughly cycles 
case network incapable processing stimuli 
damaged side space competing 
stimulus good side space easily 
explain general tendency 
rare actually competing stimuli 
relatively weak competition coming intact 
side space attention focused intact 
side 

smaller level damage produces disengage deficit 
posner task closely associated phenomenon 
parietal 
lesions shows relatively strong competing 
visual stimulus presented good side space cue 
invalid trials posner task model able 
account wide range different spatial processing deficits 
associated parietal damage depending 
location damage 

object based spatial modulation side object etc 
emphasize role inhibition 

simple model explored provides useful means 
understanding basic principles spatial attention 
interacts object processing address 
main spatial attention 
place avoid binding problem 
multiple objects next simulation basically 
previous models full object 
recognition model simple spatial attention 
model result model spatial attention 
restrict object processing pathway object time 
enabling successfully perform environment containing 
multiple objects essential ideas 
covered discussion previous models proceed 
directly exploration model 

note model currently inconsistent previous 
object recognition model updated model 
principles discussed apply new model 

open project notice 
network comes essentially same object 
recognition model addition spatial processing 
layers layers 
interconnected respectively amongst 
themselves providing interactions rise spatially 
mediated attentional effects 
network window menu 
left hand side select 

weights network trained objects 
presented main reasons done due 
involved providing feedback 
training signals based attentional selection imposed 
network itself sure feedback 
name object appropriate object 
network actually selected done spatial 
representations training signal 
equivalent 
plausible ways implementing kind training 
results likely similar produced 
network trained single objects 

weights select network 
window examine connectivity patterns spatial layers 
similar spatial extent corresponding object 
pathway representations unit unique 
location spatial system information 
location needs represented todo specify contrast 

now simply test network patterns composed 
objects random locations note objects smaller 
size previous object model necessary 
fit multiple objects display locate 
process control panel followed 
see input pattern presented network 
activations updated cycles settling notice 
updates representations objects 
activated features 
objects remain active result activation coming back 
spatial pathways supporting 
object specific object gets selected depends 
randomly level activation corresponding 
features object pathway 

particular case network object 
lower left hand object number 
note spatial locations 
represented upper left hand regions 
represent inputs lower left hand 
areas active case 
activation features object recognized 
object processing pathway robust amount 
interference appears network performs better 
allowing bit activation spatial 
constraint stronger 
weights spatial system object system allowing 
spatial units active 

note evidence based object attention 
activity pattern well case entire set 
features location activated result focusing 
spatial attention location todo 
case difficult see network 
capable determining object present 
location seems clear successful network performance 
requires spatial attention essentially 
role favors activations directly 
activate features itself 

grid log right network shows output pattern 
produced end settling left grid allows 
compare target output pattern right grid 
actual objects present input pattern 
network said performed correctly unit 
activated present target standard sum 
squared error grid log 
appropriate error measure case 
column computes error 
unit target pattern otherwise 
provides better measure actual network performance 

continue 
patterns environment notice network typically 
hard time patterns directly overlapping occurs 
frequency gets right 
hard correctly sense 
due limitations ability network obtain 
perfect recognition training network errors 
single patterns due fact 
spatial system present training 

continue patterns text log 
right report total count statistics 
roughly events todo greater 
total 
relatively large percentage errors due overlapping 
stimuli network actually performing reasonably well 

order determine spatial processing pathway 
contributing performance set 
parameter reduce impact 
spatial input meaning network 
incapable focusing individual objects 
again see object pathway gets 
activation objects produces 
output correspond objects presented 
presumably network confused features 
different objects unable bind 
single object set back 
continuing 

spatial attention useful enabling single object 
processing pathway focus single object object 
environment useful attention 
sequentially different objects present scene 
implement simple version kind attention switching 
taking advantage accommodation properties neurons 
described object 
processed network currently active neurons 
due accommodation new set neurons 
activated presumably new neurons represent 
object present environment 

press button control panel click 
button turn accommodation 
channels let potassium ions unit active 
period time switch new environment 
events overlapping non objects 
extend settling time activation dynamics time 
play now press process 
control panel see same event saw 
previously notice correctly recognized 
object lower left focus activation 
object lower right correctly recognized 
point network goes 
correctly recognize object 
performance subsequent events far perfect 
clear network least partially successful switching 
attention different objects 

likely better performance kind task require 
explicit training control system provides top 
activation spatial representations order direct attention 
towards different objects kind accommodation 
mechanism allowing attention 
further provides useful demonstration kinds 
looking dynamics emerge fairly 
simple mechanisms see far model 
taken order account phenomena 
visual search 
see 
simple model phenomena key properties 
common weve exploring 

present treatment 
different attentional object recognition phenomena 
same computational principles models described 
chapter 

memory defined effect 
experience general forms take neural 
network activity based activation memory 
changes weights based weight memory 
relationship learning memory weight changes 
clearly equivalent learning network ability 
maintain activation states learning 
explore ideas relatively generic leabra 
network provides reasonable model general memory 
cortex slow learning information 
integrating different experiences learning 
thought taking place cortex activation 
similarly cortex well memory general 
highly distributed take different forms 
different representations cortex find 
useful general distinguish cortical memory 
systems 

see basic model suffers important 
limitations level interference subsequent 
learning ability maintain activity 
absence input limitations result 
basic tradeoffs discussed 
generic cortical model specialized systems 
specialized system rapid learning arbitrary information 
hippocampus related structures hcmp 
specialized system robust rapidly maintenance 
activations prefrontal cortex pfc 

further useful distinguish set effects 
experience generic cortical system relatively short term 
activation results form short term 
priming items processed rapidly immediately 
similar items addition relatively small weight 
changes resulting processing item rise 
long term priming effects last long time period due 
based weight nature term longer effects learning 
experiences leads memories 
characterized semantic memory 
memory mediated types memory 
mediated hcmp memories characterized 

nature define nature hcmp 
memory according underlying mechanisms enable hcmp 
system learn rapidly interference 
specifically sparse conjunctive representations 
finally mediated pfc active memories constitute 
memory characterized representations 
internal context 

models previous chapter provide examples slow 
learning gradually new knowledge 
cortical processing pathways resulting semantic 
memory object recognition model learning 
features different objects 
world addition gradual process effects 
take time see following 
exploration slow learning 
immediately effects example small weight 
changes properties existing representations 
ways lead weight based long term 
priming try same system rapidly 
learn novel information find rapid learning 
distributed representations causes interference 
see subsequent exploration need 
separate specialized system achieving rapid learning 
overlap different representations model 
system hippocampus presented 

main behavioral studying priming 
completion subjects study list words 
asked simply come words complete initial word 
words constructed 
possible increased probability 
coming studied word relative control subjects 
words initially taken 
indication residual effects processing word 
words initial studying word 
subsequent processing word importantly turns 
form priming intact normal levels patients 
hcmp lesions indicating 
effects single presentation word mediated 
intact cortical system results surface appear 
challenge idea cortex learns slowly 
experiences see easily accounted 
framework 

way completion task modeling 
purposes mapping input 
different possible words related 
priming explicit 
different spelling task 
subjects 
name 
asked 
critical word word read note 
words pronounced same thing actually 
said input ambiguous pronunciation output 
possible 

model trained associate different output patterns 
input pattern simplicity random 
distributed patterns input output patterns initial 
period slow training allows network appropriate 
associations simulate subjects prior experience 
results relevant knowledge 
trial network produces 
outputs response input pattern training followed 
testing phase network presented particular 
association input tested see word 
produce input see single trial 
learning same learning rate network 
information initially results strong bias towards 
producing output 

now lets explore model open 
directory notice network 
standard layer structure input presented bottom 
output produced top press 
events shown event bottom pattern represents 
input top represents output able 
tell set events set events 
same set input patterns different output patterns 
events reflect event random 
input pattern number corresponding output pattern 
labeled event 
same input pattern corresponding output pattern 
labeled total 
different input patterns total output input 
combinations events iconify environment window 

train network standard combination 
hebbian driven error learning turn network 
turn complete 
training 

graph log shows statistics training reber 
grammar network 
possible outputs input standard 
error measure target 
closest event statistic find event 
training environment closest similar target output 
pattern output pattern network actually produced 
minus phase statistic results 
distance closest event thresholded usual 
output exactly matches events 
environment name closest event 
appear graph log 
value appear testing log later 
event currently presented network otherwise 
think binary distance error measure computed 

sum closest event epoch training 
plotted yellow graph log network starts producing 
outputs exactly match valid outputs environment 
necessarily appropriate outputs input pattern 
approach zero 
statistic graph log shows plotted blue 
input name looks part event name 
input pattern portion name 
possible outputs approach zero 
network 

noted ability learn 
mapping task depends critically presence 
kwta inhibition network standard backpropagation 
networks learn produce blend output patterns 
learning produce output inhibition 
helps network choose output 
active same time inhibitory 
constraints mapping 
adding small amount noise membrane potentials 
units processing provides 
selection output produce finally seems hebbian 
learning important here network learns task better 
hebbian learning purely error driven manner hebbian 
learning help produce distinctive representations 
output cases virtue different correlations exist 
cases 

trained network appropriate semantic 
knowledge now assess performance 
priming task press control panel 
bring new process control panel running testing 
text display results 

perform different kinds tests assess 
biases exist respond input patterns 
outputs 
baseline responses done learning turned 
turn learning train network produce 
adjust weights done training 
see trial learning produce particular output 
substantial effect probability producing output 

lets baseline values learning 
determines learning 
point press control 
panel epoch events see 
column larger text log events 
presented sequential order outputs presented 
outputs testing 
now outputs actually presented plus phase 
run minus phase means list 
basically same same input 
patterns order event presentation 
sense turn training differences 
point due noise added unit membrane 
potentials 

columns pay attention larger text log 
event name actual output produced network described 
generally producing correct 
outputs critical issue 
response see fairly random set output 
responses determine times network produced 
output pattern corresponding particular target output 
current event look column 
refer simply time 
same name event produced same output pattern 
otherwise observe roughly time 
produces same name current event smaller log shows 
result column 
column monitor actual 
network producing correct outputs note 
statistic perfect errors 
magnitude 

explain target output 
actual output roughly 

now lets turn learning see obvious effects 
trial learning input pattern subsequent 
performance same input set 
event press again process control panel 
now learning event expect 
result seen output associated 
particular input pattern list events 
network likely produce output 
comes input pattern again 
baseline case observe systematic difference 
responses network patterns recall 
network trained produce output 
output log actually produced 
minus phase reflect effects prior 
training training produce 

producing output events 
time same inputs patterns 
affect statistic summary value 
now again report happens 
time events presented 
explain behavior relate priming results 
humans described 

seen simple model cortex slow learning 
develop useful distributed representations show 
effects single trials stimuli now see 
cortical model task rapidly learning arbitrary 
associations further explore particularly challenging 
case high level overlap associations 
learned case valid 
required remember information similar things 
ones opposed previous 

task studied human subjects 
commonly known 
list learning task represents set words 
associated different sets words 
example word window associated word 
reason list window associated 
list studying list 
subjects tested appropriate 
associate words subjects study 
list multiple subsequently tested 
lists recall 
learning list subjects exhibit level 
interference initially learned associations result 
learning list remember reasonable percentage see 
data 

comparison human backpropagation network 
performance list learning task 

tried standard backpropagation 
network perform list learning task found 
network described catastrophic 
interference comparison typical human data 
networks performance shown 
human performance goes correct recall list 
immediately studying roughly learning 
list network immediately recall well 
list learned 

model explore here start 
catastrophic interference effect standard leabra network 
neural networks good 
models human cognition 
see significantly reduce interference 
important changes networks parameters 

original catastrophic interference finding 
amount subsequent research 

consistent basic idea interference 
results same units weights learn 
different associations weight changes learn association 
necessary learn obvious 
way avoid interference different units represent 
different associations previously 
solution distributed representations 
same units participate representation different 
items see set parameters 
result distributed representations case 
catastrophic interference set parameters 
result units active separated 
representations avoid interference 
benefits distributed representations 
explore model 
hippocampus related structures appears 
providing specialized memory system based sparse 
separated representations rapid learning 
interference 

basic framework implementing task 
input patterns represents stimulus 
represents list context assume 
subject develops internal representation 
different lists serves means 
produced input patterns 
hidden layer produces output pattern 
corresponding associate explore 
different representations word items 
distributed representation random bit patterns 
localist representation overlap different items 

begin open project directory 
lets look training environment press 
distributed patterns windows 
come show list lower left list upper right 
bottom patterns item list context upper 
pattern associate note item list 
same pattern item list 
true corresponding items 
lists items list share same 
context pattern items ahead iconify 
windows 

now lets see well network performs task locate 
see graph log updated network trained 
initially list list red line shows 
error number items produced 
units right side training set 
yellow line shows error testing list 
note red yellow lines start roughly correlated 
identical due fact testing 
occurs training weights different 
item presented epoch 

red line training error gets zero epochs pass 
getting zero network automatically 
training list see red line 
immediately new set training events 
results see yellow 
line immediately well indicating learning 
list prior learning 
list run batch subjects 
summary average statistics taken end 
list training subject appear text log 
lower left shows training error 
shows testing error list 

batch run simulated subjects results 
compare human data presented 
error list introduction list 

turn network 
hidden unit representations unit seems 
point active 
input pattern seems obviously problematic interference 
perspective list going activating 
same units reducing extent hidden 
unit representations distributed 
able encourage network separate 
representations learning lists items lets test 
idea reducing parameter 
units active time hidden layer result 
distributed representations 

activity report resulting average testing statistic 
describe effects 
manipulation number epochs takes network 
reach maximum error list introduction 
list results compare human data 
presented 

reason network performed well 
expected done encourage 
different sets units represent different 
way encourage increase variance 
initial random weights unit 
pattern responses encourage different units encode 
different change 
thing improve performance enhance 
contribution list context inputs relative stimulus 
list context different 
changing 
increases weight scaling context inputs 
imagine processing subject 
finally increased amounts hebbian learning 
contribute better performance perfect 
correlation items list associated list 
context representation emphasized hebbian 
learning lead different subsets hidden units 
representing items lists different 
context representations setting 

batch run new parameters testing 
shown basically best performance 
obtained network now good model human 
performance 

important dimension weve emphasized speed 
network learns clearly learning fast 
human subjects further appears manipulations 
weve improve interference performance 
longer training times observed cases 
maximum epochs reached 
learning items need play 
parameter see speed learning network keeping 
same optimal parameters set 
training run note training list stops epochs 
red line zero observed 
learning significantly faster list 
learned actually learning faster 
network learning item 
previous learning items evidence 
list interference greatly reduced 
order substantially increase rate learning words 
network appears want learning same kind 
slow integrative way try representations 
sparse separated slow learning rate necessary 
kind integrative learning meaning addition 
interference network fail learn 
fast human subjects 

weve seen improve performance moving 
away distributed overlapping representations towards 
separated representations seems simple architecture 
somehow incapable matching human performance 
task well see later hippocampal system 
specialized neural architecture properties appear 
particularly well suited role rapid 
arbitrary learning system 

particularly biologically 
plausible thing try network 
localist representations items context 
distributed ones pressing button 
control panel select 
active item context note switch 
localist representations switch simple kwta 
inhibition function output layer done 
localist layers due average unit 
based average kwta default distributed 
representations hidden layer continues based average 
kwta 

localist representations batch run 
training runs report average testing error 
better model human 
performance human explain 
localist patterns task easy network 
well press button control panel 
batch run report results 

understand function hippocampus related 
structures hcmp terms tradeoff standard 
cortical model learning new things rapidly requires 
different overlapping non sets units minimize 
interference need distributed 
overlapping representations represent underlying 
structure environment expect different 
specialized systems support different types learning 
memory considerable amount data indicates hcmp 
rapid learning system cortex slow learning 
form weve explored previous models 
section 
see model incorporates important biological properties 
hcmp perform rapid learning 
significantly interference distributed cortical 
network model based 

bidirectional connectivity hcmp 
specifically cortex wide range 
cortical areas adapted 

described hcmp top 
cortical hierarchy receiving wide range different types 
information cortical areas see 

hcmp receives inputs represent essentially entire 
cortical state time cortical representation 
current state environment order serve role 
memory system encode input pattern 
fraction original pattern original 
whole critically retrieval occurs initially 
hcmp spread back cortex resulting 
original cortical representation 

words hcmp cortical 
representations somehow particular 
memory detailed content memory 
cortex hcmp simply subsets 
cortical representations hcmp 
semantic information represented 
interconnectivity overlapping distributed representations 
cortex bind pieces semantic information 
isolated facts chance 
hcmp memories characterized 
memory associated particular 
example encode 
report details 
hcmp representation typically represents particular 
whereas cortex represents 
eventually associations cortical 
representations initially encoded hcmp 
learned cortex itself standard 
slow learning process detailed treatment set 
ideas issues see 

critical functional properties hcmp system 
summarized competing mechanisms pattern separation 
pattern completion pattern separation leads 
different relatively overlapping non representations hcmp 
different subsets units encode different memories 
interference different sets weights 
involved pattern completion enables partial trigger 
activation complete previously encoded memory 
pattern separation operates encoding new memories 
pattern completion operates retrieval existing 
memories 

pattern separation hcmp 
indicate representations composed patterns active units 
cortex overlapping relatively 
large proportion active units 
here hcmp sparse representations smaller lead 
overlap separation units hcmp 
conjunctive activated specific combinations 
activity cortex 

shows representations 
units active likely 
different subsets units represent different patterns 
pattern separation principle tried 
cortical model list learning 
relatively small effects 
right direction see hcmp sparseness 
scale larger 
effects 

way thinking sparseness produces pattern separation 
terms higher threshold units active 
come greater levels inhibition 
producing large inhibitory excitation 
overcome threshold input patterns 
overlap same unit receive activation 
patterns high threshold smaller 
words high threshold increased levels 
specialization representations particular input patterns 
think highly 
specialized due high threshold competition 
same basic principle evolution specialization 
competition effect described context 
hippocampus detailed mathematical treatment 
found 

effect sparseness individual units 
conjunctive meaning activation depends 
conjunction multiple input features units 
understood terms higher threshold activation 
unit relatively large number active inputs 
conjunction inputs threshold 
implicit notion individual units receive 
subset input units units 
active good subset 
receive input subset active subset 
connectivity increasing variance random 
initial weights list learning model saw 
bit free interference learning ideas sparse 
conjunctive representations pattern separation critical 
understanding functional role biological properties 
hcmp 

important understand need pattern completion 
pattern separation work 
previously stored information 
exactly same original input activation pattern hcmp 
new pattern separated version input 
recognizing retrieval cue existing memory 
order actually memories stored hippocampus 
mechanism pattern completion needed 

pattern completion mechanism takes partial input pattern 
subset stored memory missing parts 
asked chance 
input cue sufficient trigger completion full encoded 
memory enabling respond pattern completion 
particular properties hcmp system 
strong set lateral connections particular layer 
hcmp perform same kind pattern completion explored 

fundamental pattern separation pattern 
completion consider following event good starts 
story happened 
story 

hippocampus know information new memory 
keep separate pattern separation memories 
complete information existing memory 
story case 
hippocampus produce completely new activity pattern 
produce completely perfect 
memory presented exactly same way 
time problem obvious solution 
memories noisy inputs require call 
basically tradeoff operating hcmp itself 
pattern separation completion tradeoff 
actually understand features hippocampal 
biology 

hcmp model areas 
connectivity corresponding columns input 
example activity pattern note sparse 
activity intermediate sparseness 

shows diagram hcmp model 
contains basic anatomical regions hippocampal 
including fields 
including well 
cortex serves primary cortical output input 
pathway hippocampus output input area 
represented here likely play similar 
role greater emphasis subcortical 
motor representations describe layers pathways 
model summarize known biological properties 
contribute pattern separation completion functions 

model based 
model 

model provides framework functional properties 
memory mechanisms pattern separation learning 
synaptic modification pattern completion further 
mechanisms underlying anatomical properties 
hippocampal model basic 
computational structures hippocampus feedforward 
pathway area via important 
pattern separation pattern completion recurrent 
connectivity primarily important pattern 
completion model sparse random projections 
feedforward pathway 
strong inhibitory interactions form sparse 
random conjunctive representations emphasize 
importance region providing means 
separated representation back language 
necessary recall information happen forms 
representation 
pattern pattern rise 
place 

rough estimates size 
hippocampal areas expected activity levels 
corresponding values model data 

layer hcmp same basic excitatory 
inhibitory structure described cortex 
importantly layer inhibitory 
neurons form local feedback serve 
activity levels system 
simulate inhibition kwta inhibition function 
same kinds learning mechanisms work 
cortex hippocampus hippocampus 
research synaptic modification done 
cpca hebbian learning model task 
encoding information model 
driven error task learning operating 
hcmp 

patterns firing neurons 
cortical neurons 
highly distributed activity patterns 
contrast neurons highly sparse 
conjunctive firing specific locations 

rough sizes activity levels hippocampal layers 
shown note seems 
sparse level activity roughly 
times larger layers active 
output input layer model roughly 
scaled numbers units activations 
generally higher order obtain sufficient absolute numbers 
active units reasonable distributed representations 
additional evidence regarding sparse importantly 
conjunctive nature extent 
neurons example shows patterns 
neural firing neurons 
fire particular location particular 
direction explained neurons 
activated particular conjunctions sensory features 
present specific locations 

amount known detailed patterns connectivity 
hippocampal areas 
starting input structure 
topographic projections different cortical areas 

projections broad 
projection known pathway 
sparse focused topographic neuron receives 
synapses projection 
synapse widely significantly stronger 
inputs contrast cortex 
projections 
feedforward direct feedback regions 
known exist 

lateral recurrent projections project widely 
neuron receive large 
number inputs entire similarly 

connecting wide range 
finally interconnectivity relatively 
point point projections 

now put biological properties work model 
explaining encoding retrieval memories works terms 
areas projections general scheme encoding 
activation comes cortex flows 
pattern separated representation sparse 
distributed set units bound rapid hebbian 
learning recurrent plus learning 
feedforward pathway helps encode representation 
simultaneously activation flows 
pattern separated representation 
association representations 
encoded learning connections 

encoded information way retrieval partial 
input cue occur follows again representation 
partial cue based inputs cortex goes 
now prior learning feedforward pathway 
recurrent connections leads ability complete 
partial input cue original representation 
representation activates corresponding 
representation capable 
complete original representation input 
pattern novel weights 
particular activity pattern strongly 
driven activity pattern corresponds 
components previously studied 
conjunctive nature representations prevent successful 
recall 

addition performing pattern completion complete 
memory partial cue output hcmp presumably 
recognize items events etc new ones 
known recognition memory divided 
mechanism specific information 
item mechanism provides 
new items 
hcmp appears 
play central role component recognition 
model successful recall 
input probe pattern back 
hippocampus 

section further details model 
generally based biology shaped 
reasonably simple working model details 
details input representation 
mapping problem arises 
recall 

input representation incorporates topographic 
characteristics different cortical areas 
areas sub represented different slots 
thought representing different feature dimensions input 
color semantic features etc slots 
units slot unit slot active unit 
representing particular feature value input patterns 
constructed randomly selecting different feature 
values random subset slots 
distinct layers receives input cortical areas 
projects hippocampus 
receives projections projects back 
cortex deep representations 
layers different details assume 
equivalent same representations 

serve separated pattern 
representation back activation patterns pattern 
completion representations mentioned 
same time achieve amount pattern 
separation minimize interference learning 
mappings pattern separation explain 
hippocampus actually 
directly back input challenge implementing 
requires systematic 
mapping pattern separation requires 
highly linear non mapping done model training 
mapping pieces referred 
columns separated pattern representations 
entire representation composed systematically 
additional learning 
different combinations representations different 
columns column conjunctive pattern 
separated 

columns units entire composed 
columns column receives input adjacent 
slots units consistent relatively 
point point connectivity areas weights 
column trained taking column activity 
level training combination patterns 
slots conjunctive pattern 
separated representation patterns slots 
scheme units required 
column nonetheless consistent relatively 
greater humans relative hippocampal 
areas function cortical size further 
benefit certain combinations active units 
column correspond valid patterns allowing invalid 
combinations due interference 
imagine real system slow learning develops 
mappings columns separately time 

finally basic problem recall 
system needs able distinguish 
activation due item input directly 
via due activation coming recall 
solution problem 
suggested lesions 

phase respect drives 
point units otherwise 
providing means driven 
activation approximate mechanism 
simply turning inputs testing 
assess quality hippocampal recall comparing resulting 

note model requires least run 

exploration hcmp model same basic 
list learning 
standard cortical network hcmp able learn 
new causing levels 
interference original associations able 
rapidly possible cortical model 
least distributed representations here 

start open project 
hippocampal due ability model simulate 
wide range hippocampal phenomena directory 
shown large 
models stored skeleton needs built 
button overall control panel 

observe process activation 
network training locate control 
panel see 
particular input pattern training set presented 
network input pattern composed parts 
representing associate items representing 
list context source information list context slightly 
different item considerable overlap amongst 
context representations same list lists 
training parts presented testing 
associate pattern completion hcmp 
based partial cue stimulus 
context see activation flows 
layer pathway simultaneously 
sparse representation associated 
representation back 

end settling notice active units 
network selected remain selected 
settling next pattern allowing easily compare 
representations subsequent events 

observe relative amount pattern overlap subsequent 
events 
layers report general terms amount overlap 
layers explanation pattern 
separation mechanism relative levels activity 
different layers explain results 

epoch training consists training events followed 
testing sets testing events testing set contains 
list items contains list items 
contains set novel items sure 
network treating novel items network automatically 
testing pass training 
events smaller text log lower left display 
training events processed lets speed 
things bit turning display training events 
network select overall control panel 
train process 
monitor text log event presented 
number point network display 
again start updating test event 

network trained epochs 
detailed testing observe 
testing process point notice 
input pattern presented network missing middle portion 
corresponds associate 
activation network capable 
missing part pattern completion result 
activation back via 
processing stops epochs training testing 

now testing process detail 
control panel immediately 
right weve 
propagation network cycle cycle basis lets 
clear larger text log right side 
display contains record testing events 
multiple testing 
see testing event layer 
corresponds studied stimulus 
stimulus list context representation list 
studied likely network able 
complete pattern able see 
activation pattern gets 

network settling pattern compare 
activity pattern produced original 
stored activity pattern pressing button 
network window see target pattern 
comparison purposes layer 
active units selected relatively easily compare 
compare switching back forth 
act 

now look line testing text log critical 
columns here 
shows proportion units activated 
pattern shows proportion units 
activated active 
present pattern measures 
zero network correctly original 
pattern large indicates network 
otherwise different pattern 
relatively rare model 
large 
indicates network recall probe 
pattern common novel items 
last testing set see 

now look graph log right testing text log 
shows plotted 
axis axis 
event showing particular location extent 
lower left hand network 
accurately log automatically 
new testing set sure 
particular testing environment order code discrete responses 
network need set statistics 
following chosen 
provide reasonable performance 
say hcmp 
item shown column 
text log successful ignore 
columns point relevant simulation 

continue patterns sure 
understand relationship networks performance 
statistics turn back 
recall event continue step studied 
items tested again 
see field 
increments now presenting same 
stimulus list context 
items trained expect network 
successfully recall items learned 
next items network 
notice 
towards smaller values caused 
similarity studied items overlap 
next set testing events completely novel set items 
network high 
aspect items 

report total number responses testing 
text log testing 
respectively 

now trained tested resulting 
performance train network epochs 
examine testing log network 
automatically tested training epoch looking 
interference back control 
panel value field 
select 
process sure press 
effects prior training monitor 
training testing text network trained tested 

testing text log testing 

respectively epoch training find 
evidence interference learning testing results 
compare contrast performance hcmp model 
cortical model human data same 
basic task particular 
attention interference number epochs necessary 
learn 

found explorations biological 
properties hcmp result system particularly 
learning new information rapidly 
interference previously information 
relatively similar 

issues scaling capacity switching train test phenomena 
space conditioning 

terms based activation memory simple attentional model 
illustrates activation left 
processing cue stimulus affects subsequent processing target 
explore residual activation result 
activation based short term priming effect similar 
based weight lasting long residual 
activations useful form active memory 
longer periods time requires active 
maintenance mechanism keep activation going time 
see subsequent exploration recurrent excitatory 
connectivity provide mechanism turns 
distributed representations cause similar problem active 
memory system based weight time 
activation 
overlapping representations network required hold 
activation state support environment 
need separate specialized system 
achieving robust maintenance active memories model 
system pfc presented further 
elaborated 

next explore based activation properties 
standard cortical network saw model posner spatial 
task activation 
states affect subsequent processing explore similar 
effects priming simulation described behavioral 
data effect essentially same 
based weight priming case weight activation based 
changes result similar types primary 
difference based weight priming relatively lasting long 
based activation priming relatively 
model simply turning learning 
simulation priming effects due recent 
activation easily perform manipulation 
humans resort techniques 
way types priming 
prior immediately preceding experience builds 
weights favor response experience favors 
explore idea greater detail context 
phenomenon 

begin open project 
simulation weight priming assume 
already familiar network environment begin 
trained pre network network 
menu select 
configuration default lets test network 
different last time obtain baseline measure 
performance press 

test responses output 
presented allows determine 
impact seeing case response 
case presented output response pattern 
value case presented input 
pattern presented output response serves 
comparison pattern actual output produced network 
input observe network produces 
response trials time 
indication priming completely 
activations event presented roughly 
response reflects random biases trained network 

parameter controls extent 
activations reset event processed lets 
change activations 
completely intact trial next now 
again notice increased tendency network 
respond trials 

test trials explain 
effect performance 

now explore ability standard cortical network maintain 
active memories longer periods time absence 
supporting input presence inputs 
kind active maintenance useful keeping 
information immediately active form 
longer periods time residual activation 
explored previous section obvious neural network 
mechanism achieving active maintenance recurrent bidirectional 
excitatory connectivity activation constantly 
active units maintaining activation 

implement bidirectional excitatory connectivity 
context distributed representations find original 
activating input pattern removed maintenance pattern 
activation distributed 
representations resulting original information 
distributed representations again appear 
important form memory standard model cortex 
necessary distributed representations 
perform active maintenance same time tradeoff 
resolved prefrontal cortex pfc specialized 
robust self active memory discussed further 

related items terminal 
represented overlapping distributed fashion 
common features 

explore ideas simple overlapping distributed 
representations 
terminal terms features 
monitor shown 

open project see network 
hidden units representing features input 
units provide individual input corresponding hidden unit view 
connections hidden units enable 
maintain representations input pattern 
turned nature distributed representations 
unit needs able support unit different 
items involve different combinations features 
weights feature units 

press button grid log shows activity 
input hidden units event input 
present event input removed 
see features active input 
activates appropriate hidden units corresponding 
distributed representation terminal input 
subsequently removed activation remain 
features feature 
impossible determine item originally present 
spread occurs simply units interconnected 

problem weights exactly same 
connections likely true brain 
manipulate parameters 
control panel introduce different amounts variability 
recurrent weights see spread problem 
explain results 

obvious solution spread problem eliminate 
interconnectivity units isolated allow 
maintaining self add excitatory 
connection self unit press button 
control panel switch network type 
connectivity verify connectivity 
set back respectively 
press button 
network observe network now able maintain 
information difficulty now 
ability perform useful computations require 
knowledge features unit 
separated 

now lets active maintenance task challenging 
explore role recurrent connections noise 
change leak current reduce 
controls relative strength 
recurrent weights now network 
weaker recurrent weights case 
see longer capable 
information long todo elaborate 

enables network hold onto information 
time add noise membrane potentials units 
setting again multiple 
times network ever maintain information 
whole time find value increments 
time allows network maintain 
information repeated runs explain noise requires 
stronger recurrent weights 

section explore development interaction 
activation based weight memory previous sections focused 
forms memory separately here consider cases 
changes weights activity interact same 
system support competing responses competition 
arise weights 
response pathway network result repeated 
practice activity recent processing favors 
competing pathway addition exploring interactions 
model provides introduction important issues 
development neural network models provide important 
understanding experience genetic factors 
interact producing patterns changes take place 
develops 

studied 
extensively human infants provides 
good example competition activation based weight 
memory task subjects 
toy location typically 
allowed search object short delay 
procedure repeated subjects 
object new location following short delay human infants 
search error 
based weight memory support reaching 
activity dependent development prefrontal 
cortex pfc direct correct reaching consistent 
lesions prefrontal cortex 
performance task 

important note researchers 
tests infants knowledge 
require performance factors reaching demonstrate 
underlying object 
infants underlying fail tasks 
due performance limitations assumption researchers 
experiments demonstrate earlier 
example reduced production errors 
observed expectation 
looking reaching behaviors differ task looking 
response accurate infants errors 
task observe 

finally expectation task 
infants look longer toy hidden 
nonetheless search 

sensitivity objects new location 
commonly evidence infants know 
hidden task search due deficits 
external knowledge representations 
see understand findings terms competition 
activation based weight memory neural network 
framework knowledge 
deficits 

model explore ways activation 
based weight memory support competing responses task 
based weight memory implemented standard cpca hebbian learning 
occurs function activity units network 
activation based memory implemented recurrent connections 
representations network discussed 
style pfc active memory system previous section note 
original model exercises presented here 
simplification recurrent weights increased hand 
simulate effects development potential role 
experience shaping weights demonstrated 

input representations model based idea 
spatial location object identity processed separate 
pathways discussed 
pathway represents location hidden 
represents cover 
hidden hidden layer represents 
locations receives input object 
representations hidden layer recurrent self 
connections representation location represents 
pfc active memory system 

model output layers reaching expectation 
single difference frequency responses 
updating unit activity task 
expectation layer responds input reaching 
layer responds inputs corresponding stimulus 
reaching distance updating constraint output layers 
capture different frequencies infants reaching 
trial 
infants reach contrast prevents infants 
underlie longer looking impossible events 
trial similarly infants 
restricted experiments infants nonetheless 
reach show 
simulations responses change dynamic 
active resulting looking 
reaching 

networks initial connectivity includes bias respond 
appropriately location information look location 
presented infants appear enter 
experiments biases 

exploring model open project 
notice location units corresponding locations 
network cover input units corresponding 
default cover type different cover type 
toy units corresponding default toy different 
toy type 

now lets click observe connectivity 
input layers fully connected hidden layer 
hidden layer fully connected output layers 
see initial bias same locations 
strongly activating weights locations 
initial connection weight connections toy 
cover units relatively weak hidden output 
layers recurrent self excitatory connections back unit 
initially magnitude change 
improve networks ability maintain active representations 
starting relatively weak ones simulate 
active maintenance 

now examine events presented network 
click overall control 
panel types trials represented 
events see window trials pre corresponding 
practice trials provided start experiment 
infants reach trials trials 
trial types repeated multiple times events 
trial version running task 
consists trials pre trials trial 
trial consists corresponding 
trial follows 
place infants 
attention particular location 
infants attention location 
place 
presents place 
infants response reaching 
patterns activity presented input 
units corresponding visible aspects stimulus event 
input units activity levels input activity 
represent aspects stimulus 
aspects toy producing 
activity 

now lets run network easier tell going 
network looking grid log display 
viewing trial activation separately press 
button control panel 
training process 
network run entire experiment 
record activations weights grid log 
updated dynamically processing button 
turned 

simply specific points experiment 
buttons control panel press see 
trials pre main columns now 
column tells event presented next 
column shows 
activations network event columns show 
weights difficult interpret 
point well 

notice toy presented 
activated network looks location 
expectation layer hebbian learning taking place 
trial units active experience weight 
increases case increases network 
activate location representations 

press see testing trials 
networks tendency reach location 
note result hebbian learning hidden output 
units active here trials pre 

networks internal representations output reach 
responses delay choice trials observe 
network error explain network 
performing terms interactions weights 
learned prior experience trials recurrent 
activity representation 

now increase parameter default 
again describe network 
responds time based experience weights 
basically same cases explain network 
performs 

now decrease parameter intermediate value 
run happens trial 
time explain network exhibits different 
responses relate behavioral data showing 
expectation reaching 
measures task 

infants typically perform better errors 
delay reach 
simulate pressing button 
default delay press try 

explain effects delay networks behavior 

set back reduce 
think effect manipulation press 
case 

representation recurrent weights 
explain going here 

causes increase nature knowledge task 
interactions different forces 
importance mechanistic models 

wide range memory phenomena potentially 
explained terms interactions specialized brain 
areas described posterior motor cortex 
pmc prefrontal cortex pfc hippocampus related 
structures hcmp sketch areas contribute 
phenomena section explore 
explicit models due magnitude complexity models 
required 

hcmp model account 
contribution hippocampus range recognition 
memory phenomena hcmp enable subject 
experience seen item recognition 
addition hcmp recognition pmc 
form priming signal idea here 
sensitive effects small weight changes 
resulting item enabling people recognize 
item familiar computationally biologically 
account consistent process models 
recognition memory 
subjects signal 
fails 

list length list strength effects etc 

effects 

pfc hcmp interactions subjective organization sequential 
recall etc 

frequency level encoding etc 

pfc hcmp contributions capacity experience etc 

language particularly interesting domain study 
cognition plays important role cognitive 
phenomena depends wide range specialized processing 
pathways question regarding 
relationship language thought require 
neural network perspective provides useful intermediate 
based language input output representations 
associated wide range representations different 
parts brain enabling mutual interaction 
shaping representations verbal input activate 
perceptual verbal non motor representations via 
associations versa vice similarly time learning 
shaped influences language perception perception 
language short language viewed set specialized 
processing pathways perceptual motor 
association areas participate overall 
pathways produce human cognition 

issues dominated study language phenomena 
neural network perspective nature relationship 
rule processing processing exceptions 
regularities language contains 
aspects rules 
rules absolute seem exceptions 
metaphor computer perspective implement 
system set rules table 
exceptions dependent knowledge 
embedded nature neural network processing require 
separation types processing neural 
network models allow parsimonious accounts 
complex regularities exceptions language modeling 
unified system exhibits sensitivity 
regularities exceptions function frequency 

perceptual level language involves form object recognition 
takes visual sensory information 
representations primary building blocks language 
letters phonemes words perceptual pathways appear 
located appropriate general pathways different 
modalities ventral object recognition pathway 
recognition word processing viewed specialized 
versions apply ideas developed 
modeling visual word recognition 
provides representations words 
orthography explore basic features speech 
develop phonological representations capture 
features 

output end language able 
internal representations correct 
primary output pathways speech writing 
explore mapping perceptual motor output context 
simple models reading visual word input 
speech output domain regularities 
exceptions sound spelling orthography phonology mapping 
extensively studied modeled 

important challenge models ability account 
ability pronounce novel nonwords systematic ways 
generalization tests complex interactions 
different arise 
regularities mapping applicable 
apply locally different 

mapping system internal semantic representations 
word meaning speech output study 
words explicitly language 
feature specific types information specific 
case tense past inflectional system played large role 
application neural networks language phenomena 
period 
regular tense past inflection rule 
add producing things went 
taken indication based rule system 
applies rule detailed 
pattern data modeled neural network framework 
see correlational sensitivity hebbian learning 
combined driven error learning important capturing 
behavioral phenomena 

purpose language convey meaning naturally 
issue semantics assume semantics 
involves associations language 
representations rest cortex 
complex issue way language 
input shape semantic representations 
occurrence relationships different words idea 
words occur likely semantically 
related way shown 
hebbian based pca mechanism develop useful semantic 
representations word occurrence large text 
representations appear capture sense common 
relationships words explore model idea 
cpca hebbian learning developed 

taken different orthographic phonological 
semantic representational systems provide distributed 
representation words 
traditional models language discrete 
possible isnt pure area 
brain representations words simply distributed 
number different pathways specialized different 
aspects words idea distributed 
model language neural network 
perspective begin 
chapter model idea 
orthographic phonological semantic representations words 
interact activation areas produce 
appropriate corresponding activation areas 
relatively scale small model provides nice overall framework 
understanding relationships different component aspects 
distributed examined greater 
detail described 

finally language individual words 
sensory input reading stream speech 
somehow integrated time produce representations 
meaning scale larger structures 
etc similarly complex internal 
representations sequence simpler 
speech production writing temporally 
extended sequential processing developed 
critical understanding aspects 
language specialized memory systems hcmp pfc 
discussed likely 
important temporally extended structures language 
characterized see 
networks learn simpler real 
natural language capture important features 
behavioral data suggests regularities natural 
language highly specific case interpretation 
sentence depend specific words 
involved again easy account embedded 
knowledge dependent representations neural network 
explore interaction semantics grammar 
sentence model 
todo 

language involves range different brain areas 
models follow identify main brain areas 
potential interactions discuss relevant aspects 
output input modalities language assume 
familiar visual properties words focus 
details phonology people explicitly familiar 
details obviously implicit 
ability produce 
speech 

location areas showing 
relationship relevant specific areas 
motor output visual word processing 

psychology familiar brain 
areas commonly described important language 
area located posterior 
prefrontal cortex areas 
left side brain 
apparently responsible production speech output including 
higher order aspects patients damage 
area said deficit 
aspect speech unable 
produce speech function words 
structure sense think area 
level higher control area motor outputs 
speech production area located right 
temporal lobe parietal 
occipital area see 
apparently responsible aspects 
semantic content language patients damage 
area said exhibit 
speech produce semantically 
speech 
sense area important 
language processing representations temporal lobe 
semantic information encoded occipital lobe 
visual semantics parietal lobe spatial functional 
semantics 

thought complementary 
deficits surface properties speech production 
deep semantic properties language 
production respectively model examine 
similar set complementary deficits damaged different 
places domain reading speech observed 
model terms 
refer deficits reading ability 
dyslexia 

number brain areas damaged produce 
language cover detail 
useful generalization language seems 
areas surrounding part temporal cortex including 
adjacent parietal frontal occipital cortex further 
language function appears typically left 
left individuals 
effects right damage 
language catastrophic left 
damage detailed neural basis language 
see 

phonology sounds speech 
complementary terms 
speech production characteristics 
human sound producing terms 
resulting sound system 
clearly relationship differences 
way phoneme produced typically corresponding 
differences characteristics focus 
aspect phonology models focus speech 
production 

major features 
responsible producing speech sounds 

human speech production system based 
air aka 
nose pathway called 
illustrated 
open 
air speech sounds 
open phoneme said whereas 
closed changing 
positions things tongue affects 
properties air come 
different phonemes defined largely 
positions parts system discussion 
details distinctions phonemes 
critical representations 
models distributed representation roughly captures 
similarity structure different phonemes 

general categories phonemes vowels 
consonants own sets phonological 
characteristics discuss turn starting 
vowels standard system 
different phonemes 
labels here require non 
take character represent models 
operate things simpler 
standard english letters represent phonemes adopt 

known pmsp presented show 
examples sounds represent 

dimensional organization vowels according 
position tongue back 

representations vowels pmsp phoneme 
labels features based location tongue 
position short long note 
long vowels represented phoneme letters 
represent 

vowels form central word 
provide means air word 
said vowel sound same 
true consonants different dimensions 
vowels vary captured representations 
shows dimensions 
based position tongue back 
dimensions 
dimensions positions todo here 
length vowel sound short long long 
vowel typically 
similar short vowel shows vowel 
example try values 
dimensions activity pattern corresponding vowel 
network contains unit active group representing 
back 
representing possible combinations 
long short done avoid features active 
representations network itself develops 

representations consonants pmsp phoneme 
labels features based location 

soft 
manner 
vowel liquid 

vowels consonants typically produced 
restricting causes distinctive sound depending 
occurs way done called 
manner example 
sound air small opening produced 
tongue aka 
sound same 
closed critical 
features consonant representations location 
restricted way done 

different locations 
called 

soft 

follows 
phoneme air restricted 

constant sound phoneme 
produced lot vowel 
phoneme liquid phoneme 
finally involves complete 
air nose primary 
phoneme see 
full consonants 
features 

phonological representation word case 
centered vowel repeating consonants 
onset coda slots 

finally need scheme combining individual phonemes 
representation whole word idea adopt here 
commonly vowel centered representation slots 
side onset coda consonants 
surround word sufficient words 
models chapter onset 
coda slots consonant cases 
different consonants slots 
alternative 
scheme blank phoneme extra slots 
high frequency 
learning enables 
systematic orthographic phonological mapping developed 
words share onset coda 
position example 
overlap onset coda 
overlap 

purposes reading words represented 
distributed fashion orthographic visual word 
recognition phonological speech output semantic areas 

number separable representational systems different brain 
areas potentially involved representing aspect 
word large restrict focus 
areas essential reading number 
consisting principally orthographic visual 
word recognition representations phonological speech output 
representations semantic representations 
section explore 
model based developed 
demonstrates interaction different areas 
complete bidirectional connectivity see 
model provides 
distributed idea 
number relevant behavioral phenomena reading 
model address primarily understand 
effects brain damage reading ability 

dyslexia generic term reading problem 
different types categories reading problems 
identified type best known 
dyslexia focus main categories 
dyslexia phonological deep surface dyslexia 

phonological dyslexics selective deficit reading 
nonwords compared reading words 
terms model shown 
phonological dyslexia understood lesion direct 
pathway connecting orthography phonology preserved 
reading goes via semantics representations 
nonwords 

deep dyslexics phonological dyslexics read 
nonwords exhibit significant levels semantic 
errors read words semantically related 
words reading deep dyslexics 
visual errors reading 
combined visual semantic errors 
read presumably via tend 
errors abstract words compared 
concrete words reflect 
semantic representations 
items summarized implemented 
model way think deep dyslexics 
phonological dyslexics damage 
semantic pathway terms model shown 
deep dyslexia produced 
lesion direct pathway connecting orthography phonology 
preserved reading goes via semantics additional damage 
semantic pathway semantic errors produced 
see necessary additional 
damage semantic pathway semantic pathway 
come rely presence direct pathway 
direct pathway lesioned errors reflect similarities 
semantic representations semantic errors 
account deep dyslexia reflects damage direct 
pathway phonological dyslexia 

surface dyslexia contrast phonological deep 
dyslexia characterized preserved ability read nonwords 
problems semantic information written 
words todo provide example break bit critical 
difficulty reading exception words exception 
words words pronunciation follow 
regularities present words 
presumably easier 
mapping learned via semantics directly 
orthographic phonological representations 
mappings direct pathway surface dyslexics 
visual errors semantic errors 
interpret surface dyslexia resulting damage 
semantic pathway model shown 
preserved ability direct orthography phonology 
pathway reading 

simulation exercises see forms 
dyslexia phonological deep surface emerge 
kinds damage 

important feature model explore arises 
interactive bidirectionally connected nature regardless 
activation orthography semantics phonology 
flow simultaneously direct indirect pathways 
allowing contribute activation 
representations pathways allow 
division labor processing different types 
words example reading word based orthographic input 
direct pathway phonology indirect pathway via 
semantics participate pathway relatively 
important word division labor 
established network learns fact 
pathway find type mapping easier 
rely greater extent unless pathways 
damaged relatively difficult see effects 
division labor addition obvious effects 
pathway damage damage direct pathway ability 
read nonwords model exhibit 
subtle effects pre division labor certain 
words different pathways 

pmsp 
actually simulate full set pathways showed kind 
division labor interactive neural network explain 
characteristics surface dyslexia showed 
partial lesioning model direct orthography phonology 
pathway produce right patterns deficit 
cases surface dyslexia simulated effect 
semantic pathway actually implementing 
providing partial correct input appropriate phonological 
representations training direct pathway model 
reduced amount input network 
learned simulating semantic pathway lesion showed 
network general properties surface dyslexia 
virtue learned presence simulated 
semantic pathway direct pathway dependent 
imagine 
words direct learn 
regular words relatively semantic pathway 
removed exception words particularly low frequency 
ones dependent upon 
lesioned semantic pathway 

phenomenon division labor different pathways 
function learning network apparent 
pathways damaged novel mechanism 
patterns behavior dyslexia neural network models naturally 
provide explore aspects phenomenon model 
network completely interconnected 
kind form 
particularly interesting result 
effects possible tell simple story 
complementary direct semantic pathway lesions producing deep 
surface dyslexia respectively 
semantic pathway comes depend partially orthographic input 
phonological system errors 
input missing errors semantic nature 
deep dyslexia due overlap semantically related items 
difference phonological deep dyslexia 
account lesion phonological 
pathway see todo 

traditional based rule accounts reading 
reading direct based explicit rules 
mapping orthography phonology containing 
based word table exceptions rules related type 
mechanism 
traditional accounts seem 
interactive distributed model approaches 
debate actually general 
existence basic pathways discussed 
direct orthography phonology pathway indirect pathway 
via semantics central issue debate 
versus single nature 
processing mechanisms taking place 

real contrast explicit rules 
direct pathway traditional model compared neural 
network sensitive mappings 
contrast understanding way 
divide labor discussed 
neural network direct pathway sensitive 
regularity frequency whereas based rule 
system concerned regularity direct pathway 
neural network learn regular 
frequency high irregulars frequency low irregulars 
rely semantic pathway provide specific word input 
contrast based rule system expected 
frequency high irregulars further discrete 
switch network approach involves 
interactive processing division 
labor 

present model small number words focused 
capturing level relationships different 
types capable 
frequency regularity effects play important role 
debate nature processing direct pathway 
well revisit issues next section 
explore elaborated realistic model 
direct pathway based pmsp model results model 
show neural network implementation direct pathway 
learn high frequency addition 
irregulars general empirical data appear support 
neural network approach pmsp 

model full bidirectional connectivity 
orthography phonology semantics 

model based directly developed model deep dyslexia 
same set words 
roughly same representations words described 
original model pathways orthography 
semantics phonology 
direct pathway orthography phonology pathway 
original model assumed completely lesioned deep dyslexia 
further damage parts original model allowed 
replicate effects seen deep dyslexics model includes 
direct pathway able explore range phenomena 
including surface phonological dyslexia possibly simpler 
account deep dyslexia 

model looks essentially identical 
shown hidden 
units direct pathway hidden layer 
semantic pathway hidden layers distinction 
concrete abstract words described detail 
semantic hidden layers units semantic hidden layer 
divided groups corresponding concrete semantic features 
units abstract semantic features units 
interconnected layer units same group 
implement idea emphasized 
main representational layers 
orthography semantics phonology connection self 
recurrent weights implement pattern completion via 
dynamics encourage layer settle 
trained patterns initially 
pattern known reduces number blend 
responses contain components different words 

words simulation concrete 
abstract words roughly orthographic similarity 
shown phonological representation word 

train network representations developed 
orthography phonology semantics 
set words words concrete 
physical objects abstract 
choice word types mentioned deep 
dyslexics treat types words specifically 
deep dyslexics semantic errors abstract words 
concrete ones presumably semantic representations 
concrete words richer robust damage sets 
words closely orthographic features 
orthographic representations 
distributed representation letter 
features letter representation appearing 
right left order letter slots 
word phonological representations slots 
individual units slot representing phonemes 
appear slots word phoneme 
blank unit represented fact semantics developed 
set features characterized properties word 
features concrete words shape main 
found abstract ones 
location difficulty quality 
obviously semantics 
important distinction concrete words features 
richer semantic representations abstract words 
distinction models ability simulate deep 
dyslexics performance categories words 
cluster plot semantic similarities shown 
shows abstract items 
cluster concrete items main clusters 
corresponding roughly versus things 

cluster plot similarity semantic 
representations different words abstract 
words central cluster main 
concrete words correspond roughly 
things 

slightly modified representations 
model consistent simple representations 
orthography phonology orthography individual units 
represent letters slots simpler 
consistent phonological representations 
eliminate representation similarity 
similar letters level similarity otherwise 
networks behavior visual confusions based 
words sharing letters common captured orthographic 
representations phonology representations 
consistent subsequent simulations 
specifically pmsp phoneme labels described 
distributed 
features phoneme individual units 
original model slots 
original model repeating consonants scheme 
reasons described earlier 

network trained randomly selecting trial 
main representational layers act input layer 
setting network learns 
take aspect words representation map onto 
corresponding representations standard parameters 
hidden unit activity 

open project directory 
see network constructed stored 
skeleton keep project file relatively small 
network takes time train epochs load 
trained pre network begin 
overall control panel 

initial exploration observe behavior 
network words presented orthographic input 
layer press overall control panel select 
process control panel press 
see activation flow network settle 
correct pronunciation semantics word 
large square see 
record networks performance word including name 
input word column name word 
closest output produced network 
column case same 
input word pronounced correctly distance 
actual network output closest event shown 
column column 
fact same name input reflected 
zero column 
error here indicated meaning 
same step patterns monitor 
values sure reading correctly step 
words pay attention network display point 
phonological representations activated 

direct input via orthography indirect input via 
semantics cases initial 
phonological pattern subsequently later input 
describe find discuss 
behavior network damage 
pathways 

now explore ability network read 
pathways phonology removed action relatively 
simple manipulation provides insight networks behavior 

expect semantic pathway leaving intact 
direct pathway characteristics surface 
dyslexics read words semantic 
representations visual errors 
direct pathway reading semantic pathway 
potential semantic confusions arises expect 
replicate effects deep dyslexia finding semantic 
visual errors note phonological dyslexia form 
deep dyslexia explore perform 
amounts partial damage lesioning entire pathways 

hit overall control panel 
select pathway lesion 
actually units network structure 
lesion entire layer 
note entire pathway network rely 
intact means errors expect 
associated properties intact pathway 
lesioned example lesioning direct pathway 
network rely semantics allowing possibility semantic 
errors extent semantic pathway 
things right missing direct pathway 
completely lesioning semantic pathway itself lead 
semantic errors semantic information left 
errors based test network process 
control panel patterns 
want network 

reading overall count number 
column look 
counted notice 
column indicating phonological 
output closely known word larger 
threshold output 
novel blend phonological features correspond 
closely words training set 
fact say properties 
network 

errors compare word network produced 
input word produced 
word similar 
input word called visual error 
based visual properties semantic properties 
word 

lesioned semantically networks errors visual 

now lets try opposite lesion 
selecting time again 

errors visual errors 
common sense cluster plot 
common 
sense determine semantic 
similarity response input word 
semantic errors count number cases 
think true 

now scroll bar bottom 
scroll log see columns right 
hand side log reflect automatic 
networks responses according visual semantic overlap 
word patterns set columns applying 
concrete words set abstract ones input 
orthography response orthography determined response 
phonology overlap letters error 
visual error semantic errors difficult code 
variable amount activity pattern 
semantic representations overlap 
measured normalized product 
patterns semantic error formula 

goes overlapping non patterns 
completely overlapping ones value good job 
including cluster plot shown 
column 
semantic visual errors blend responses 
column apply 

summarize results far seen lesion 
semantic pathway results purely visual errors lesion 
direct pathway results combination visual semantic 
errors order approximation observed 
surface deep dyslexics respectively shown pmsp 
model surface dyslexics actually likely errors 
frequency low irregular words 
manipulation simple corpus words examine 
aspect performance critical difference here 
surface dyslexics semantic errors deep 
dyslexics errors visual errors 

now perform realistic form lesion sequence 
series different lesion types corresponding 
different layers semantic direct pathways 
units increments layer 
effects reading performance lesion types 
semantic pathway hidden layers 
replicate effects surface dyslexia next 
direct pathway replicate 
effects phonological dyslexia high levels produce 
deep dyslexia next lesion types 
semantic pathway hidden layers again 
pathway corresponds model deep dyslexia explored 
finally last lesion type 
direct pathway hidden layer again 
full lesion semantic pathway produce 
extreme form surface dyslexia included 
particular 
motivation 

begin lesioning select overall 
control panel choose see graph 
appear process control 
panel graph right 
display performance automatically error types 
function level damage type graph 
left display results 
last lesion level different type lesion 
lesion types axis order described 
starting 

press process control panel 
network display update times 
beginning lesion turned 
final corresponding actual 
lesion network 
lesion 
items read network want 
keep graph display 
results see point note 
level damage intact network 
interesting next time network display 
updated notice units 
layer missing reflects partial damage 
increase next time notice 
right graph begins reflect presence visual errors 
concrete abstract words damage levels axis value 

damage levels axis value 
right graph start next layer 
proceed layer 
lesion types final damage performance 
cases shown left graph log 
axis indicating lesion type again values correspond 
sequence layers described correspond 
indirect semantics pathway hidden layers 
same time complete lesion direct pathway 
value corresponds lesions direct 
pathway complete lesion indirect 
pathway 

able sense results 
obtain random nature lesion 
need average different instances lesion 
type reliable statistics expected types 
errors lesion type process ran configured 
run type lesion run 
function control panel 
lesion produced 
read easier main results 

error types semantic pathway lesions 
intact direct pathway point 

shows results different 
lesions semantic pathway intact direct pathway 
lesion types concrete abstract plotted separately 
generally case complete lesion semantic 
pathway network visual errors 
generally consistent surface dyslexia expected 
semantic errors 
lesioning semantic pathway remember intact 
direct pathway providing orthographic input directly 
phonological pathway input generally 
phonological output related orthographic input 
prevents semantic errors 
words think damage semantic pathway 
causing want semantic errors 
direct orthographic input well see moment 
direct input removed semantic errors 

interestingly lesions semantic layer itself produce 
errors concrete versus abstract words 
understood considering 
division labor develops semantic system 
better able process concrete words compared abstract ones 
direct pathway take 
processing abstract words effects damage 
understood complementary ways perspective 
damaged semantic pathway concrete words 
depend damaged pathway 
perspective intact direct pathway abstract words 
depend intact pathway 
shows general effects semantic variables 
network specific confusions words 
similar semantics semantic errors aware 
data surface dyslexics existence 
effect humans possible factors 
frequency regularity effect 

error types semantic pathway lesions 
conjunction completely lesioned direct pathway 
point 

shows same semantic pathway 
lesions previous figure time conjunction 
complete lesion direct pathway lesion types 
corresponds type lesion studied 
model deep dyslexia 
levels semantic pathway lesion now see semantic errors 
visual errors relatively large number 
errors pattern errors generally 
consistent deep dyslexics kinds 
errors comparing previous figure see 
direct pathway playing important role generating correct 
responses particularly semantic confusions 
semantic pathway otherwise 

compare bar graph corresponding case 
direct pathway lesion damage semantic pathway 
subsequent bars additional semantic pathway 
damage appear necessary produce semantic error 
deep dyslexia explain direct 
pathway lesion leads semantic errors 

thing notice figure relative number 
semantic errors concrete versus abstract words 
characteristics deep dyslexia patients semantic 
errors abstract words relative concrete words 

concrete abstract words number semantic errors 
representations types words 

error types direct pathway lesions 
intact semantic pathway full complete 
semantic pathway lesion 

shows effects direct pathway 
lesions lesion type lesion type 
intact semantic pathway lets focus case 
intact semantic pathway full figure 
notice smaller levels damage 
relatively semantic errors produced 
errors visual corresponds well phonological 
dyslexia assuming damage direct pathway 
pronunciation nonwords presumably 
read via direct orthography phonology pathway 
test aspect model 
words scale large model direct pathway described 
next section produces nonword pronunciation deficits event 
relatively small amounts damage 

interestingly level damage increases model 
increasingly semantic errors 
performance high levels damage provides good fit 
deep dyslexia characterized presence semantic 
visual errors plus pronounce nonwords 
discussed context phonological dyslexia occurs 
network rely semantic pathway 
previously intact direct pathway now 
pattern overlap semantic representations causes 
semantic confusions reading further see aspect 
deep dyslexia data greater proportion semantic 
errors abstract words concrete ones 
add semantic visual semantic errors 
abstract words 

finally last case direct pathway damage completely 
lesioned semantic pathway produces visual 
errors clear relationship 

model illustrates words represented distributed 
fashion set different specialized areas layers 
damage layers produces behavioral results similar 
observed different types dyslexics general 
framework interactions orthographic semantic 
phonological representations model 
elaborated subsequent models model specific pathways 
greater detail 

general point results 
difficult tell damage based patterns 
error example levels direct semantic 
pathway lesions produce roughly visual errors 
larger direct pathway lesions semantic errors 
start appear important demonstration purely 
behavioral approaches cognitive neuroscience 
require evidence additional 
including computational models 

previous model included direct pathway 
orthography phonology small number words 
real exploration important issues regularities 
exceptions mapping larger number words needed 
regularity 
complexity nature mapping english described 
mapping written word spelling orthography sound 
phonology studied extensively roughly english 
words series influential models 

models central issues relationship 
processing regular versus exception words specifically 
single system process ability replicate 
systematic performance human novel 
nonwords depends properly encoding subtle 
regularities letter typically pronounced 

understand issues understand nature 
regularities exceptions regularity defined 
mapping letter phoneme present 
relatively large number examples language example 
consider pronunciation vowel words 
hint etc case 
pronounced same way short vowel 
regular pronunciation group words define regularity 
called neighborhood contrast word 
pronounced long vowel 
exception word critical note 
respect vowels regular pronunciation depends letters 
word example words mind 
find etc form neighborhood regular pronunciation 
long form vowel time further consider 
example familiar rule regarding effects final 
produces regular neighborhood words 
long vowel pronunciation fine 
etc 

non english 
aware regularities english simple form 
good reason believe single system necessary 
perform mapping factors need taken 
account determine regular response 
irregulars thought extreme examples process 
pronunciation dependent configuration entire 
word general continuum regularity 
neural network appropriately trained weights 
deal continuum naturally taking appropriate 
contingencies account response contrast 
traditional based rule account direct sound spelling pathway 
requires elaborate collection rules deal 
properties mapping 

existence regularities described 
sense english asked 
pronounce novel nonword regularity 
pronounce 
complex nature regularities systematic behavior 
appropriately sensitive conjunctions letters cases 
pronounced 
pronounced 
conjunctions initial pronounced same 
correct way independent letters system 
sensitive wrong letter conjunctions respond 
word analogy familiar word 
ignore pronounce existing word 
correct generalization performance nonwords 
depends critically developing representations 
appropriate combination conjunctive depending letter 
conjunctions combinatorial 
allowing arbitrary combinations initial 
combined goal neural 
network models reading show appropriate 
representations develop learning pronounce known 
english words 

significant neural network model reading developed 
model 
learned perform mapping english 
words due choice output input 
representations model generalize 
nonwords well meaning produce same kinds 
systematic human subjects produce same 
word inputs representations model called 
due conjunctive nature 
ideas represented 
conjunction letters phonemes entire word input 
represented activation combinations sequential 
letters phonemes including initial final blank 
represented contained word 
represented 
useful capturing pronunciation 
letters discussed allow combinatorial 
representations letters individual letters phonemes 
encoded entirely different units fact 
letter pronounced same way regardless 
surrounding context 

model developed 
pmsp model problems representations 
single units represent letters phonemes regardless 
surrounding context combinatorial representations 
specifically divided word parts onset 
vowel coda set phoneme letter 
representations parts example word 
onset plus unit described 
unit vowel units coda 
mapping letter pronunciation 
systematic same representations 
again regardless surrounding context 
think lead exact nature 
word order information part actually 
encoded input 
represented turns isnt 
problem english example 
typically different words differ letter order 
parts onset vowel coda pmsp model 
added conjunctive representations specific 
pronunciation consequences 
pronunciation extra unit active referred 
similar scheme developed output phonology 
combining combinatorial conjunctive elements 

short pmsp model tuned hand representations 
significantly simplified learning 
orthography phonology mapping model 
provides useful demonstration importance appropriate 
representations fact representations 
researchers learned network 
itself remains problem model present here avoid 
problem network appropriate 
representations own model based ideas 
developed invariant object recognition model 

perspective object recognition model see 
pmsp models same tradeoff position 
invariant representation recognize features objects multiple 
locations emphasized pmsp model conjunctive 
representations capture inputs 
accomplished conjunctive units 
pmsp satisfying resolution tradeoff 
object recognition model development hierarchy 
representations produce increasingly invariant 
increasingly complex representations expect 
same approach applicable word reading well 
reason believe word recognition 
special case object recognition same neural 
pathways same basic mechanisms explore here 
reading model based object recognition model 

basic structure orthography phonology 
reading model orthographic input representations 
words appearing possible locations letter 
position input next hidden layer receives 
slots forms locally invariant conjunctive 
representations next hidden layer representations 
map standard slot centered vowel repeating 
consonant phonological representation output 

basic structure model illustrated 
orthographic input presented 
string letter activities letter slots 
slot units representing different 
letter plus additional space 
simulation present 
related simulations words presented positions 
allowed entire word fit 
word presented starting 
slot analogous input patterns 
object recognition model number different positions 
smaller 

next layer input called layer 
representations object recognition model 
units window onto 
letter slots units develop locally invariant 
representations unit letter 
slots conjunctive representations 
encode local order information letters 
words units develop tuned hand 
pmsp input representations next hidden layer fully 
connected hidden layer phonological output 
performs mapping orthography phonology 
phonological representation standard slot centered vowel 
repeating consonant representation described 

actual network skeleton view 
slots input units total units 
hidden layer units 
hidden layer slots phonology output units 
total 

pmsp corpus words train 
network word presented according square 
actual frequency square frequency 
pmsp models necessary here enable 
network train frequency low words reasonable amount 
time pmsp demonstrated similar results actual 
square frequencies word appear 
positions input entire word fit 
slots words different positions 
longer ones word itself 
position word combination subject frequency 
manipulation words appear 

activity constraints set activity hidden 
layers proportion hebbian learning set 
value necessarily small prevent constant pressure 
hebbian component error term 
successful learning corpus further object 
recognition model large networks necessary reduce 
learning rate epochs prevent 
subsequent weight changes cause 
interference previous ones parameters standard 
standard non parameters activity smaller 
hebbian learning learning rate decrease time 
characteristic larger networks trained large 

note simulation requires minimum 
run 

open project sound spelling 
directory again large network 
looks shown 
constructed well build 
load trained network took couple train 
overall control panel 
network window 
select 

well see network read words presented 
standard list probe words developed 
locate process 
control panel press 
word network best presented left edge 
input read word input pattern 
individual letters activated 
slots slots corresponds location 
contains units corresponding letters 
unit blank slots begin 
unit lower left right 
right 
input patterns 
verify best fact pattern presented output 
pattern produced correct pronunciation 
verify need view phonology patterns compare 
phoneme slots output appropriate patterns 

press overall control panel 
select initial slots 
onset find clicking 
button consonant window see 
pattern matches slots output next lets 
look coda 
click button consonants window scroll 
button number select 
middle button left button patterns 
match produced network finally select 
again select look vowel 
pattern click button see 
matches central vowel slot pattern 

probe words test network 
settling times regular consistent means 
inconsistent whereas regular 
inconsistent versions examples ambiguous 
clear regularity exceptions 
exceptions regularities 

need pattern matching 
process time simulator iconify 
consonant vowel windows locate 
shown column components 
orthography basic phonology repeated consonant phonology 
network actually produce special code indicating 
shows 
mean return later explore settling 
time properties network pronunciation network 
actually produced shown column 
show correct output produced note output 
contain indicates phoneme 
position exactly match valid phoneme patterns 
shown number cycles took network settle 
error todo show epoch 

network times notice 
best input appears 
positions input differences input location 
network capable producing correct output spatial 
invariance coding explored 
requires network maintain 
information local letters 
best example treat entire 
pattern same regardless appears well see 
moment network developed same general solution 
problem previous combination locally 
spatially invariant conjunctive encoding 
continue patterns 
want switch network updating update trial 
cycle overall 
control panel select observe 
error items lower frequency 
irregular network pronounce words 
correctly 

now lets explore connectivity network click 
left portion orthography input click 
left button see skeleton network 
shown clicking back 
forth skeleton weights clear 
units receiving left 
letter slots letter slot group units click 
back left button verify 
groups units receive 
overlapping groups letter slots 

click units pay attention 
patterns weights thing notice 
cases unit strong weights same input 
letters slots press 
control panel select 
particularly good example unit unit 
selected middle region layer 
unit receives letter slots 
providing locally spatially invariant 
representation letter units layer 
object recognition model 
networks hidden layer learns pronunciation consequences 
associated letter particular position 
representation learning automatically generalize 
locations letter kind 
thing pmsp tuned hand input representations 
accomplish see network learned own 

link unit 
hidden layer clicking hidden layer unit unit 
projects strongly selecting again 
selecting time original 
unit plus examining moment selected 
addition single hidden layer unit see unit 
strongly driven unit 
viewing receives clear pattern central 
vowel slot phonological output layer weights 
network generally symmetric interpret 
unit projects looking receives 
sure lets select show sending weights 
switching back forth 
see generally exactly symmetric 
hebbian learning component symmetric 
driven error 

interpret phonological output pattern hidden unit 
produce need again select choose 
phoneme particular hidden unit 
produce possible outputs 
letter overall pattern connectivity good sense 
further see hidden unit receives 
units code letter 
weights unit right side layer 
strongly interconnected hidden layer unit click 
back viewing see unit 
represents letter input locations 

example type invariant coding found 
letter 
case output hidden unit projects 
features corresponding coda verify 
couple phonemes coda 
performs kind coarse coded 
representation multiple related mappings 
looking weights unit input 
shows receives letter letter 
similar shown 
looking patterns finally 
look complex input pattern 
receiving vowels plus letters 
serve activation coda 
output mapping vowel present accounts 
production consonants 
general level unit shows complex conjunctions 
input letters represented here 
layer units object recognition model 
model 

important exercise 
weights network seems learning right kinds 
representations allow good generalization 
representations similar layer object 
recognition model combine spatial invariance 
conjunctive feature encoding able obtain 
insight looking representations 
easily further networks complex activation 
dynamics picture difficult 
figure processing input 
know nature mapping problem itself lots subtle 
forces determine 
pronounce word finally fact easily 
interpret units weights place due 
hebbian learning causes weights reflect 
probabilities unit occurrence 

networks weights 
relatively clear example representations sense 
terms output input mapping performed specify 
letters units encode hidden 
units combine phonemes hidden units 
produce output relate analysis need 
spatial invariance conjunctive encoding 

next test networks ability generalize knowledge 
pronounce english words pronounce nonwords 
regularities spelling sound mapping 
number nonword sets exist 
sets pmsp test model set nonwords 
lists derived regular words 
exception words set 
constructed determine nonwords actual 
words pronounced better set 
lists control list list 
set nonwords derived 
regular exception probe word lists test 
network earlier 

start testing model nonwords back 
nonword list 
click back network looking 
see network correctly 
pronounced nonword producing 
turn network display see 
nonwords errors possible see 
produced similar real word different 
reasonable pronunciation word 

summary nonword reading performance raw 
values single provided output 
whereas allowed shows results alternative outputs 
consistent training corpus allowed 

errors regular exception 
nonwords phonological representation 
repeating consonant phonology cycles number cycles 
network took settle output actual output produced 
network explanation 
network output based training corpus 

errors control 
nonwords columns previous table 

errors nonwords 
columns previous table 

note described exact same 
network testing show slightly different errors 
fixed later 

errors nonword lists 
summarized 
output different locations input 
seems network solve 
task pmsp network view file 
cases error note 
error single invalid phoneme total error 
output pattern 
closer correct pattern didnt 
error error tried determine network 
produced output cases output 
reflected valid pronunciation present training set 
didnt happen pronunciation list 
following pmsp computing networks 
overall performance 

total model pmsp 
human data shown 
clearly present model performing roughly same level 
humans pmsp model conclude 
network capable extracting complex subtle 
underlying regularities regularities sub present mapping 
spelling sound english applying 
nonwords 

model single pathway processing regular 
exception words direct traditional 
theories hold direct mapping 
spelling sound occur regular words via 
application explicit rules case 
model exhibits significantly performance low frequency 
irregulars word categories expected 
discussed previous model indirect via semantics 
important words todo clear 

semantics phonology mapping showing separable 
inflectional component semantic representation 
past tense onto regular phonological inflection add 

section explore model mapping semantics 
phonological output presumably 
trying internal output 
end reading system shown 
explored general mapping 
semantic representation word phonology 
essentially random relatively rare cases 
interesting task 
issue inflectional 
know change inflection word 
convey different aspects meaning example want 
indicate event occurred past past 
tense inflection adding relevant verb 
think 
tense component semantic representation gets 
onto appropriate inflectional representation phonology 
idea illustrated 

types inflections applied english verbs 
include adding indicate person 
adding past 
adding case 
reading model explored 
previous section irregular general rule 
add past tense inflection 
went similar issues 
regarding processing versus irregulars come 
domain 

shaped curve irregular tense past inflection 
production axis shows minus 
overregularization rate axis 
beginning overregularizations right 
starts rate 
least years assume point 
significant frequency 
represented line end graph 

turns issue inflectional specifically 
past tense played important role 
development neural network models language 
issue phenomenon known shaped curve 
irregular tense past inflection due overregularization 
shaped curve performance initially good gets 
worse middle gets better 
exhibit shaped curve producing correct inflection 
irregular verbs initially correctly produce irregular 
inflection saying went 
long period overregularizations 
irregular verb regular 
went finally learn treat irregulars irregulars 
again shows overregularization 
pattern 
speech serves 
primary phenomenon figure 
clear overregularization phenomenon 
occurs relatively low frequency 
considerable individual emphasized 
subset studied show strong 
evidence early correct production irregulars 
subject individual well 

overregularization phenomenon originally 
reflecting action based rule system gets bit 
early language 
developed neural network model 
showed shaped overregularization curve argued 
networks sensitive regularities 
output input mapping tendency 
problems 
model 

shaped effect apparently due manipulation 
training set large number frequency lower regular 
words introduced network learned smaller 
number frequency high words irregulars 
regular words caused network start 
treating irregular words overregularization 

number network models past tense learning 
developed 

entirely successful capturing essential 
properties shaped curve plausible manner 
introducing manipulations 
work example model widely 
fully account 
limitations depends critically manipulation 
training environment starts original 
model small number high 
frequency irregular verbs adding new verbs 
add training set 
clear adding new regular verbs 
triggers overregularization network 
manipulation discrete 
change basic problem isnt network itself 
driving overregularization further 
unable replicate original results 
realistic corpus based english opposed 
artificial corpus original model problem 
model task mapping base 
phonology representation form 
particularly plausible compared semantics phonology version 
here 

possible reason prior explore here 
models based backpropagation algorithm 
important limitation 
overregularization data show features 
distinguish leabra algorithm pure backpropagation 
competitive kwta activation dynamics hebbian learning 
contribute producing shaped overregularization curve 

couple ways understand pure driven error 
backpropagation learning likely produce shaped 
curve general level 
performing error 
produce error curve shaped 
detailed level network learned 
achieve mapping semantic representations corresponding 
past tense inflection phonological 
output see part 
learning task influence weight updates weight 
updates driven purely error signals network 
regular inflection pressure further 
shaped curve likely 
short 

arguments clear hebbian learning activity 
competition enhance shaped curve again general 
level constitute additional constraints 
otherwise driven error learning 
specific level seen playing important role 
hebbian learning drives weights represent 
correlated activity provide sustained influence strong 
correlation tense past aspect semantics regular 
past tense inflection regularity continue 
influence learning network regular 
inflection hebbian learning care network 
producing correct output role activity competition 
obvious according results important 
hebbian learning way thinking effects terms 
competition hidden units favor 
regular mapping favor irregular 
limited number units active regular mapping 
occurs expect units representing 
regular mapping competition frequency 
irregular verbs producing overregularization network 
activity competition similar competition 
regular irregular mapping taking place 
weights dynamics activation 
settling pattern activation dynamics allow 
forth back mappings 
gradual weight changes allow 

explore ideas model past tense 
mapping illustrated compare 
results leabra standard backpropagation 
network test idea distinctive features 
important producing realistic shaped curve 

past tense network mapping semantics 
phonology 

basic structure model shown 
semantic input hidden layer 
phonological output layer general structure approach 
model based work 
provided initial corpus words model 
network trained produce appropriate pronunciation 
different english verbs irregulars 
different inflections total training items 
words presented square actual 
frequency general scale 
simulation roughly same order previous model 

inflectional forms english verbs 

different ways verb english shown 
inflections thought 
extra bit information verb 
semantic input model main components 
semantic pattern 
represents basic meaning verb constant 
inflections basic 
inflectional component represents senses 
verb produced attempt accurately 
represent semantics mapping 
phonology largely random anyway random bit patterns 
units semantic layer encode 
pattern sequential groups units represent 
inflections next units total 
semantic input units 

phonology representations standard centered vowel 
repeating consonant ones described 
additional slot end additional 
inflectional phoneme needed inflectional 
component purposes human output included 
extra phonemes represent inflections occurred last 
slot same features 
combination same 
features person sounds 
changes past 
same features hidden units 
active time level sparseness 
useful learning arbitrary mapping semantics 
phonology due need keep different patterns relatively 
separate saw hebbian learning 
level ran network pure driven error 
learning generic backpropagation network comparison 

open project 
directory again project stored network skeleton 
form overall control panel 
load trained pre weights 

start well observe network produces phonological 
outputs semantic patterns locate process 
control panel see 
semantic input presented network settling produces 
activation pattern phonological word 
base form know looking 
activations provides 
actual output produced column 
identity input pattern column event 
coded initial number representing index 
verb followed target pronunciation followed code 
indicating inflection type order 
regularity verb 
irregular regular here represents base 
inflection irregular verb see 
errors network took cycles 
produce output 

looking back network notice active units next 
last row semantic input units 
indicate base inflection produced now 
next word see units adjacent 
previous now activated indicating past inflection 
produced case continue 
remaining inflections verb 
now lets regular word press 
button process control panel enter 

todo 

now see network learned task lets try 
connectivity determine working 
interested tense past mapping well focus 
find hidden units selective 
past tense inflectional semantics click 
network sending weights click tense past 
inflectional semantics unit unit left 
last row click adjacent tense past units notice 
good consistency hidden units activated 
reflecting work hebbian learning rule notice 
different hidden units activated click last base 
inflectional semantics unit unit left last 
row person inflectional semantics unit 
unit right last row now lets back 
past tense inflectional unit strongly 
connected hidden units subsequent pressing 
menu window appears type 
enter comparison value 
see hidden units now selected 

now click left units 
see unit favor particular onset phonemes 
clear particular coda phonemes see 
phonemes correspond lets click 
select window 
containing phoneme patterns last slot inflections 
click event button inflectional 
phoneme left side window see pattern 
inflection click middle button 
blank event see pattern 
active phoneme produced clear 
hidden unit favors outputs now lets determine 
phonemes last next slot iconify inflections 
window press again select 
click middle button buttons 
see unit produce patterns 
last next phoneme slot similar weight patterns 
found hidden units 

clear subset units encoded regularity 
present tense past mapping presumably units 
activated irregular words 
able regular inflection see 
competition takes network learns 

production inflection 
same technique past tense 
inflection active units code appropriate 
inflectional phonological pattern describe steps 
took reach answer 

plot overregularizations total responses 
network learns clear initial period 
overregularizations network producing valid 
responses words overregularization 
minus number overregularizations begins 
eventually relatively 
provides fit data 

interesting see network 
learned task relevant empirical data 
course time overregularizations network learned 
output training pattern analyzed 
outputs code different kinds errors primarily 
interested overregularizations irregular past tense words 
counted number times network said things 
went results network 
human data 
note following 
overregularization plotted minus proportion 
overregularization errors characteristic 
shape evident graph 

graph networks performance plots proportion 
valid responses network showing 
network achieves substantial level responding prior 
onset overregularization note measure 
indicates extent network producing kind 
output overregularization 
errors take account inflectional 
errors tells extent network produce 
semantic input pattern 
network demonstrates correct early period irregular verb 
production critical aspect empirical data previous 
models capture manipulations 
training corpus parameters further 
overregularization continues low 
extended time period characteristic human 
data 

total number valid responses 
network prior overregularization 
provides measure extent early correct period 
prior onset overregularization hebbian learning 
leabra inhibitory competition leabra appear 
contribute relative generic backpropagation network 

assess influence hebbian learning activation competition 
length correct early period compared 
different types networks standard explored 
leabra hebbian learning leabra network 
hebbian learning generic backpropagation network ran 
networks type initial period training 
point overregularization starts counted number 
valid responses prior 
overregularization 

results show hebbian learning extent 
activation competition important producing longer 
correct early period 
understood terms role hebbian learning 
activation competition producing greater specialization 
representations early frequency high irregulars 
encoded 
overregularization 

total number overregularizations 
training inhibitory competition leabra appears 
important contribution overregularization relative 
generic backpropagation network clear 
difference significant 
appear hebbian learning playing 
significant role here 

finally counted total number overregularizations produced 
full training epochs single 
types networks computationally 
practical run results shown 
show leabra networks 
considerably backpropagation network 
interestingly appears work done 
activation competition here leabra network 
hebbian learning produces slightly overregularizations 
hebbian learning based single run impossible 
determine significant difference appears 
hebbian learning least adding top 
activation competition substantial difference 
leabra networks backpropagation 
due chance 

described activation competition seems 
overregularizations presumably allowing dynamic 
based activation competition units encoding regular 
irregular mappings way understanding terms 
output multiple network explored 
multiple outputs case irregular regular 
inflections irregular verb activation competition 
present possible network produce 
discrete different output patterns different trials 
competition network patterns 
pattern correct case backpropagation 
network learns overregularization 

results exploration show model comes 
closest capturing characteristic shaped learning curve 
hebbian learning activation competition 
error driven learning hebbian learning appears 
specifically important early correct period 
activation competition appears specifically important 
achieving large number overregularizations overall 
findings detailed 
evaluation models fit human data complicated 
factors 

human data itself 
data cases 
starting well onset language production 
difficult determine shaped curve truly 
seen subset individuals 

difficult achieve valid mapping network 
learning human learning network completely 
focused semantics phonology mapping trial 
learns rapidly compared 
example network achieved well valid responding 
irregular trials training difficult 
know map number trials onto learning 
experience assumes 
speech rate word 
contributes learning semantics phonology 
equivalent time period achieve valid responding 
amount entire time 
shaped curve network complete corpus 
roughly trials years 
observed empirically apply kind generic 
scaling networks performance likely major part 
difference due complexity larger 
language task performing 
expect relatively simple models provide 
detailed picture 

finally advantage model relative 
completely training environment 
networks gradual learning environment results 
gradual nice 
contrast external 
task modeled semantic 
production task undoubtedly case semantic 
representations gradually developing same time period 
place important constraints learning process 
captured well existing corpus techniques 

best interpretation model 
demonstrates network environment 
exhibit shaped curve principles 
hebbian learning activation competition well 
number reasons model need 
additional properties order better fit human 
data 

word occurrence 

illustration semantic information 
distributed number specific processing pathways 
represented here different sensory modalities 
representations orthography phonology associated 
corresponding distributed activation patterns semantics 
figure adapted 

general think semantic representations word 
representations distributed different 
specialized brain areas notion distributed semantics well 
captured adapted 
shows representations items 
etc different specialized 
processing pathways pathways represented 
sensory figure actually imagine 
number types 
sequential oriented task etc 
contribute overall distributed representation item 
essentially pattern activity brain 
contributes specific item information considered 
semantic representation 

section explore model semantic representations 
takes advantage word occurrence statistics form 
distributed representations word model 
implements larger distributed semantic network 
emerge information words tend 
occur speech reading 
shown method call 
semantic analysis word occurrence statistics 
semantic representations good job 
human semantic example system 
trained text psychology textbook able 
multiple choice based text 
performance roughly correct well short good 
nonetheless simple procedure 
perform well 

interesting aspect word occurrence approach 
captures word association semantics 
semantics example definition word 
word words highly 
appears 
important capturing structure human semantic memory 
example semantic priming word 
presented followed word read faster 
word 

expect hebbian model 
learning provides natural mechanism learning word occurrence 
statistics fact method involves performing 
essentially sequential principal components analysis 
spca correlation matrix words expect 
cpca hebbian learning develop useful representations 
follows explore simple cpca network trained 
word textbook probe 
resulting network see captured important aspects 
information presented text 

model shown input layer 
unit word projects hidden layer containing 
units training network follows 
trial units representing words present 
individual activated network settles 
hidden activation pattern function input 
settling cpca hebbian learning takes place encoding conditional 
probability input units active hidden 
units repeated 
text multiple times 

key insight method works comes fact 
whenever similar input patterns presented likely 
similar hidden units activated extent 
reliable correlations subsets words hidden 
representations encode further network 
learn words similar 
happen appear same words 
likely occur similar words likely 
tend activate common subset hidden units 
hidden units learn similar high conditional probability 
words words presented 
network produce similar hidden activation patterns 
indicating semantically related 

textbook language contain detailed information 
phonology interested learning 

chapter revisit fundamental issues 
computational cognitive neuroscience covered previous chapters 
towards integration chapters explore 
remaining challenges field 
addressed next generation models bring 
conclude ways computational models 
towards development cognitive neuroscience 

covered broad range text 
ions individual neuron level learning 
complex cognitive functions planning 
goal understand findings different levels 
interactive approach emphasizes 
connections neurobiological cognitive computational 

saw individual neurons act 
detectors constantly inputs responding 
matches pattern weights synaptic 
ion channels neuron compute balance excitatory 
inhibitory leak currents reflected membrane 
potential potential exceeds threshold neuron fires 
sends inputs neurons network rate firing 
encoded continuous variable activation 

showed individual detectors 
network exhibit useful properties 
provide building blocks cognition network properties 
build ability individual detectors compute balance 
excitatory inhibitory inputs properties include 
input patterns patterns emphasize 
distinctions collapse bidirectional top 
bottom processing pattern completion amplification 
bootstrapping inhibitory competition activity regulation 
sparse distributed representations multiple constraint 
satisfaction networks updating 
constraints possible 

built basic network properties 
showing neurons network learn 
weights according activation values sending receiving 
units learning local variables results coherent 
beneficial effects entire network analyzed main 
learning model learning task learning model learning 
causes network capture important aspects statistical 
structure environment task learning enables network 
learn specific tasks learning complementary 
achieved known properties 
synaptic modification mechanisms extra mechanisms 
learn temporally extended sequential tasks mechanisms 
main forms learning updating internal 
context representations temporal reinforcement 
based mechanism 

overall cognitive 
architecture builds mechanisms described 
different brain areas defined basis fundamental 
tradeoffs arise basic neural network mechanisms 
tradeoff captures distinction hippocampal system 
rest cortex terms learning rate system 
learn arbitrary things rapidly extract underlying 
regularities environment tradeoff captures 
difference prefrontal cortex posterior cortex 
terms ability perform robust active maintenance active 
maintenance suffers representations interconnected 
activation away memory 
useful kinds processing 
pattern completion satisfaction constraint 
generally posterior cortex understood terms 
relationship specialized brain areas 
generally posterior cortex seen set specialized 
organized pathways transformations 
building upon 

provided good example 
organized sequence transformations lead ability 
recognize objects spatially invariant fashion level 
units input combining features integrating 
different locations sizes model 
recognize individual objects well confused multiple 
objects present adding spatial representations interact 
object processing pathway enables system sequentially 
process objects scene containing multiple objects 
accounts effects lesions spatial processing pathway 
performance posner spatial task level 
cortical visual processing neurons encode correlational 
structure present visual images 

explored different ways neural 
networks implement memories weights activations saw 
priming tasks cortical memories taking form small 
weight changes produced gradual learning residual activation 
network saw basic cortical model short 
capturing human memory consistent 
discussion fundamental computational tradeoffs 
regarding tradeoff rapid 
learning arbitrary information slow integrative learning 
regularities find basic cortical model 
learning regularities perceptual domain performs 
required rapidly learn novel information 
suffers catastrophic interference hippocampus 
sparse pattern separated representations avoid 
interference learning rapidly regarding tradeoff 
active maintenance rich interconnectivity find basic 
cortical model fails hold onto information delays 
face interference prefrontal cortex isolated 
representations finally interaction activation 
based weight memory complex saw model 
task 

language explored requires number 
specialized processing pathways representations interact 
build perceptual pathways 
representations specialized pathways operate according 
same principles pathway saw distributed 
pathway model word representations account 
patterns dyslexia damaged focusing direct pathway 
orthographic input phonological output saw network 
captured regularities enable generalize pronunciation 
nonwords same way humans focusing pathway 
semantics phonology saw network learn 
regularities exceptions inflectional 
produced shaped overregularization curve learned 
explored model semantic representation 
learning based occurrence statistics words large 
text resulting semantic representations capture relevant 
aspects word similarity finally saw network 
learns next word produced sentence 
learn certain important aspects grammar including verb 
embedded 

chapter took challenge applying biologically realistic 
neural network mechanisms explored previous chapters modeling 
higher level cognition saw notion controlled 
processing implemented combination top biasing 
prefrontal cortex binding hippocampus accounts 
general kinds things higher level cognition 
higher level focusing role prefrontal cortex 
pfc viewed maintaining internal context 
representations explored sequential processing 
saw simple model account 
normal performance task important 
next step see dynamic control maintained pfc 
activations via gating implemented dopamine 
leads flexible task performance compared simple 
based weight task learning explored context 
simple task dopamine control 
pfc representation updated basal ganglia 
important determining pfc representations updated 
possible see mapping dynamic controlled 
gating mechanism firing production system 
neural network models higher level cognition able 
production system models 
higher level cognition well learning 
major challenge domain higher level cognition 
understand appropriate pfc representations learned 
experience enable flexible task performance 

brief summary clear computational 
models lot say different aspects cognitive 
neuroscience excited range different 
phenomena current framework address number 
important challenges remain future work address 
section attempt identify challenges 
sketch ideas addressed 

history neural network modeling dominated periods 
extreme extreme past entire 
approach based limitations 
research subsequently overcome limitations 
rule delta learning taking similarly approach 
find list future challenges long issues 
here cover question 
entire hand 
existing ignore important 
limitations encourage balance 
approach simply remaining 
challenges feel recent years 
balance 
same 
balance continue 
future years 

need simplification neural network modeling 
strengths 
strength allows extract example 
essential insight biological properties 
important particular behavior 
simplification largely 
details simplification 
facts 

relationship different levels 
modeling varying level detail range cognitive 
phenomena covered modeling 
extent different levels detail 
phenomena beneficial mutual constraints 
models 

true individual researchers face tradeoffs 
important study point time 
larger field allows multiple parallel 
approaches see key ultimately solving 
simplification problem different models different levels 
analysis detail overlap 
model 
simplification further detail extent 
models compared area overlap 
simplified model taking 
account behavior detailed model least 
limitations simplified model known 

examples effective multiple overlapping models 
found places book 
compared performance unit fires discrete spikes 
computes rate code compared 
detailed implementation inhibition inhibitory 
interneurons kwta simplification examples showed 
reasonable detailed 
case clear differences 
possible explore models 
otherwise language chapter 
explored number phenomena simplified model 
main representations involved reading larger 
realistic models explore detailed aspects performance specific 
pathways simplified model possible explore certain 
aspects behavior effects damage 
manner provided general framework 
detailed models examples sample 
kinds benefits multiple levels 
analysis 

sections follow discuss areas 
detailed models 

reading 
neuroscience brain research etc 
biological details biological 
properties largely ignored models general answer 
question powerful 
require lots biological actually implement 
undoubtedly fail capture 
mechanisms capture main effect 
generally speaking useful relate functional 
properties detailed mechanisms simpler 
find exactly differences 
matter following particularly relevant 

powerful simplification activity regulation 
kwta function likely lot biological 
necessary keep neurons firing right activation 
addition basic feedforward 
feedback inhibition includes lots channels 
lines discussed section regulation self chapter 
factors expression 

major simplification neural network models 
initial networks simply starting 
random connectivity additional topographic 
constraints network already constrained receive 
particular types inputs huge largely problem 
biology cognitive neuroscience understand 
biological structure complex sequence 
interactions genetic chemical surface 
etc significant portion initial 
brain kind process 
development brain greater degree 
subject influences experience know specific 
examples detail early visual system 
example random noise coming retina plays important role 
neurons 
line set initial configuration ends 
learning begins undoubtedly understanding 
brain gets initial configuration critical 
understanding later effects learning 

typically simplified control processing 
learning inputs presented controlled fashion 
networks activations reset input 
phases input plus minus learning constrained 
operate appropriate information real system 
simple events defined pre 
activation occurs 
controlled tests simulations show 
activation important rapid successful learning 
needs explored context spiking models 
generally exhibit prior states hysteresis 
code rate models constant output activation 
interconnected units existing activation patterns 
activation important discrete spiking 
models biological reality potential implementation 
explored well take issue phases learning 
learning generally later section 

argued exploring aspects 
biological complexity greater detail strong argument 
thinking neuron simpler 
biology suggest summarize neuron shouldnt 
able take advantage further complexity 
brain noisy needs robust simple graded signals 
actually useful neurons output signal 
receive inputs isnt 
complex processing inputs 
useful form single output 
shown detailed spike timing cortical neurons 
seem convey meaningful information 

keep mind cases simpler things 
work better currently technique 
images large number smaller images 
serve pixels larger image close 
images looks random images 
step back eyes larger overall 
picture simpler describe overall image 
picture describing properties 
individual component images similarly 
eyes biological 
details produces simpler relevant picture neural 
computation obviously take 

pieces put right 
look simple 

models text focused cortex including 
hippocampus exception 
way thalamus basal ganglia reality 
course cortex operates context large number 
brain areas including 
areas computational models including fact 
generally determined 
means complex specialized 
neuron types easily terms 
patterns weight values otherwise generic unit model 
case cortex cortical model 
simply learning playing dominant role 
producing useful model difficulty modeling additional 
brain areas raw increase computational complexity 
involved adding additional brain areas model 

missing brain areas seen providing basic 
inputs cortex 
effects nature processing cortex 
explored role 
dopamine context prefrontal active 
maintenance learning addition number 
similarly important roles 
example 
ability cortical neurons remain 
particular task function task performance variables 
important neurotransmitter 
regulation states versus 

part possible ignore extent 
assume cortex completely focused task 
hand richer complete model cognition 
require factors play 
critical role human performance 
psychological task situations 

argued brain kind similar self 
structure coarse models multiple brain 
areas relatively units same principles 
fine models individual columns large number 
units number good reasons believe 
generally true multiple models approach test 
scaling assumptions 
test well detailed fine model 
coarse clearly coarse 
model represent small fraction information compared 
fine overall dynamics relevant 
behavioral characteristics ultimately 
limitation computational power impossible 
fine models multiple brain areas 
computers computational rapidly 
example computer power basically period 
book written increasingly 
possible implement large fine models test 
simplified scaled models provide reasonable approximation 
realistic 

presented biological mechanisms 
implement driven error task learning showed models 
learn basis mechanisms models simply 
imposed necessary minus plus phase structure required 
learning occur provided target outcome patterns 
output layer open challenge demonstrate expectation 
outcome representations actually arise naturally simple 
motor perceptual system operating simulated environment 
particularly perception outcome happens 
same layers represented expectation challenge 
address question system 
phase plus perform learning 
resolution challenges further modeling work 
evidence brain kind switching phase proposed 
underlie driven error learning well signals 
signal learn 

evidence comes updating spatial representations 
parietal cortex function neurons actually 
new input coding function 
motor same neurons update reflect 
actual input coding result motor 
specifically parietal neurons represent 
see movement 
movement update reflect actually 
see movement actual 
representations clear expectation outcome 
difference drive learning 

issue learn related dopamine 
examined role driving learning based differences 
expected obtained reward 
requires dopamine signal occur difference expected 
versus obtained outcome reward todo add 

finally body evidence suggests 
cortex involved detecting errors seems 
likely brain area plays important role driven error 
learning exact role remains 

temporally extended tasks planning relationship production system 
kinds models etc initial important issues 
described clearly 
work needs done 

important question higher level cognition necessarily 
requires scale large models important aspects 
captured smaller models words higher 
level cognition happens large critical cortex gets 
going basic principles 
scaled models clearly believe latter least 
partially true 

number problems neural 
network models past successfully 
existing models generally constitute major 
challenges discussed problems 
represent ongoing challenges field continue pay 
attention address future models 

neural network models theoretical big 
parameters 
nice 
here add additional 

work statistical 
learning provided purely theoretical 
account tradeoff rapid arbitrary slow integrative 
learning related complementary roles 
hippocampus neocortex learning memory arguments 
work based general principles particular 
truly theoretical 
principles demonstrate 
required assumptions 
etc argued 
theoretical importance network models 
issues analyzed terms general principles 
apply kind learning mechanism based statistical 
properties environment models understood terms 
principles todo 

example comes 
provided 
implemented model effects regularity 
frequency learning reaction time 

number models produced form 
relatively implementation shows effect 
clear principled understanding 
increasingly rare field improve 
important maintain focus 
understanding models performance terms set theoretical 
principles particular 

issue closely related previous 
book times planning writing 
algorithm 
leabra possibly write textbook based 
completely new algorithm level true 
leabra new algorithm widely 
specific version leabra here didnt exist 
final form well writing book 
implementation 
general level essential principles define 
leabra algorithm 
number times different 
single algorithm entire book 
computational cognitive neuroscience 
particular algorithm algorithm 
connection saying 
collection principles important combined 
appropriate fashion help understand wide range 
phenomena ion channels language processing higher level 
cognitive planning 

distinction 
implementation leabra general principles 
based upon discussed similar distinctions clearly 
came side balance 
multiple levels analysis feel same way here 
results obtained book say 
specific implementation general principles 
found models largely successful 
conclude implementation principles 
pretty good undoubtedly better performance way 
obtained specific cases different 
implementation different set principles 
time know obvious alternative algorithms sets 
principles provide good fit wide range 
performance ones chosen book 
emerge adopt subsequent 
book 

related points traditional 
neural network models typically backpropagation 
networks learning 
learned emphasized people 
neural networks solving practical problems todo 

number points here 

brain complex dynamic system 
likely easy restricting 
models easy understand 
likely good approach 

number reasons believe standard 
backpropagation algorithm interpretation 
typically highly 
learning weights themselves 
relevant aspects task discussed length 
showed hebbian model 
learning provides generally useful bias advantage 
producing constrained easily weights 

advantages hebbian constraint interpretation 
text particularly good examples 
object recognition model 
reading tense past language models 

resulting models provide nice balance computational 
power models 
currently computationally weak localist models 

related focus number free parameters neural 
network models argument 
parameters models learn 
showing learn 
impossible know parameters crucial learning 

essential emphasize challenge 
modeling neural networks 
basic nature free parameters 
neural networks people fail generally 
people say train network 
true interesting aspects 
models based aspects network 
performance generalization new problems response 
damage example fact network behaves certain way 
damaged due fact trained 
way specifically trained perform 
correctly observed resulting performance 
basic computational properties model damaged 

further learning shapes weight parameters model 
researchers precise control 
learning precise understood well set 
principles shape networks weights interaction 
environment understanding principles network 
learns apparent complexity model reduced 
relatively simple application small set principles 

important aspect models included text 
generally exactly same set standard parameters 
models differ parameters differences based 
principled parameter 
example main parameters models 
activity level hidden layers saw 
activity level important 
tradeoff learning specific information individual 
patterns sparse activations versus integrating patterns 
distributed activations 

neural networks 
etc 

learning terms representations automatically 
produce generalization object recognition model 

see showed 
networks form systematic internal representations 
enable good generalization 

introduce completely novel unit 
network know people 
novel combinations existing representations via distributed 
represent novel items 

care real humans mechanistic models 
tell real 

binding comparison multiple time hierarchical structure 
etc 

section main contributions 
computational approach rest cognitive neuroscience 
areas science computational models 
theoretical obvious 
involved advantages theory theory 
cognitive neuroscience 
empirical approaches empirical researchers 
found common sense theoretical approaches 
simple box process models 
general computational modeling 
field 

things weve tried text demonstrate 
specialized theoretical computational approach necessary 
complexity brain 
behavior show approach actually 
practical 

general theories important sense data here 
examples ways computational models sense 
data sense means showing fits 
thing observe things 
understand basic force 

memory systems 

pfc deal reward executive control 

complicated models help principles 
division labor learning interactivity etc 

number constructs cognitive neuroscience 
psychology single isolated viewed 
terms brain true things attention memory 
working memory neural network models help 
field whole constructs 
mechanistic constructs fit better 
underlying biology 

example saw attention emerge interaction 
brain areas inhibitory activation constraints overall 
process multiple constraint satisfaction operating 
inhibitory constraints representations 
situation notion attention 
proposed non network neural perspective 
idea 
neural network perspective considerable support 

memory good example constructs priming 
semantic priming considered separate 
viewing mechanistic computational basis see 
important distinctions types priming weight versus 
activation based see priming 
typically typically identified 
term long effects corresponding based weight mechanism whereas 
semantic priming typically identified transient 
based activation effects based activation 
based weight semantic priming 
recently demonstrated case semantic priming researchers 
working neural network perspective 

need introduce new box new finding 
disengage inhibition 

working memory active maintenance top activation attention 
executive control etc 

difficult see gradual development 
spatially invariant representations work 
implemented model 

sentence model complex emergent representations capture 
lots things 

semantics via word occurrence 
